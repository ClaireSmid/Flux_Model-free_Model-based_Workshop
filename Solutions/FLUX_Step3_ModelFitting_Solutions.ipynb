{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b425559",
   "metadata": {},
   "source": [
    "# Flux Computational Workshop 17 September 2021\n",
    "## Computational modeling of goal-directed and habitual reinforcement-learning strategies\n",
    "\n",
    "Claire Smid<sup>1</sup> and Wouter Kool<sup>2</sup><br>\n",
    "<sub><sup>1</sup> University College London<br>\n",
    "<sup>2</sup> Washington University in St. Louis<br></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf59a71",
   "metadata": {},
   "source": [
    "### Workshop Step 3: Goal and Purpose\n",
    "\n",
    "In the previous section, you successfully implemented a  model that could perform the two-step task by using either model-free control, model-based control, or a _mixture_ of both.\n",
    "\n",
    "The main reason research have developed these mixture models, is to use them to explain human behavior on this task. In order to do that, you need to first recruit a bunch of participants to perform a task like the one we just described. Then, you can fit the computational model to each participant's data. This results in a set of values for each parameter in the model (i.e., $w$, $\\alpha$, $\\beta$), that best describe the data. In other words, you can use model-fitting to infer _how_ model-based a participant was, _how_ high their learning rate was, and _how_ 'greedy' (high inverse temperature) or _how_ 'explorative' (low inverse temperature) they where.\n",
    "\n",
    "Most computational modeling papers rely on techniques like this. For example, the authors may want to investigate differences in behavioral processes between children, adolescents and adults, estimating these with their computational model. Other papers use within-subject manipulation of particular variables (such as cognitive load), and then use model fitting to estimate the effect of this manipulation on the parameter of interest. Other papers might fit a whole array of different computational models to their behavior, and use model comparison or selection to find the model that best describes the behavior for their participants.\n",
    "\n",
    "Unfortunately, we don't have data for the current task quite yet. This is partly because this particular task is brand new! Therefore, we can't fit the model to behavioral data. So, we are going to fit the model to simulated data.\n",
    "\n",
    "You may think that's a little bit silly: if we already know the parameter values used when simulating behavior, why would we want to then fit the model back to this data? \n",
    "\n",
    "We do this because this allows us to get an insight into how _recoverable_ parameter values are. In other words, if we give each parameter a certain value, then how well can we recover those values (get the same values back) using our model-fitting technique? \n",
    "\n",
    "#### So what will we do?\n",
    "First, we will just use the exact same mixture model as in the previous section to simulate an agent with parameter values we define. Next we will _fit_ this data to a version of the model that will find the best corresponding parameter settings for this participant. Then, we can see how similar the values we put in are to the values we get out. That's **parameter recovery!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664a4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we'll import our usual packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# and one additional package for our optimizer\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c910c81",
   "metadata": {},
   "source": [
    "### Step 1. Drifting reward probabilities\n",
    "Just as like in the previous script, we are going to generate drifting reward probabilities in the same way. You can run this script as is (it is identical to the code in the first script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e99c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drifting reward rates \n",
    "def generate_rewards(n_trials, bounds, init_vals, drift_rate):\n",
    "\n",
    "    # pre-allocate the rest\n",
    "    reward_probs = np.ones((n_trials, 2)) * 0.5\n",
    "\n",
    "    # To start with, we initialise one planet lower than the other\n",
    "    reward_probs[0,:] = random.sample( init_vals, len(init_vals))\n",
    "\n",
    "    # next, we loop through the trials, and add Gaussian noise to each planet's reward probability for each trial separately\n",
    "    for t in np.arange(n_trials-1)+1:\n",
    "\n",
    "        for state in range(2):\n",
    "\n",
    "            upd = np.random.normal(0, drift_rate) # random Gaussian noise\n",
    "            \n",
    "            reward_probs[t, state] = reward_probs[t-1, state] + upd\n",
    "            \n",
    "            # reflecting bounds \n",
    "            reward_probs[t, state] = min(reward_probs[t, state],max(bounds[1]*2-reward_probs[t, state],bounds[0]))\n",
    "            reward_probs[t, state] = max(reward_probs[t, state],min(bounds[0]*2-reward_probs[t, state],bounds[1]))\n",
    "            \n",
    "    return reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7dc5c4",
   "metadata": {},
   "source": [
    "### Step 2. Softmax and temporal difference updating rule\n",
    "Please insert your softmax and temporal difference functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f0a82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def softmax(Q,beta): \n",
    "#     p = # calculate the reward probabities\n",
    "#     return p # here we return the computed reward probabilities\n",
    "\n",
    "# hint: you should us np.exp, and np.sum. You can multiply numpy arrays with a scalar  using *, and you can divide numbers using /\n",
    "\n",
    "# To be removed:\n",
    "def softmax(Q, beta):\n",
    "    p =  np.exp(beta*Q) / np.sum(np.exp(beta*Q))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b63d0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def td_rule(alpha, Q_chosen, PE):\n",
    "#     Q_chosen_updated = # and then use to compute the Q-value\n",
    "#     return Q_chosen_updated\n",
    "\n",
    "# To be removed:\n",
    "def td_rule(alpha, Q_chosen, PE):\n",
    "    Q_chosen_updated = Q_chosen + alpha * PE\n",
    "    return Q_chosen_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963e0d4",
   "metadata": {},
   "source": [
    "### 3. The model for our mixture agent\n",
    "We will need our mixture model (from the previous section) in order to simulate the data. \n",
    "\n",
    "The biggest difference in the model we have here, is that we will only return the choices, rewards and spaceship pairs for each trial. We have already completed this for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55928591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a for loop, and generate choices for the agent as they go through the trials and learn from rewards\n",
    "\n",
    "def mb_mf_agent(n_trials, alpha, beta, w):\n",
    "    \n",
    "    # We'll use your generate_rewards function to generate drifting reward probabilities\n",
    "    bounds = [0, 1]\n",
    "    init_vals = [0.4, 0.6]\n",
    "    drift_rate = 0.2\n",
    "    reward_probs = generate_rewards(n_trials, bounds, init_vals, drift_rate)\n",
    "    \n",
    "    # pre-allocate empty arrays for the data (in chronological order)\n",
    "    \n",
    "    # from previous trial\n",
    "    prev_rews = np.zeros(n_trials) # this will log the rewards won on the previous trial (i-1) \n",
    "    \n",
    "    # first stage (spaceships)\n",
    "    first_stage_state = np.zeros(n_trials) # this will log which spaceship pair the participants saw\n",
    "    state_sim = np.zeros(n_trials) # this will log whether participants saw the same spaceship pair, or the other one \n",
    "    choices = np.zeros(n_trials) # which rocket the participants chose\n",
    "    \n",
    "    # second stage (planet)\n",
    "    second_stage_state = np.zeros(n_trials) # this will log which planet the participants transitioned to\n",
    "    stay = np.zeros(n_trials) # whether participants repeated their transition to the same planet, or not \n",
    "    rewards = np.zeros(n_trials) # this will log how many rewards the participants won on this trial\n",
    "\n",
    "    # Initialise model-free values for the spaceships\n",
    "    Qmf1 = np.zeros((2,2)) + 0.5 # do we need to explain arrays in arrays?\n",
    "    \n",
    "    # Initialise model-free values for the planets\n",
    "    Qmf2 = np.full(2, 0.5)\n",
    "    \n",
    "    # Initialise model-based values\n",
    "    Qmb = np.full(2, 0.5)\n",
    "    \n",
    "    # Now, we loop through the trials, making choices\n",
    "    for i, reward_prob in enumerate(reward_probs):\n",
    "\n",
    "        # Randomly start in rocket pair 1 (0) or pair 2 (1)\n",
    "        s1 = first_stage_state[i] = np.random.choice([0, 1])\n",
    "        \n",
    "        # calculate model-based q-values - fill these in\n",
    "        # note that Qmb just reflects the MB values of the currently available actions. \n",
    "        # remove: They are calculated online, so there is no updating necessary\n",
    "        Qmb[0] = Qmf2[0]\n",
    "        Qmb[1] = Qmf2[1]\n",
    "        \n",
    "        # mix the two sets of Q-values here with the mixing weight (remember that when w = 0, only the model-free\n",
    "        # q-values matter, while when w = 1, only the model-based values are taken into account)\n",
    "        Qnet = w * Qmb + (1-w) * Qmf1[s1]\n",
    "        \n",
    "        # Agent makes a choice using softmax\n",
    "        p = softmax(Qnet, beta) # Fill this in\n",
    "        \n",
    "        # Make a weighted choice using the np.random.choice function and the variable p (you can use help(function) \n",
    "        # to find out more) (identical to first model)\n",
    "        choice = choices[i] = int(np.random.uniform(0,1) > p[0])\n",
    "        \n",
    "        # Make the transition to the planet\n",
    "        s2 = choice # finish this line to transition to the right planet\n",
    "\n",
    "        # Calculate first prediction error (no reward here)\n",
    "        pe_1 = Qmf2[s2] - Qmf1[s1,choice] # Finish this to calculate the first pe                            \n",
    "        \n",
    "        # update choice\n",
    "        Qmf1[s1,choice] = td_rule(alpha, Qmf1[s1,choice], pe_1) # finish this with the td rule \n",
    "        \n",
    "        # Agent receives a reward (1 or 0) (depending on reward probability)\n",
    "        r = rewards[i] = int(np.random.uniform(0,1) < reward_probs[i,s2])\n",
    "        \n",
    "        # Calculate second prediction error (actual reward)\n",
    "        pe_2 = r - Qmf2[s2]    # Finish this to calculate the second pe                                        \n",
    "        \n",
    "        # update value of alien (planet)\n",
    "        Qmf2[s2] = td_rule(alpha, Qmf2[s2], pe_2) # finish this with the td rule for the alien\n",
    "        \n",
    "        # update value of choice (spaceship)\n",
    "        Qmf1[s1,choice] = td_rule(alpha, Qmf1[s1,choice], pe_2) # finish this with the td rule for the spaceship choice\n",
    "        \n",
    "        \n",
    "        # you can uncomment the lines below to see what happens each trial in printed form.\n",
    "        # note that if you uncomment these, the figure in the cell below will print all the way \n",
    "        # at the bottom of the printed statements (just comment the lines out again to see figure immediately)\n",
    "        \n",
    "#         ###   comment code between this this out or in (shortcut = ctrl + /)\n",
    "        \n",
    "#        # print spaceship choice and which pair was seen \n",
    "#        # spaceship chosen\n",
    "#         if choice == 0:\n",
    "#             position = 'left'\n",
    "#         else:\n",
    "#             position = 'right'\n",
    "            \n",
    "#         print(f'trial: {i+1}, rocket pair: {s1+1}, spaceship chosen: {position}')\n",
    "              \n",
    "#         # print which planet was transitioned to\n",
    "#         if s2 == 0:\n",
    "#             color = 'red'\n",
    "#         else:\n",
    "#             color = 'purple'\n",
    "\n",
    "#         print(f'transitioned to {color} planet, first prediction error: {pe_1}')\n",
    "#         print(f'stay: {bool(stay[i])}, agent received: {r}')\n",
    "#         print(f'{color} planet prediction error: {pe_2}')\n",
    "#         print(f'Model-free q-values for spaceships:\\n[1:left, 1:right,\\n2:left, 2:right]\\n{Qmf}')\n",
    "#         print(f'Model-based q-values for spaceships:\\n{Qmb}') # this is now 2 values\n",
    "#         print(f'Qnet for planets [red, purple]: {Qnet}\\n')\n",
    "        \n",
    "#         ###   comment code between this out or in\n",
    "\n",
    "        \n",
    "    # data returned by function\n",
    "    return choices, rewards, first_stage_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e816a4",
   "metadata": {},
   "source": [
    "## How is model-fitting different from generating data?\n",
    "So far in this workshop, we have generated a lot of data with the models you coded up. In many of the exercises, you changed the values of the parameters that determine this behavior, such as the learning rate (or how quickly agents learn from rewards), the inverse temperature (how greedy or explorative agents are), and the model-free model-based mixing weight (which dictates how much the participants apply their knowledge of the underlying structure). \n",
    "\n",
    "Now we are going to discuss how we can extract these parameters from behavioral data, _without knowing beforehand how quickly people learn, how much they might exploit, and how much they rely or don't rely on the task structure during decision making_. To this end, we are first going to explain some of the tricks scientists use to fit their model to real data.\n",
    "\n",
    "### The log-likelihood function\n",
    "In order to find the best-fitting parameter values for a given participant, we need to be able to quantify how well a given set of parameter values (in our case, a triplet of values for $\\alpha$, $\\beta$, and $w$ for each participants) describes their behavior. If we have such a function, one that puts a number on the quality of this description, the only thing we then need to do is to _maximize_ it. That is, we need to find the set of values that gives us the highest possible score on this function.\n",
    "\n",
    "Most of the time, computational psychologists aim to maximize the _likelihood_ of the data given the model. The likelihood is a simple probability: $L = P(data|model)$. This describes the probability that you would see this data under a given model. In our case, we can write down this likelihood function a bit more precisely: $L = P(behavior|\\alpha,\\beta,w)$. That is, it is the probability you would see the behavior, given certain values of $\\alpha$, $\\beta$, and $w$. So, now we only need to find the set of values for these three parameters that maximize $L$.\n",
    "\n",
    "In our model, this likelihood is simply equal to the probability that the participant made the choices given the parameter values. This is great, because we already have a way to compute the probability of each, namely with our softmax function. Therefore, in our likelihood function, we are going to store the probability of each choice that was actually made (as computed by softmax), and then compute the likelihood of all choices made by an agent on the task.\n",
    "\n",
    "As you may remember, the probability of a series of independent events is equal to the product of the probabilities of each individual event. Because these probabilities will never be larger than 1, the likelihood of a series of choices is bound to be a very small number (especially when the number of trials is high!). Unfortunately, computers do not do too well with representing very small numbers (this is called point underflow). Therefore, we are going to again use a clever trick and compute the log-transformed likelihood. Because the log-transformation of a product is the sum of the log-transformation of the individual components of a product ($log(A\\cdot B) = log(A) + log(B)$, we can avoid underflow by computing the log-likelihood instead of the likelihood.\n",
    "\n",
    "So in our case, the log-likelihood is the sum of the log probabilities of each choice under the given model. Our goal is to find the parameter values (alpha, beta, w) where the log-likelihood is _maximized_, so where the probabilities for the actual choices were the highest. \n",
    "\n",
    "If you want to read more about the loglikelihood in a fun blogpost, you can find one here: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "\n",
    "Now that we got that out of the way, let's take a look at the log-likelihood function!\n",
    "\n",
    "We have written it almost completely out for you below, and as you will see, it looks very much like our previous models, with 2 small (but very important changes):\n",
    "- The parameters alpha, beta and w are now entered into the function as part of x (alpha = x[0], beta = x[1], w = x[2]). we do this so it makes it easier to use the function elsewhere, and also it reduces human error! (We often forget all of the variables we are working with...)\n",
    "- It outputs only one variable: the negative log-likelihood, summed over trials, as you'll see at the end (note that because it's summed over trials, this means that with higher trial numbers, the log-likelihood also increases)\n",
    "\n",
    "<mark>First, try to read through the function below, and see if all still makes sense.</mark>\n",
    "\n",
    "<mark>Next, you can try computing the log-likelihood!</mark>\n",
    "    \n",
    "<mark>There are 2 lines that you will need to complete, you will see them in the code below with a comment. What you will need to do is:</mark>\n",
    "1. Compute the _log_-likelihood for the choice on each trial. We have put the functions you should use in a comment above the line where you need to fill this in. \n",
    "2. Finally, you need to _sum_ the log-likelihood over trials, and make it _negative_, so we get the _negative log-likelihood summed over trials_.\n",
    "    \n",
    "As always, feel free to ask Claire or Wouter for help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbe6587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb_mf_negloglik(x, choices, rewards, first_stage_state):\n",
    "    \n",
    "    # unpack the parameter values for use\n",
    "    alpha = x[0]\n",
    "    beta = x[1]\n",
    "    w = x[2]\n",
    "    \n",
    "    # pre-allocate log likelihood\n",
    "    LLH = np.zeros(choices.size)    \n",
    "    \n",
    "    # Initialise model-free values for the spaceships\n",
    "    Qmf1 = np.zeros((2,2)) + 0.5 # do we need to explain arrays in arrays?\n",
    "    \n",
    "    # and for the planets\n",
    "    Qmf2 = np.full(2, 0.5)\n",
    "    \n",
    "    # Initialise model-based values for the spaceships\n",
    "    Qmb = np.full(2, 0.5)\n",
    "    \n",
    "    \n",
    "    for i, (c, reward, spaceship_pair) in enumerate(zip(choices, rewards, first_stage_state)):\n",
    "        \n",
    "        #print(f'curr trial: {i}, curr c: {c}, curr reward: {reward}, curr s1: {s1}')\n",
    "              \n",
    "        choice = int(c)\n",
    "\n",
    "        # Randomly start in rocket pair 1 (0) or pair 2 (1)\n",
    "        s1 = int(spaceship_pair)\n",
    "        \n",
    "        # calculate model-based q-values \n",
    "        Qmb[0] = Qmf2[0]\n",
    "        Qmb[1] = Qmf2[1]\n",
    "        \n",
    "        # mix the two sets of Q-values here with the mixing weight (remember that when w = 0, only the model-free\n",
    "        # q-values matter, while when w = 1, only the model-based values are taken into account)\n",
    "        Qnet = w * Qmb + (1-w) * Qmf1[s1]\n",
    "                \n",
    "        # probability of choice\n",
    "        p = softmax(Qnet, beta)\n",
    "        \n",
    "        # calculate the log probability for this choice here\n",
    "        # you should use the numpy.log() function (remember we call numpy as 'np')\n",
    "        LLH[i] = np.log(p[choice])              #--- Fill this in\n",
    "         \n",
    "        # Make the transition to the planet\n",
    "        s2 = choice \n",
    "\n",
    "        # Calculate first prediction error (no reward here)\n",
    "        pe_1 = Qmf2[s2] - Qmf1[s1,choice]                             \n",
    "        \n",
    "        # update choice\n",
    "        Qmf1[s1,choice] = td_rule(alpha, Qmf1[s1,choice], pe_1)  \n",
    "        \n",
    "        # Agent receives a reward (1 or 0) \n",
    "        r = int(reward)\n",
    "        \n",
    "        # Calculate second prediction error (actual reward)\n",
    "        pe_2 = r - Qmf2[s2]                                           \n",
    "        \n",
    "        # update value of alien (planet)\n",
    "        Qmf2[s2] = td_rule(alpha, Qmf2[s2], pe_2) \n",
    "        \n",
    "        # update value of choice (spaceship)\n",
    "        Qmf1[s1,choice] = td_rule(alpha, Qmf1[s1,choice], pe_2) \n",
    "        \n",
    "    # data returned by function: the negative loglikelihood\n",
    "    # you need to sum the log likelihood here using numpy.sum() and make it negative\n",
    "    return - np.sum(LLH)                        #--- Fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe7785",
   "metadata": {},
   "source": [
    "### Simulate and model fit one agent\n",
    "Now that we have the log-likelihood function and our simulated data that outputs choices, rewards and the first stage state, we are ready to fit our model to the data! \n",
    "\n",
    "What we will try to do here is as follows: \n",
    "1. We will simulate choice behavior for one agent, according to the values we specify at the top of the cell below\n",
    "2. we will use an _optimizer_ to search through the _parameter space_ (which is essentially different combinations of values for $alpha$, $beta$ and $w$) to find the combination of parameters that provide the best (the smallest) negative log-likelihood, which means that the model was able to predict the agent's choices well with these parameter values.\n",
    "\n",
    "Before we jump in, let's briefly talk about optimizers.\n",
    "\n",
    "Optimizers are algorithms that allow you to _optimize_, most often to _minimize_, a function. The optimizer will use the log-likelihood function that we wrote out above, and along with the behavior of a participant, find the best parameter settings that minimize the log-likelihood for this participant. So, it will tell us the values for alpha, beta and w, that it thought the participant's behavior could best be explained by. Isn't that exciting! Note that the optimizer wants to minimize the function, but we want to maximize the loglikelihood. This may seem like a problem at first, but we solve it by calculating the negative log-likelihood: minimizing the negative log-likelhood is the same as maximizing the log-likelihood.\n",
    "\n",
    "This analysis is a form of a parameter recovery analysis. Because we know what values of alpha, beta and w we are putting into the model, we also know what their _optimal_ values should be: identical to the ones we used to generate the data. In essence, we are testing how well our model-fitting technique works.\n",
    "\n",
    "<mark>Before you run the function below, take a look at the help() entry for the optimizer function by running the cell below. If you scroll all the way to the bottom of the help() output in the cell, you can find some examples of using the optimizer on data. While this might look complicated, you can see how we applied this to our model below, and that it's actually quite straightforward to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69eb9218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where ``x`` is an 1-D array with shape (n,) and ``args``\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where 'n' is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr.\n",
      "        If it is a callable, it should be a function that returns the gradient\n",
      "        vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where ``x`` is an array with shape (n,) and ``args`` is a tuple with\n",
      "        the fixed parameters. If `jac` is a Boolean and is True, `fun` is\n",
      "        assumed to return and objective and gradient as an ``(f, g)`` tuple.\n",
      "        Methods 'Newton-CG', 'trust-ncg', 'dogleg', 'trust-exact', and\n",
      "        'trust-krylov' require that either a callable be supplied, or that\n",
      "        `fun` return the objective and gradient.\n",
      "        If None or False, the gradient will be estimated using 2-point finite\n",
      "        difference estimation with an absolute step size.\n",
      "        Alternatively, the keywords  {'2-point', '3-point', 'cs'} can be used\n",
      "        to select a finite difference scheme for numerical estimation of the\n",
      "        gradient with a relative step size. These finite difference schemes\n",
      "        obey any specified `bounds`.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy}, optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "        callable, it should return the  Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "        parameters. LinearOperator and sparse matrix returns are\n",
      "        allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "        for numerical estimation. Or, objects implementing\n",
      "        `HessianUpdateStrategy` interface can be used to approximate\n",
      "        the Hessian. Available quasi-Newton methods implementing\n",
      "        this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Whenever the gradient is estimated via finite-differences,\n",
      "        the Hessian cannot be estimated with options\n",
      "        {'2-point', '3-point', 'cs'} and needs to be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "        Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "        `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "        dimension (n,) and `args` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for L-BFGS-B, TNC, SLSQP, Powell, and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "    \n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform. Depending on the\n",
      "                method each iteration may use several function evaluations.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return. If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken. If bounds are not provided, then an\n",
      "    unbounded line search will be used. If bounds are provided and\n",
      "    the initial guess is within the bounds, then every function\n",
      "    evaluation throughout the minimization procedure will be within\n",
      "    the bounds. If bounds are provided, the initial guess is outside\n",
      "    the bounds, and `direc` is full rank (default has full rank), then\n",
      "    some function evaluations during the first iteration may be\n",
      "    outside the bounds, but every function evaluation after the first\n",
      "    iteration will be within the bounds. If `direc` is not full rank,\n",
      "    then some parameters may not be optimized and the solution is not\n",
      "    guaranteed to be within the bounds.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "    interior point  method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handles complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as many operations.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an `OptimizeResult`\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", :arxiv:`1611.04718`\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bff77",
   "metadata": {},
   "source": [
    "We wrote the parameter recovery for one participant out for you below. You can run it as many times as you want. \n",
    "    \n",
    "<mark>Try to understand what's happening in the script, and if you get stuck, let us know! Try to run this cell several times, and, changing the values for alpha, beta and w that you use to simulate the behavior, see if they also come back out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105e53d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha = 0.1, estimated alpha = 0.1111506712676336\n",
      "True beta = 4, estimated beta = 5.516792750598304\n",
      "True w = 0.5, estimated w = 0.21787710214297687\n",
      "Loglikelihood = -104.15828571058694\n"
     ]
    }
   ],
   "source": [
    "# initialise the number of trials, alpha, beta and w to simulate behavior                              \n",
    "n_trials = 200                                        \n",
    "alpha = 0.1 #--- Fill this in with some good values\n",
    "beta = 4\n",
    "w = 0.5\n",
    "\n",
    "### simulate\n",
    "# simulate behavior for one participant according to the values above\n",
    "[choices, rewards, first_stage_state] = mb_mf_agent(n_trials, alpha, beta, w)\n",
    "\n",
    "### fitting\n",
    "# initial \"guess\" for the parameter values            #--- Fill this in (use some mid-range values for the parameters)\n",
    "# remember that x0[0] is alpha, x0[1] is beta, and x0[2] is w\n",
    "x0 = [0.5, 2, 0.5] # we provide some mid-range values here\n",
    "\n",
    "# bounds for the parameters. For our model, learning rate can never be bigger than 1, or smaller than 0 (else it wouldn't)\n",
    "# make much sense. This is the same for w, it should never be beyond 0 or 1 because it's a mixing weight. While for beta,\n",
    "# it can take any positive value, but a max at 10 is probably sufficient to capture the variation.\n",
    "# Note: we added a small amount of noise in that we start the lower bounds at 0.01. This is to help the optimizer perform\n",
    "# better\n",
    "bnds = ((0.01, 1), (0.01, 10), (0.01, 1)) # parameter bounds (same order as x0)\n",
    "\n",
    "# run the optimizer (minimize)\n",
    "# enter the loglikelihood model, the simulated data\n",
    "# this will output the object 'result' that summarises the results from the optimizer\n",
    "result = minimize(mb_mf_negloglik, x0, args=(choices, rewards, first_stage_state), bounds=bnds, tol=1e-8)\n",
    "\n",
    "### print out our results \n",
    "print(f'True alpha = {alpha}, estimated alpha = {result.x[0]}')\n",
    "print(f'True beta = {beta}, estimated beta = {result.x[1]}')\n",
    "print(f'True w = {w}, estimated w = {result.x[2]}')\n",
    "print(f'Loglikelihood = {-result.fun}') # note the negative sign\n",
    "\n",
    "# result # you can uncomment this if you want to see the full output from the minimize() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42d55cd",
   "metadata": {},
   "source": [
    "Any given run of an optimizer will return a different result, because it runs an algorithmic search through parameter space. The function doesn't know the shape of the function, it's simply trying to find the best value by poking around. Run the model a few times, and see what happens to the loglikelihood. \n",
    "\n",
    "<mark>Compare the loglikelihood for 'parameter solutions' that are relatively close and for some that are relatively far away. What's the difference? How can you explain that?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf539b",
   "metadata": {},
   "source": [
    "## Closer to real life: Simulating behaviour for many agents\n",
    "Until now, we've only been working with a single agent (dataset) at a time. However, in order to run a true recoverability analysis, it is more efficient to simulate many agents (each with different parameter values) and then look at the solutions in terms of correlations between the true and estimated parameter values.\n",
    "\n",
    "Moreover, this also more closely corresponds to analyzing data from human participants, since we normally have at least a few more than one!\n",
    "\n",
    "So now, we run the simulation function above several times using a loop, so we have some more data to work with. You can run the script below to generate the behavioural data for how ever many agents are defined below after (<code>n_agents</code>). \n",
    "\n",
    "We will also use pandas here, which if you are not familiar with, is a nice way of working with larger datasets in Python, similar to structures or tables in Matlab or objects in R. Data comes out in spreadsheet form that we can easily subset and also has columns for labelling the data. We will save both 'metadata' and trial-wise data for each participant. \n",
    "\n",
    "The metadata will consists of the values used to generate that agent (learning rate, beta, w and number of trials as well as including the total rewards won for each agent) while the trial-wise data will consist of rows for each trial per 'participant', indicating what choices they made, if they won a reward or not on that trial, and what spaceship pair (i.e., first-stage state) occured on that trial. (So <code>MetaData</code> will have as many rows as participants, while <code>TrialData</code> will be a large file with number of agents $\\times$ number of trials)\n",
    "\n",
    "For each agent, we are going draw a random value from a uniform distribution for each parameter, so we get some nice variance in their values.\n",
    "\n",
    "Have a look at the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3575b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed at the start\n",
    "random.seed(2)\n",
    "\n",
    "# define how many agents and trials we will use\n",
    "n_agents = 200                         #--- Fill this in\n",
    "n_trials = 200                         #--- Fill this in\n",
    "\n",
    "# set up panda structures to save trial data\n",
    "column_names = [\"Sim_no\", \"Trial_no\", \"Choices\", \"Rewards\", \"First_stage_state\"]\n",
    "AllTrialData = []\n",
    "\n",
    "# and for metadata\n",
    "col_names = [\"Sim_no\",\"Trials\", \"alpha\", \"beta\",\"w\", \"total_rewards\", \"performance\"]\n",
    "\n",
    "# preallocate this for no. of agents\n",
    "MetaData = pd.DataFrame(columns = col_names, index=np.arange(n_agents))\n",
    "\n",
    "# counter which we use as Sim IDs \n",
    "sim_id = 1\n",
    "\n",
    "# loop through agents\n",
    "for a in range(n_agents):\n",
    "       \n",
    "    # here we pick random values for alpha, beta and w from within these distributions\n",
    "    alpha = round(np.random.uniform(0.01,1),2)\n",
    "    beta = round(np.random.uniform(0.01,10),2)\n",
    "    w = round(np.random.uniform(0.01,1),2)\n",
    "    \n",
    "    # generate behaviour\n",
    "    choices, rewards, first_stage_state = mb_mf_agent(n_trials, alpha, beta, w)\n",
    "    \n",
    "    # save trial data\n",
    "    ids = [sim_id]*n_trials\n",
    "    trialno = list(range(1,n_trials+1))\n",
    "    TrialData = pd.DataFrame({'Sim_no': ids, 'Trial_no':trialno,\n",
    "                        'Choices':choices, 'Rewards':rewards, \"First_stage_state\":first_stage_state})\n",
    "    \n",
    "    # append this agent's data\n",
    "    AllTrialData.append(TrialData)\n",
    "    \n",
    "    # save metadata\n",
    "    MetaData.loc[a, \"Sim_no\"] = sim_id\n",
    "    MetaData.loc[a, \"Trials\"] = n_trials\n",
    "    MetaData.loc[a, \"alpha\"] = alpha\n",
    "    MetaData.loc[a, \"beta\"] = beta\n",
    "    MetaData.loc[a, \"w\"] = w\n",
    "    MetaData.loc[a, \"total_rewards\"] = sum(rewards)\n",
    "    MetaData.loc[a, \"performance\"] = str(sum(rewards)*100 / n_trials) + '%'\n",
    "    \n",
    "    # update counter\n",
    "    sim_id += 1\n",
    "    \n",
    "# save and append all trial data\n",
    "AllTrialData = pd.concat(AllTrialData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c7518",
   "metadata": {},
   "source": [
    "Great! We now have a more properly-sized dataset. \n",
    "\n",
    "Let's take a look at <code>MetaData</code> first, which has one row per simulated agent, summarising their initial parameter value settings and their performance. We print it out in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a348c4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sim_no</th>\n",
       "      <th>Trials</th>\n",
       "      <th>alpha</th>\n",
       "      <th>beta</th>\n",
       "      <th>w</th>\n",
       "      <th>total_rewards</th>\n",
       "      <th>performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.29</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>104.0</td>\n",
       "      <td>52.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3.84</td>\n",
       "      <td>0.48</td>\n",
       "      <td>132.0</td>\n",
       "      <td>66.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>0.86</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.61</td>\n",
       "      <td>115.0</td>\n",
       "      <td>57.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.97</td>\n",
       "      <td>107.0</td>\n",
       "      <td>53.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.99</td>\n",
       "      <td>0.44</td>\n",
       "      <td>127.0</td>\n",
       "      <td>63.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>200</td>\n",
       "      <td>0.75</td>\n",
       "      <td>5.36</td>\n",
       "      <td>0.54</td>\n",
       "      <td>115.0</td>\n",
       "      <td>57.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>200</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.26</td>\n",
       "      <td>0.42</td>\n",
       "      <td>107.0</td>\n",
       "      <td>53.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>200</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.96</td>\n",
       "      <td>112.0</td>\n",
       "      <td>56.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.79</td>\n",
       "      <td>114.0</td>\n",
       "      <td>57.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>0.68</td>\n",
       "      <td>5.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>102.0</td>\n",
       "      <td>51.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sim_no Trials alpha  beta     w total_rewards performance\n",
       "0        1    200  0.29  4.83  0.02         104.0       52.0%\n",
       "1        2    200  0.15  3.84  0.48         132.0       66.0%\n",
       "2        3    200  0.86  6.38  0.61         115.0       57.5%\n",
       "3        4    200  0.24   2.2  0.97         107.0       53.5%\n",
       "4        5    200  0.49  9.99  0.44         127.0       63.5%\n",
       "..     ...    ...   ...   ...   ...           ...         ...\n",
       "195    196    200  0.75  5.36  0.54         115.0       57.5%\n",
       "196    197    200  0.12  1.26  0.42         107.0       53.5%\n",
       "197    198    200  0.49   8.2  0.96         112.0       56.0%\n",
       "198    199    200  0.01   8.7  0.79         114.0       57.0%\n",
       "199    200    200  0.68  5.23  0.32         102.0       51.0%\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d41999",
   "metadata": {},
   "source": [
    "When we look at the trial data, you will see that we have many many rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e934283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sim_no</th>\n",
       "      <th>Trial_no</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Rewards</th>\n",
       "      <th>First_stage_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>200</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>200</td>\n",
       "      <td>197</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>200</td>\n",
       "      <td>198</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sim_no  Trial_no  Choices  Rewards  First_stage_state\n",
       "0         1         1      0.0      1.0                0.0\n",
       "1         1         2      0.0      1.0                1.0\n",
       "2         1         3      0.0      1.0                0.0\n",
       "3         1         4      0.0      1.0                0.0\n",
       "4         1         5      0.0      0.0                1.0\n",
       "..      ...       ...      ...      ...                ...\n",
       "195     200       196      1.0      0.0                0.0\n",
       "196     200       197      1.0      1.0                1.0\n",
       "197     200       198      1.0      0.0                0.0\n",
       "198     200       199      1.0      0.0                1.0\n",
       "199     200       200      1.0      1.0                0.0\n",
       "\n",
       "[40000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AllTrialData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4ab80",
   "metadata": {},
   "source": [
    "We can extract the data for individual participants by filtering on the \"Sim_no\" column, which is useful for looping through the data. see below, we extract all 200 trials for Simulation 1 and store it a new variable called <code>participant</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d20f0974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sim_no</th>\n",
       "      <th>Trial_no</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Rewards</th>\n",
       "      <th>First_stage_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1</td>\n",
       "      <td>197</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1</td>\n",
       "      <td>198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1</td>\n",
       "      <td>199</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sim_no  Trial_no  Choices  Rewards  First_stage_state\n",
       "0         1         1      0.0      1.0                0.0\n",
       "1         1         2      0.0      1.0                1.0\n",
       "2         1         3      0.0      1.0                0.0\n",
       "3         1         4      0.0      1.0                0.0\n",
       "4         1         5      0.0      0.0                1.0\n",
       "..      ...       ...      ...      ...                ...\n",
       "195       1       196      0.0      0.0                1.0\n",
       "196       1       197      0.0      1.0                1.0\n",
       "197       1       198      0.0      0.0                1.0\n",
       "198       1       199      0.0      1.0                0.0\n",
       "199       1       200      0.0      0.0                1.0\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participant = AllTrialData.loc[AllTrialData[\"Sim_no\"] == 1]\n",
    "participant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadf3e7",
   "metadata": {},
   "source": [
    "## Fitting the models in a batch\n",
    "Now, we are going to do something exciting! We are going to loop over all our 'participants' and find their best-fitting parameter values. Then, we will make correlation plots, showing how similar the true parameter values are to the estimated parameter values! This will tell us how well our model-fitting technique can measure the behavior in terms of parameter values.\n",
    "\n",
    "We are ready to go. Note: running this will take a little bit of time, but should not take more than a few minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41db6e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant 1/200 - 0.5% complete\n",
      "Participant 2/200 - 1.0% complete\n",
      "Participant 3/200 - 1.5% complete\n",
      "Participant 4/200 - 2.0% complete\n",
      "Participant 5/200 - 2.5% complete\n",
      "Participant 6/200 - 3.0% complete\n",
      "Participant 7/200 - 3.5% complete\n",
      "Participant 8/200 - 4.0% complete\n",
      "Participant 9/200 - 4.5% complete\n",
      "Participant 10/200 - 5.0% complete\n",
      "Participant 11/200 - 5.5% complete\n",
      "Participant 12/200 - 6.0% complete\n",
      "Participant 13/200 - 6.5% complete\n",
      "Participant 14/200 - 7.0% complete\n",
      "Participant 15/200 - 7.5% complete\n",
      "Participant 16/200 - 8.0% complete\n",
      "Participant 17/200 - 8.5% complete\n",
      "Participant 18/200 - 9.0% complete\n",
      "Participant 19/200 - 9.5% complete\n",
      "Participant 20/200 - 10.0% complete\n",
      "Participant 21/200 - 10.5% complete\n",
      "Participant 22/200 - 11.0% complete\n",
      "Participant 23/200 - 11.5% complete\n",
      "Participant 24/200 - 12.0% complete\n",
      "Participant 25/200 - 12.5% complete\n",
      "Participant 26/200 - 13.0% complete\n",
      "Participant 27/200 - 13.5% complete\n",
      "Participant 28/200 - 14.0% complete\n",
      "Participant 29/200 - 14.5% complete\n",
      "Participant 30/200 - 15.0% complete\n",
      "Participant 31/200 - 15.5% complete\n",
      "Participant 32/200 - 16.0% complete\n",
      "Participant 33/200 - 16.5% complete\n",
      "Participant 34/200 - 17.0% complete\n",
      "Participant 35/200 - 17.5% complete\n",
      "Participant 36/200 - 18.0% complete\n",
      "Participant 37/200 - 18.5% complete\n",
      "Participant 38/200 - 19.0% complete\n",
      "Participant 39/200 - 19.5% complete\n",
      "Participant 40/200 - 20.0% complete\n",
      "Participant 41/200 - 20.5% complete\n",
      "Participant 42/200 - 21.0% complete\n",
      "Participant 43/200 - 21.5% complete\n",
      "Participant 44/200 - 22.0% complete\n",
      "Participant 45/200 - 22.5% complete\n",
      "Participant 46/200 - 23.0% complete\n",
      "Participant 47/200 - 23.5% complete\n",
      "Participant 48/200 - 24.0% complete\n",
      "Participant 49/200 - 24.5% complete\n",
      "Participant 50/200 - 25.0% complete\n",
      "Participant 51/200 - 25.5% complete\n",
      "Participant 52/200 - 26.0% complete\n",
      "Participant 53/200 - 26.5% complete\n",
      "Participant 54/200 - 27.0% complete\n",
      "Participant 55/200 - 27.5% complete\n",
      "Participant 56/200 - 28.0% complete\n",
      "Participant 57/200 - 28.5% complete\n",
      "Participant 58/200 - 29.0% complete\n",
      "Participant 59/200 - 29.5% complete\n",
      "Participant 60/200 - 30.0% complete\n",
      "Participant 61/200 - 30.5% complete\n",
      "Participant 62/200 - 31.0% complete\n",
      "Participant 63/200 - 31.5% complete\n",
      "Participant 64/200 - 32.0% complete\n",
      "Participant 65/200 - 32.5% complete\n",
      "Participant 66/200 - 33.0% complete\n",
      "Participant 67/200 - 33.5% complete\n",
      "Participant 68/200 - 34.0% complete\n",
      "Participant 69/200 - 34.5% complete\n",
      "Participant 70/200 - 35.0% complete\n",
      "Participant 71/200 - 35.5% complete\n",
      "Participant 72/200 - 36.0% complete\n",
      "Participant 73/200 - 36.5% complete\n",
      "Participant 74/200 - 37.0% complete\n",
      "Participant 75/200 - 37.5% complete\n",
      "Participant 76/200 - 38.0% complete\n",
      "Participant 77/200 - 38.5% complete\n",
      "Participant 78/200 - 39.0% complete\n",
      "Participant 79/200 - 39.5% complete\n",
      "Participant 80/200 - 40.0% complete\n",
      "Participant 81/200 - 40.5% complete\n",
      "Participant 82/200 - 41.0% complete\n",
      "Participant 83/200 - 41.5% complete\n",
      "Participant 84/200 - 42.0% complete\n",
      "Participant 85/200 - 42.5% complete\n",
      "Participant 86/200 - 43.0% complete\n",
      "Participant 87/200 - 43.5% complete\n",
      "Participant 88/200 - 44.0% complete\n",
      "Participant 89/200 - 44.5% complete\n",
      "Participant 90/200 - 45.0% complete\n",
      "Participant 91/200 - 45.5% complete\n",
      "Participant 92/200 - 46.0% complete\n",
      "Participant 93/200 - 46.5% complete\n",
      "Participant 94/200 - 47.0% complete\n",
      "Participant 95/200 - 47.5% complete\n",
      "Participant 96/200 - 48.0% complete\n",
      "Participant 97/200 - 48.5% complete\n",
      "Participant 98/200 - 49.0% complete\n",
      "Participant 99/200 - 49.5% complete\n",
      "Participant 100/200 - 50.0% complete\n",
      "Participant 101/200 - 50.5% complete\n",
      "Participant 102/200 - 51.0% complete\n",
      "Participant 103/200 - 51.5% complete\n",
      "Participant 104/200 - 52.0% complete\n",
      "Participant 105/200 - 52.5% complete\n",
      "Participant 106/200 - 53.0% complete\n",
      "Participant 107/200 - 53.5% complete\n",
      "Participant 108/200 - 54.0% complete\n",
      "Participant 109/200 - 54.5% complete\n",
      "Participant 110/200 - 55.0% complete\n",
      "Participant 111/200 - 55.5% complete\n",
      "Participant 112/200 - 56.0% complete\n",
      "Participant 113/200 - 56.5% complete\n",
      "Participant 114/200 - 57.0% complete\n",
      "Participant 115/200 - 57.5% complete\n",
      "Participant 116/200 - 58.0% complete\n",
      "Participant 117/200 - 58.5% complete\n",
      "Participant 118/200 - 59.0% complete\n",
      "Participant 119/200 - 59.5% complete\n",
      "Participant 120/200 - 60.0% complete\n",
      "Participant 121/200 - 60.5% complete\n",
      "Participant 122/200 - 61.0% complete\n",
      "Participant 123/200 - 61.5% complete\n",
      "Participant 124/200 - 62.0% complete\n",
      "Participant 125/200 - 62.5% complete\n",
      "Participant 126/200 - 63.0% complete\n",
      "Participant 127/200 - 63.5% complete\n",
      "Participant 128/200 - 64.0% complete\n",
      "Participant 129/200 - 64.5% complete\n",
      "Participant 130/200 - 65.0% complete\n",
      "Participant 131/200 - 65.5% complete\n",
      "Participant 132/200 - 66.0% complete\n",
      "Participant 133/200 - 66.5% complete\n",
      "Participant 134/200 - 67.0% complete\n",
      "Participant 135/200 - 67.5% complete\n",
      "Participant 136/200 - 68.0% complete\n",
      "Participant 137/200 - 68.5% complete\n",
      "Participant 138/200 - 69.0% complete\n",
      "Participant 139/200 - 69.5% complete\n",
      "Participant 140/200 - 70.0% complete\n",
      "Participant 141/200 - 70.5% complete\n",
      "Participant 142/200 - 71.0% complete\n",
      "Participant 143/200 - 71.5% complete\n",
      "Participant 144/200 - 72.0% complete\n",
      "Participant 145/200 - 72.5% complete\n",
      "Participant 146/200 - 73.0% complete\n",
      "Participant 147/200 - 73.5% complete\n",
      "Participant 148/200 - 74.0% complete\n",
      "Participant 149/200 - 74.5% complete\n",
      "Participant 150/200 - 75.0% complete\n",
      "Participant 151/200 - 75.5% complete\n",
      "Participant 152/200 - 76.0% complete\n",
      "Participant 153/200 - 76.5% complete\n",
      "Participant 154/200 - 77.0% complete\n",
      "Participant 155/200 - 77.5% complete\n",
      "Participant 156/200 - 78.0% complete\n",
      "Participant 157/200 - 78.5% complete\n",
      "Participant 158/200 - 79.0% complete\n",
      "Participant 159/200 - 79.5% complete\n",
      "Participant 160/200 - 80.0% complete\n",
      "Participant 161/200 - 80.5% complete\n",
      "Participant 162/200 - 81.0% complete\n",
      "Participant 163/200 - 81.5% complete\n",
      "Participant 164/200 - 82.0% complete\n",
      "Participant 165/200 - 82.5% complete\n",
      "Participant 166/200 - 83.0% complete\n",
      "Participant 167/200 - 83.5% complete\n",
      "Participant 168/200 - 84.0% complete\n",
      "Participant 169/200 - 84.5% complete\n",
      "Participant 170/200 - 85.0% complete\n",
      "Participant 171/200 - 85.5% complete\n",
      "Participant 172/200 - 86.0% complete\n",
      "Participant 173/200 - 86.5% complete\n",
      "Participant 174/200 - 87.0% complete\n",
      "Participant 175/200 - 87.5% complete\n",
      "Participant 176/200 - 88.0% complete\n",
      "Participant 177/200 - 88.5% complete\n",
      "Participant 178/200 - 89.0% complete\n",
      "Participant 179/200 - 89.5% complete\n",
      "Participant 180/200 - 90.0% complete\n",
      "Participant 181/200 - 90.5% complete\n",
      "Participant 182/200 - 91.0% complete\n",
      "Participant 183/200 - 91.5% complete\n",
      "Participant 184/200 - 92.0% complete\n",
      "Participant 185/200 - 92.5% complete\n",
      "Participant 186/200 - 93.0% complete\n",
      "Participant 187/200 - 93.5% complete\n",
      "Participant 188/200 - 94.0% complete\n",
      "Participant 189/200 - 94.5% complete\n",
      "Participant 190/200 - 95.0% complete\n",
      "Participant 191/200 - 95.5% complete\n",
      "Participant 192/200 - 96.0% complete\n",
      "Participant 193/200 - 96.5% complete\n",
      "Participant 194/200 - 97.0% complete\n",
      "Participant 195/200 - 97.5% complete\n",
      "Participant 196/200 - 98.0% complete\n",
      "Participant 197/200 - 98.5% complete\n",
      "Participant 198/200 - 99.0% complete\n",
      "Participant 199/200 - 99.5% complete\n",
      "Participant 200/200 - 100.0% complete\n"
     ]
    }
   ],
   "source": [
    "# get all ids\n",
    "participants = pd.unique(MetaData[\"Sim_no\"])\n",
    "\n",
    "# pre-allocate our output file, which will have the real and the outputted parameters\n",
    "col_names = [\"Sim_no\",\"real_alpha\", \"est_alpha\", \"real_beta\",\"est_beta\", \n",
    "             \"real_w\", \"est_w\",\"loglik\"]\n",
    "\n",
    "# preallocate this for no of agents\n",
    "Param_recov = pd.DataFrame(columns = col_names, index=np.arange(participants.size))\n",
    "\n",
    "\n",
    "for i, pp in enumerate(participants):\n",
    "    \n",
    "    # to let us know progress\n",
    "    print(f'Participant {pp}/{len(participants)} - {pp*100/len(participants)}% complete')\n",
    "\n",
    "    # we extract the trial data of one participant (in our case, a sim) at a time\n",
    "    participant = AllTrialData.loc[AllTrialData[\"Sim_no\"] == pp]\n",
    "    \n",
    "    # import real parameter values for this participant\n",
    "    real_alpha = MetaData.loc[i,\"alpha\"]\n",
    "    real_beta = MetaData.loc[i,\"beta\"]\n",
    "    real_w = MetaData.loc[i,\"w\"]\n",
    "     \n",
    "    # import trial data for the this participant\n",
    "    choices = participant[\"Choices\"]\n",
    "    rewards = participant[\"Rewards\"]\n",
    "    first_stage_state = participant[\"First_stage_state\"]\n",
    "    \n",
    "    # initial \"guess\" for the parameter values\n",
    "    # remember that x0[0] is alpha, x0[1] is beta, and x0[2] is w\n",
    "    x0 = [0.5, 2, 0.5] # we provide some mid-range values here\n",
    "\n",
    "    # bounds for the parameters. For our model, learning rate can never be bigger than 1, or smaller than 0 (else it wouldn't)\n",
    "    # make much sense. This is the same for w, it should never be beyond 0 or 1 because it's a mixing weight. While for beta,\n",
    "    # it can take any positive value, but a max at 10 is probably sufficient to capture the variation.\n",
    "    # Note: we have set the lower bounds at 0.01 and not 0, because we found that helps the optimizer perform\n",
    "    # better.\n",
    "    bnds = ((0.01, 1), (0.01, 10), (0.01, 1)) # parameter bounds (same order as x0)\n",
    "    \n",
    "    # run the optimizer (minimize)\n",
    "    # enter the loglikelihood model, the simulated data\n",
    "    # this will output the object 'result' that summarises the results from the optimizer\n",
    "    result = minimize(mb_mf_negloglik, x0, args=(choices, rewards, first_stage_state), bounds=bnds, tol=1e-8)\n",
    "    \n",
    "    # if you want to see the estimated and true parameters printed out live, you can uncomment this\n",
    "#     print(f'True alpha = {real_alpha}, estimated alpha = {result.x[0]}')\n",
    "#     print(f'True beta = {real_beta}, estimated beta = {result.x[1]}')\n",
    "#     print(f'True w = {real_w}, estimated w = {result.x[2]}')\n",
    "#     print(f'Loglikelihood = {-result.fun}\\n\\n')\n",
    "    \n",
    "    # index for saving the data\n",
    "    pos = pp - 1\n",
    "                \n",
    "    # save data\n",
    "    Param_recov.loc[pos, \"Sim_no\"] = pp\n",
    "    Param_recov.loc[pos, \"real_alpha\"] = real_alpha\n",
    "    Param_recov.loc[pos, \"est_alpha\"] = result.x[0]\n",
    "    Param_recov.loc[pos, \"real_beta\"] = real_beta\n",
    "    Param_recov.loc[pos, \"est_beta\"] = result.x[1]\n",
    "    Param_recov.loc[pos, \"real_w\"] = real_w\n",
    "    Param_recov.loc[pos, \"est_w\"] = result.x[2]\n",
    "    Param_recov.loc[pos, \"loglik\"] = -result.fun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e90342",
   "metadata": {},
   "source": [
    "Let's take a look at the data we have created. As you can see, this new dataset has the true and estimate parameter values for $\\alpha$, $\\beta$, and $w$, as well as the log-likelihood of the best fitting solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb24411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sim_no</th>\n",
       "      <th>real_alpha</th>\n",
       "      <th>est_alpha</th>\n",
       "      <th>real_beta</th>\n",
       "      <th>est_beta</th>\n",
       "      <th>real_w</th>\n",
       "      <th>est_w</th>\n",
       "      <th>loglik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.326514</td>\n",
       "      <td>4.83</td>\n",
       "      <td>4.237949</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-100.878587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.153047</td>\n",
       "      <td>3.84</td>\n",
       "      <td>3.461278</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.701544</td>\n",
       "      <td>-124.752398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.861929</td>\n",
       "      <td>6.38</td>\n",
       "      <td>6.412792</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.544343</td>\n",
       "      <td>-57.173623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.313107</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.651375</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.620837</td>\n",
       "      <td>-119.675187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.501744</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.779225</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.360276</td>\n",
       "      <td>-35.532169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sim_no real_alpha est_alpha real_beta  est_beta real_w     est_w      loglik\n",
       "0      1       0.29  0.326514      4.83  4.237949   0.02      0.01 -100.878587\n",
       "1      2       0.15  0.153047      3.84  3.461278   0.48  0.701544 -124.752398\n",
       "2      3       0.86  0.861929      6.38  6.412792   0.61  0.544343  -57.173623\n",
       "3      4       0.24  0.313107       2.2  2.651375   0.97  0.620837 -119.675187\n",
       "4      5       0.49  0.501744      9.99  9.779225   0.44  0.360276  -35.532169"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Param_recov.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09238f35",
   "metadata": {},
   "source": [
    "### Visualise the parameter recovery results\n",
    "In order to assess how well we can recover the \"real\" parameter values from the data, we can inspect scatter plots that plot the real and estimate values against each other for all 'participants'. If our model-fitting technique works well, we should see fairly strong correlation between the \"real\" and estimated values, indicating that we can indeed recover parameter values from behavior.\n",
    "\n",
    "So, let's plot the scatterplots for each of the parameters now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a3a4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABZgAAAI4CAYAAADnFoykAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAEAAElEQVR4nOzdeXxcVf3/8deHNGlDIZQl7JCUfamytSw/VEASZBELqEhQ1lHxq5VFcEdZREERQUUENcgiRASRVgGBsgoqtqVgWQUhQco2LA0QQrP0/P44d9rJdGYy+70z834+HvOY5M6de8/cTObM+dzP/RxzziEiIiIiIiIiIiIikq9Vwm6AiIiIiIiIiIiIiFQnBZhFREREREREREREpCAKMIuIiIiIiIiIiIhIQRRgFhEREREREREREZGCKMAsIiIiIiIiIiIiIgVRgFlERERERERERERECqIAs4iIiIiISBHM7Aozc2Z2ZthtEREREak0BZhFREQk8pKCN6m3t8zsYTM738w2DrudtcDMzgxuU8Jui0gt0P+UiIiI1LoJYTdAREREJA/DwBvBzwa0AjsEt8+a2cHOufvDalyNOCO4vwJYEl4zRGqG/qdERESkpimDWURERKrJ351z6we39YDVgKPxQZspwPVm1hxi+0REREREROqKAswiIiJStZxz7zrnrgZODBatDxwSXotERERERETqiwLMIiIiUgv+ACwLft4FwMwazGwfM/upmS0ws1fMbMjMXjSzP5nZhzNtzMzuCWo8H2tmU8zsh2b2pJm9a2ZLktZby8yOMbM/Bo+/bWYDZva4mf3EzDbMsP32RB3p4PddzWy2mcWDbfzdzA5MWr/JzL5uZo8GbXjFzC4zs7WyHRQzm2Zml5vZc2b2npktMbMHzOwLZtaYsu4VifYEnkupd31Fmu23mtm5ZrbIzN4JXvujZvb9TG0zs95ge3ub2UZmdomZPWtmS83s4WyvJ2kbxwbbuCf4/dNmdq+ZvR4sPyRl/Q+Y2e/N7IVgP6+b2Vwz6zIzy7IfM7NPmdnNZvZy8NzFZnafmZ1iZmunec4qZhYL2vNGcNyfM7NfmdkWadY/PWjz/HFec1ew3qtmtlKZu3xfY3D8nZn1Br8fYGa3BttfZmYnB+8dZ2Y3jNO2s4L1/p5tvZTnnJl4XwXHbJaZ/St4jzoz2zFl/YOD/5GXzf8fv2pmfzazj2TZxw5mdlXwnltq/n/rWTP7a/D6Vk1ZP/Feb8+wvTH/tzm8xrz+p8xsppndYv7/ezh4/zxlZj1m9qlc9ikiIiISBtVgFhERkarnnFtqZq8B6wItweJtgbuSVlsKDAEb4LOcDzGzbzvnfpBl063AAmCzpOcn+xZwatLvbwHNwb63BT5jZh3OuX9n2oGZfQy4Af+97C182Y89gD+b2RHAn4Fbgb2B9wAXvM7PAzPMbHfnXGq7MLNZwE9ZkVAwEGz7/wW3T5nZQc65d4PH+4FXgPWC318DRpM22Z+y/Q8As4FEIHkoWH/74HaUmXU6557K8NK3Aq4H1gHexdfXzpuZ/Qz4Mv4EQz8rTjQkHv8h8LWkRW/jy6nsG9w+Zmafds6lPm8N/N+lI1jkgu2vD2wIfBB4E19XN/GcVYE/AfsFi4aD19YOfA5/TI5wzs1O2tW1wPeAXcxsK+fcfzK81K7g/g/OuZFSvMak558K/DjpNSbW+w1wHHCwma3tnHs9zXMNOCb49fIMbc/GgBuBmfj3z9sp228Efgt8OmnxW/j/zY8CHzWz851zX0t53oHATUDiRMrS4HVNDW4fAf4KPFlAm3OV8/+UmX0f/3mS8Db+s2Sr4LYPcF0Z2yoiIiJSMGUwi4iISNUzX3e5Nfh1SXA/hA9gHowPCjY751bDB3u+gw/0nGNmu2XZ9HfxAaoDgFWdcy3A9KTHFwPnATsDqzvn1gAmBuvcFrTp2nQZpEmuCm4bOOem4IPHs/Hf0y7EB/62wQfTVgNWxwfj3gZ2Aj6b5njMBH4ODOKDVusFr70ZH/x8Ch+wvjDxHOfcSc659ZM2MyOp3vX6zrmTkrbfhg98r4UPQm4TbHsyMA0fuNsEuNHMGjK87guAl4A9nXOTg/Z9IstxSmcXYBZ+ErW1nXNrAWsCfw/aeRI+8BoHvgisGfwNJwOHB/s/Avh6mm1fgw8uDwInAWs559YMXuf7gLPxAeZkP8Ef36XAF/DviSnA1sA9wCT8+2GrxBOcc88CDwa/dpGGma2JD4iCD0gnP1bMawT///BD4BL8e3BN/PvsBufc34HHgSbGBniT7Qu04U9gFBIAPQzYP2h7S7D/9YBng8d/FOy7FziSFf9nqwMn4IPNXzWz1GP3c/z/7l+ArZ1zk4LnrQF8CPg1/oRN2eT6PxVkTH8jWOdcoNU51+Kca8Yfi08AN5ezrSIiIiJFcc7ppptuuummm266RfqGzxJ1wD0ZHp8VPO6Aj+e4ze8E6/82zWP3BI8NAdMKbPNE4LFgO3ulPNae1N670jx3Mj670aV7fkr770pZ3oAPxjng0Axtmwq8g8+w3SDlscQ+27O8tt8F6/w0w+NNwMPBOp9IeSzRtjfxge9Cju2xSe38QYZ1puCD8MPArhnW2R2f1foG0JS0/MBg28uA/XNsUxv+pIUDTkjz+KrAM8HjV6U8dmKw/MkM2/5s8PhzgJXoNe6ddAyvzfK6TgnWWZjh8WuDx6/I8294ZtL+P59hnS2DY/omsFmGdQ4PtvFo0rJ1k7ad83tsvPc+Sf+3aR67InjszAK2m3gNTxTy/6CbbrrppptuuukW9k0ZzCIiIlKVzGs3s9PwWY4AffjM2lwk1tszyzq3OuceLaR9zrmlwB057OO8NM8dAP4Z/Pp359y9aZ53Z3A/LWX53vhgZ69z7k8Z2vZcsP0Jwfo5C7LFPxn8+pMM2x/Cl5cA6Mywqaucc6/ks+80RjO1Afg4PhP3fufcv9Kt4Jz7Jz5Tdk2C2t2Bo4P725xzf82xLYfhs85fxmd1p+7rXVa8Tw9Lyey+Dv9atjazndNsO5Gd2+OcS67pW8xrTHZ+huXgs+uHgB3NbKfkB4IyIocGvxZSHgPg9SzPPRp/TG9yPtM7nRvxGePbm9kGwbK3WVHmY4O0z4qWt4L7NVLrQouIiIhUA9VgFhERkWqyV5YJtl4CDnFJ9YiDYOgX8CUltsMH2VK//6SdiC/wj/EaZGbb4DOoP4TPcFwNX1c2130syrD81eA+U4A7EZxdM2X5/0vs08xezrLfNYL7TbKsk850fIYywINZqn80j7P9cY9tDp5xzr2W4bHEcdhtnOOQqCG9SVKbdg/ub8mjLYnA8N+cc6MZ1knUBJ+ML5vxOIBz7hUzuwsfjD8SeCjxhCBounfw65jyGBT3GhMGgUcyPdE597qZ3YTPsj0OWJj08JH4sh9PO+fuy7L/bOa7lJrSSRKv7xNmdkCWbSTqLG8CvOScGzSze/F1i28zs5/jS2UsyvK3CdOD+AzzDYB/mNkvgDuCE0EiIiIikacAs4iIiFSTYXwgBvwl5QP47Mw7gN8455bXxA0Cc/fgJ8hKGMBfbr8MX0piHXywL5N4tsYEk/BdxYoAV2KiuaXB76sF28+4D+fcSxkeSgTCxns89ftcImOziRWTi2WTb8ZkckZoMdvPemxzlG0biXY2syLYnU1yOxOv6/k82pKoAb44yzovpFk/4Vp8gPlTZvbVpEzlT+GzeBelyaYv5jUmvO4yTP6X5Df4APORZnZa0kmc44P73+aw70xy+RuuFtzGk/z6PosPKm+Ln0Txe8A7ZnYf0AP8Pktgu6Kcc2+a2VH4ut/vBy4DCE4a3A5cnuEqBhEREZFIUIkMERERqSZ/dysmyNrAObeFc24/59z5ycHlwEX44PKz+FICaznnVnPOrev8xFu7M76M2Y5m1oqfKKwRX+JgOjDJObdmoo2smEQv2yR/pZb4fvcn55zlcDuzwO2/meP2986wnVJkkmbbRqKdF+bYzitK0B7wtbczyZR9D77Uw3vAxvhs+IREeYzU7GUozWvM5e8wF1//eW3gYwBmtj3+PT8KXJnDNjLJ5W94Uo6v757EE4OSGu/Hl/D4FfAEPkh9IHA1Pvs+l6B1RTjnbsFfAfF54A/Ai/jJSY8G7jGzX4XXOhEREZHsFGAWERGRmmNmTfiyGACfds7dmCYAnUv2bTYH4ANWjwNHOucWOOeGS7yPQiRKZ2xX5u2vaWbrl2kfpVDMcUg8ty2P5yQycbM9J7lcyJjMXefcW8DNwa9dAGa2GbArPjDdk6Wd5fpbJ9rmWFEn+bjgPhbc3+ace7FMuy7q9TnnRpxzNznnTnDObYfPiP4qPpC/M3BGylMSwe5JGTa5RoblJeGc63fO/do59ynn3EbA9viTWACfM7ODyrl/ERERkUIpwCwiIiK1aB1WZJIuzLBOR5H72Di4/3e6EgPmixN/uMh9FCJRY3frIMs0X4ks20xZ1/OBRGmBwwrYfqUkjsNeZrZ2ns9NTLB4YB7PSdRN3i3LRG2J98MA8FSaxxNZyp8ws0ZWZC//3TnXl2b9Yl5jvn6LD8B+xMzagM8Eywud3C8Xidd3cHA8iuKce9k592P81Q0Ae6WssiS435j0ZhS66+A+rysZnHOPO+c+z4r3Y2p7RURERCJBAWYRERGpRW+xIqjzvtQHg/rMXy5yH/3B/TRLP9Pd54DNi9xHIe5kRe3gC82sIdOKZpY6QSD4YwcwJd1znHNvA38Mfj3dzDJmaZvZhBDLEFyPD+ROAs7PtmKa43BVcL+fme2f4/5uxNfgXhtf5iB1H6vis2cBbsww2dzN+PfV2sB+ZC+PAcW9xrw45xYDt+Jrl1+DryEdB+YUs91xXIk/phsC38y2YvLrM7PGDP+TCYPBfWo5k8SEmzNTlmNmE4GTx2lvJln/p4IrLrLJ1F4RERGRSFCAWURERGqOc+4dVmT9XW5mOwKY2Spmti9wL8XXRZ6LD2JPA35mZlOCfbSY2VeBXwCvF7mPvAVlOr4ctK0TuN3MdksE3IKg7y5mdh6+PnWqx4L7o7MEp7+Bn2xxA+DvZnZoEIAj2McWZnYyvu7t9FK8rnw5515nRVDyODP7g5lNSzxuZpPM7ANm9gvggZSn3xrcDPijmX056e/bZGbvM7MLzOyQpP314Wv9ApxnZp9PHBMz2wofPN4CeBc4J0Obl+ID1QBn40skjOADyaV+jYX4TXC/Z3D/uzRlYUrGOfcEK7KNzzKzXwRlQwAws9XMrNPMrmbsMdoeeNTMTjazrZLe+41m9nHgK8F6t6Xs8g/B/efM7Likv9/2wC34QHchxvuf+j8zu83MjgxOfiVe3xQz+xawd4b2ioiIiESCAswiIiJSq07BZ/69D1hoZu8A7+ADw2uzooZsQZxzT7Ei+DULeNPM3sAHXn+EzyS+tJh9FNG2OfjXN4Qvy/BP4F0zew1ff3Y+8HXSZ1QmgognA++YWZ+Z9ZrZj5O23wvsj5+IbDN8UPQdM3vNzN4DnsZPcLgF2Se2Kyvn3M+B7wRt+CSwyMwGgr/TAPA34ItAc8rzHHAk/kTEqsDPgNeD5w0C/8YHKaek7PJU4A58pullwNtm9ia+HMbewFJ8ve7/ZGl2Ilt55+D+DudcPNPKhb7GAt0MvJT0eznLYyR8Dfhl8PMXgf+a2VvBcX0LuB1friM1cLsd/j34FDBoZq/j3/s34Gspz2flQP9vgAfxf7/L8e/pfuBRYEdW1J/O13j/U4bPWL8GeNHM3gle35vA94PHfxVMBCgiIiISOQowi4iISE1yzj0I7AHchA/UNAKv4gN/OwKPlGAfX8GXQ1iIDx5OAB7GB5IOYkWt4opzzv0W2BofBH8saMsa+Kzqu4HTgPYMz/sc8K/gOZvgJ65bJ2W9ecA2+ED134G38QHXQXzw7ofADOfcvSV+aXlxzp0D7IDPLn4aH6ybjA+U3gr8H7BbmuctwQfnj8GflHgDP6njS/jA88mklIdwzr2Ln/zxs/jA7rv4AHUfPsj4Pufc7HGafBdjg7iZymMU/Rrz5ZwbAf4c/DrPOfdosdvMYZ+jzrkvAh8Afoc/lk34gPnzwJ/wf6NDkp72BPAJ/Amehfjayi34gPT9+Az/PYOJFZP3NYzP+j8f6MWX5xgArgB2ocDPjBz+p64NHr8uaPswK95rc4CZzrkTCtm3iIiISCWYT9AQERERERHJzsz+A2wJ/J9zLpQMfRERERGJFgWYRURERERkXEH98rn4rN4NUzOARURERKQ+qUSGiIiIiIhkZWbr4EtHAFyu4LKIiIiIJCiDWURERERE0gomojscWB9fx/w1YHvn3KuhNkxEREREIkMZzCIiIiIiksk6+EnpBoHbgQ8ruCwiIiIiyZTBLCIiIiIiIiIiIiIFUQaziIiIiIiIiIiIiBREAWYRERERERERERERKYgCzCIiIiIiIiIiIiJSEAWYRURERERERERERKQgCjCLiIiIiIiIiIiISEEUYBYRERERERERERGRgijALCIiIiIiIiIiIiIFUYBZRERERERERERERAqiALOIiIiIiIiIiIiIFEQBZpE6Y2a9ZnZP2O0QERERERGpRWZ2rJk5M9u7wOfvHTz/2BzXv8fMegvZV1SYWXvwms8scjvOzK4oTatKvx8zOzN4bnvpWxVdub5H833vS3QowCySg6QPudPCbouMFXx5OznsdoiI1AL1d9kFg98zzWzHsNtSLcxsx+CYtYfdFhGpD0l9mTOzizOss66ZDQXr3FPhJoqI1BwFmEXqz9bAfmE3ooSOBU4OuQ0iIlIf2oEzgB3DbUZV2RF/zNrDbYaI1KH3gCPNbGKax44CDBipbJOkhjQDnyvgeecEz+0rbXNqxn3443N12A2R/CjALFKlzKzRzCbl+zzn3FLn3FA52lQMM2sws1XDboeIiFQPM1s97DbUkyge7yi2SUQi40/AmsDMNI8dB9wCLK1oi6RmOOfec84NF/C8keC5rhztqnbOuWXB8RkNuy2SHwWYRUrMzLY0s6vN7KXgsqteMzvfzCanrLeNmV1iZo+Z2dtm9q6ZLTCzlc6CJtVp2t7MfmJmL+DPyO+eVN/rw2Z2mpn918yWmtl/zOyYNNtaqQZzYlnQppuD9vSb2Q1mtn6abbzfzG43swEze93MrjSzdXKtQ5XU5g4z+46Z/Td4PYcHj+9nZteZ2bNmNmhmS4L97ZXabmAvoC3pMrgxtc5y/XuIiEh6yTURzeyjZjbPzN4LPlfPN7MJSeteF3zWrpNmO1sH27koZfmnzOz+pL7wQTP7RJrnOzO7wsz2DdZ/B/hz8NhaZnZh0Ae+F/RNC8zsq2m2k9P+0jzvWODu4NffJvU59yStY2b2f8G+3w32cbeZ7ZPlmB5uZg8H/d0zZnZcsM6mQT/8RrCd31lKMDU4Hs7MWs3squB1D5jZnWa2U4bXUYrjvaGZXRC0+83gmD9uZl83s4akbZwJ/Db49e6kY3ZF4nHLUIfS0n9fydim4PHpZvYnM3vN/Hehp8zs28nvURGpKw8Bj+CDycuZ2a7A9qz4fFqJmR1iZg+Y2TvB7QEzSxeoxsw+a2ZPBp87z5jZSfjs6HTrrmFmPwzWW2pmcTPrMbPNCn6VY7e/mZnNNj+Weyv4TNwsZZ1Vgs/G+8zsZfP99vNm9kszWzvNNo82s3+ZH5MNmB+jXWNmrSnr5TzuMrMPBMd00MxeMV/KZLU8X+v2ZvbXoE1vBP3kulnWz7n/N7N9zI+LXw/6uGfNrNuSvt9YmrGvmR1kZvcG/dBgcFxvNLOtktZJ2/eZ/25wdXA8lpr/TvMDS0nCSnr+1sHjLwTrP2JmB+Z47BLj8X3N7Ltm1he090Ez2z1YZ6/geA0Ef9PvpNlOTuP2LO1Y28z+Ebxf9w2WrVSDOXmZmR1nPoayNGj31zJs+//Mfw94z3xsZJYVWRtdstOXLZESMrNdgLuAJcBlwGJgB+BEYE8z2yvpLOfewIeAvwDPAZOBTwK/MrN1nHPnptnFNcAgcAHggJdYccnpD/CXklyGPxP/f8AVZvaMc+6BHJq/EXAP/kz/V4N2nwC0kFRSw8y2BP6GP0H1s+A1HgjcmsM+Uv0YaAR+DbwFPBUsPxZYC7gKeCFo22eBO81sH+fc34L1TgbOBdYBTkna7hNBW/P5e4iISHYHAl8ELgUux2eEnQa8ie+DAK7Enyw8Akite3l00joAmNk5wLeBvwLfAZYBhwLXm9ks59wvUrYxHfg4vt+4Mmn59fg+9TJ8MGFVYBt8X3t+EftLdl/wOr8F/ArfFwK8krTO1UAXcAM+cDER+DRwh5kd5pybk7LNjwJfAC4B3gBiwOVmNhTs665gfzOA4/EnYz+bpm1/DZ5/JrA+MAu4z8z2cM49WsTrz3S83w8chv/O8F98X34AcB6wGf77A8CNwAbA54PX80Sw/L9pXkOu0rYpGFT/CXgG/z3pDWAP4Gx8mY5PFrFPEalevwV+YmYbO+deCJYdD7yKH4etxMy+CPwCeBJfzsDhxyc3mdkJzrlfJa17MnAhvu/5Fr7/+Wqw/dTtrgH8HdgU348+hv+M/CLwoJlNd84VUzZhMv5E6L+AbwJbBtve3cx2cs69HKzXFLTxj8BsYADfz8SAD5jZLokrXs3sM/jP2r8B38WPRTfFf+avC8SD9XIed5nZbsBc4G3gh8FzjsCP/XJiZlODNk3Ef9/4H3Awvn9Lt37O/Z+ZnQD8MngNv8SXstg02P7GwGsZ9rEXMAdYhB+jLgE2BDqALYD/ZHk9bfi/2xrBPv+D/w7zTfzx29c5l1rO5UpgGD+mbsKPjW8ys62cc72Z9pXiPKAB+GmwjVOB28wnqnXjv+9cg/9ud7aZPeec+13S848lt3F7utfcDtwGrA7s5Zx7OIf2fgFYL2jbEuAzwA/N7AXn3LVJ2/568NoeYuz/ZTyHfUihnHO66abbODf8h7sDThtnvUfwX0RWT1l+aPD8Y5OWTU7z/FXwQd5+oDFp+ZnB8+8BJqQ859jgsYVAU9LyjfCB5p6U9XuBe9Isc8DhKct/ESzfJmnZH4Jle6ase12w/IocjmeizU8Bq6Z5PN2xWQ/fmd+SsvweoLfYv4duuummm27p+zv8iUyHH4C2Jy034FHgpaRlDfiTn/9K2a7hB2j/Tlq2c7DdH6Rpx034E4+rJy1zwa0jZd01guWXjPPa8trfOMfn2DSPJfqWz6csnwDMx59MtjTHtC1p3VZ8EHkZ8JWU7dwIDAGrJS27ItjOjYltB8t3Cbbx11If7+Cx5uT9JS2/GhgFNkhadmywnb3TrH9m8Fh7msd6Wfn7Sqb3wCTgZfxJgNTvSadk2r9uuulWm7ekz+rTgLXxY6JvBY814wNTPw5+fyf5swZfUuMd/MmqlqTlLfiTY28DU4JlU4LP8cdJGtPgg5DvpH724IN4g8AOKe1tCz6Dr0jzGo7N8TXfE6x/UcryRN90adIyA5rTbCNGypgw6F/eSv1sTfPcfMbBf8f3Z1slLWvCB1gdcGYOr/faYN19Ul7Xn0gZk5JH/xf87ZYGf9MpadZfJenn1P38JFi27jhtP5OUvg8fxHXAgSnrnh8sj6V5/l8Y2/fPCJafm8PxOzZY9yHGxhA+FiwfAWak/H1eAv6Rsp2Cxu34E78vBe+Z9pT1VnrvJy17Mfnvgg8cx5PbhQ94DwL/BiYlLV8fH2fRd4Iy3VQiQ6REzOx9+Iyea4GJ5ktGrBNcRnM//svH8kxg59xA0nMnmb8caS3gdvwXmG3S7OYit/KZy4RLXFJtZefcYvyZzy1zfAkvOuf+kLLsruB+i6CdDfgMtn+5lbOiL8hxP8l+6Zx7N3VhyrFZLTg2o8CDwG65bDjfv4eIiIzrJpeUEeP8t/W7gfXNbLVg2Sh+kDTDzJL7sb3x2T/JWbCfxn/JvzL5Mzr4nJ6Dz2jZI6UNjzjn5qYsG8QPBndLvdw0RSH7y8dn8IGHm1K2PQVfxqGdlfvkm1xStppzLo4/+boMf5I32d/wmcLtafb9o+DvkdjOAuAOoCPxt6F0xxvn3GBif2bWZL5EyTr4TKRV8FnG5ZKuTZ34Ae1vgSkpr+2WYB31+SJ1yDn3Ov4z7thg0WH4E5OXZ3hKJz4T+GfOubeStvMW8HN8GYeOYPF++ADXL5LHNM5nSl+TvFEzM/zn8H3A4pTPqQHgn5Tmc+q85F+cc3/C9yuHJC1zzrnBoF0NZjYlaEdi7Jc83uoPXuNBwWtYST7jLvMlLPYAZjvnlmf0BuPYC3N5gWa2Cj6beL5z7u7k1wX8KM1T8un/PokPpp7lnFuSuiHn3LIsTesP7j9ueZRmCl7Px4CFzrlbUh4+lxXZ1ql+mtL3z8N/D8l1/A9+PJ48P1Mi4/ifwfYS2x7CnwAYs+1Cxu1m1oH/P+jFJ6315tHe3yb/XYL/u3+mtKsTf+L5l86595LWfZmU/0spLZXIECmdbYP7s4JbOuslfggGfGfiLzfZJM26a6ZZlvGyGuDZNMtex58Rz0Wm54M/8w8+s2oyK0pZJEu3bDxpX4+ZbQ58H/gIfmCezK30hPTy+nuIiMi4xusn3gl+vhJ/ieXR+MsSCX5OBJ8TtsVnGz2ZZZ+pn9Mr9RvOuaHgEuWfAs+Z2eP4QfJNzrk7i9xfPrbFD1JfybLOeox9DemO6Zv4rPDUiafeDO5Xqo/JitITyR7HD+jb8Jdhl+R4AwQD52/g/65bsHKt0XTfYUolXZsSfX6mgBGozxepZ78FbjazD+DLY/zLOfd4hnWnBvePpXksUXJos5T7dJ+rqdtvxX9+70fmy/SzBS+xlefGGXLOvZH0+xK3ogxGsieAQ8xsciIgaGaH4/vqnfAnL5Mlf4b/AF+C6ibgdTO7F18a8Trn3NvBOvmMu/I5Zpmsiw/057qNfPq/RKByYY5tSXYxvnzYJfiyDffjS3L0BCeQM2nFv56V3nPOuTfM7CVWHLdk6b5DvEH67wmZjNmGc+7N4DzCc2nWfTN12wWM29fDn/h9HNg3XbJZPu0NvJ7SrsT/cKliFpIjBZhFSicxuLqADLWfWDE4BH+G96P4ukb34TuDEXyG8Cmkn4Qz2wdwpllW055pzuP5ydvItq1cA7/JVno9QeD9Pnwg+yJ8Dau38V+4vgl8OMdt5/v3EBGR7HLpJ3DOLTKzh4HPmNm38Zcjfxy4PWXga/i+44As204dbKXtB51zl5rZbOAg/OSvnwBmmdl1zrkjithfPgwfNDgyyzqPpvyeqR05Hesc2pP6e0mON/4y4C/jy2N9H19rdBh/GfIPyX0i8WzfHTKNU9K1KfFavwo8nOF5L+bYJhGpPbfh6+meAeyDn6smk1w/Y5PXTfdZlu4zGHzt4R/msY9kL6X8fi/+CqGETJ+pY9piZofhP7//BZyEr1/8Hr7M1V9J+gx3zj1tZtsB+wa3vfA18M8ysw855/5LfuOufI5ZJtm2kWn9XPu/fLe9nHPudTObAXwQn0X7IXxW9llmdqBz7h9Z2leIYsf/2baR7XuI30lh4/Y38GU5DsJnlv86j7bm1C4KP55SJAWYRUrn6eB+NN3lpMnMbAo+uHy1c+4LKY91pH1SNLyKv8Rp6zSPpSvpUYh98ZMhHO+c+23yA8HkDKkydf45/z1ERKTkrsQPqvbBT2C0OmPLY4D/nN4feN45ly4DNy/OuZeA3wC/CUo6XQ10mdkFwWWepdhftgHn08BW+MtK38myXjlsi79ENHXZKL72NZT2eB8F3JcUvAfAzLZIs262Y5bIvFsLf6lsYjuT8O+bZ3JsT6LPH1CfLyKpnHOjZnYVPug1CPw+y+qJSUi3B+5MeWy74P7ZlHW3ZUV5CZKWJYvjaz+3FPE51Znye2qyzJpmtn6aLOZtgFeTyhkchQ8o75OcQZpS2mq54IqaW4JbYlLVm4GvAF8iv3FX8jFLlW5ZOq/ir5pKt/52aZbl0/8lMlx3YsXryllQKuye4IaZvR9YAJyOD6qm8yo+MLt96gNmtia+P3w437ZUQL7jdvAnoxMnOC4zs0bn3CUlblci+3prVv6/TBfHkBJRDWaR0lmIz0z6gpmtdAmLmU0ws7WCXxNn3lLPJm9A+tnhIyHoMG8FdjWzPVMePrVEu8l0bPYjfR2nd/BfplLPVObz9xARkdK6Fn9VztHBrR8/U32yq4P7HwQB4TGCOo3jMrNVzWzV5GVBf/Xv4NfEZ30p9pcIHKfrP67Cf7c+N0M7y1mi4WvJ/aCZ7YyvEXpnUrC7JMc7MMrK/fRk/BVYqbIds0S5i9ST65mu5MrkNvwA/Rvp+nYzazaz1fPYnojUnkvx5Ru+4Jzrz7LeHfiEmi8nf24EP38Z/5l2R9K6g8CXkvshM9uYlKtZgtq91+DHUZ9It+PxPoedc3NTbgvSrPaNlG0eig+q3ZS0eBR/8m+VpPUMHwRNbdM6afbxUHCf+LzNedzlnHsVf0J0ppltlbROE+n7kJUEffxfgOlmtk/Ka/hamqfk0//dgJ+A8Awza0mzbsbs2AzH6kn8eyTjuDN4b/wZ2MnM9k95+Bv4v9OfMj0/RPmO2wFwzg3jy4TeAPzCzE4qcbvuwM/N8X/BCetEu9bHZ01LmSiDWSQ/+yZ/SCV5Lbg89yj8WbJ/m9nl+EttVsXXJzwMf9b8Cufc22Z2O/7y4UFgHr5G4gn4M2751E2qtNPxNZb+amYXAy/gz8a2Bo8XUioj2f34meAvMD9Z0wv4WWaPwl92876U9f+Jzwa/2Mz+ju/o7nLOvZrr36PI9oqISIrgM/hWfKmKSUB38kQrwTrzzOwM/ID/YTO7Hl/GYANgF3zJqKYcdrcVcK+Z/Qk/wH0Tn9X0f/g+9W8l3N/j+CyjL5rZu/hstFedc3c5524ws9/iS3PsjB/8voafkX4PfN+TroZiKbQBt5nZHPzrmYUf0H41sUIJjzf4QeEJZnYd/nLv9fB1TV9Ps+48/OWy3w4ysQaA55xzDwbPfRI4O5gY6DngA8Du+GOXE+fcgJkdjQ+gPBX0+c/g60Fug+/zDyXIKBOR+uOcex4//8146y0xs6/hJ1p90MyuCB46Fv85fkIiQB3Uq/0O8GPg70GW9KrAF/DZrzulbP7bwJ7AH8zsD/hxzBD+M/xAfKbrsQW/SP+5eZiZbYj/vNsS+CJ+boAzk9a7AV+66q6gzY34SQDHnKwN3G5m/fhSCP/Df64eix/zXQ1+cr08x11fCdr3gJn9At+XHkF+8anT8SUv/mJmP8ePGQ9mxZh0uXz6P+fcC+bndfgFsCg4Pn3ARvj6yseTOZv418HJhduD5zQDn8JfxXXVOK/nW/gM9ZvM7BJ8H/ah4Pn3sfJVYFGQ77h9OefciJl14TOaLzKzCc65C0rRqKBUyVn4+uEPmNnv8O/Fz+NPbE+n+JiFpKEAs0h+9g9uqZ4CLnXOPWxmO+E70I/hv1y8jb/s8wrGXmb1GfwsvwcDx+C/hHwb/yE75hKTKHHOPWVmH8J/kToJf3nVX/CXRz2LH9AWs/0lZvYR/AzAX8Z/Ti3Ad/wxVu6oLsIP2D+BP96r4C/JfjXPv4eIiJTWlfg+DjIMrJxzZ5vZAuBE4GR8Hb9X8YHiXDNa/oef3G0f/AB5Ir7W5q+BHyZf/lvs/pxzg2Z2BHAOvv+ZiK+BeVfw+PFmdjd+EPNN/ID1ZXy21zdzfD2F2B9fF/ks/ID2n8BXnXP/Tl6pRMcbfHDgbXwG0kz83+BX+GDymMujnXPPm9nxwNeBX+IDGVcCDwaXrc8Efobv84fwA/O9gAfyaA/OudvM1778Bv47Viv+ZMN/8cfm31meLiKynHPuEvMTq30VX7cZ4BHgUOfcTSnrXmBm7+A/F8/Ffx7+GH/lzuUp6/YHV4GeyorPzxF8YO5+fJmnYgzg695eiB9nGr4m8qlBGalEO34fZGSfErT1TXwG7TdY+UThL4O2noDPwn0dn7H8Zefc3UnbzHnc5Zz7h5l1Bm38BvAWcH2wr0W5vFDn3H/N7IP4us9fxmes3ooPbq402W4+/Z9z7pdm9l/83/9EfF//YvAa/pelWVfjg+/H4Pugt/Anpj/hnPvjOK+nz8x2A87G92FT8O+Lc4FznHMj2Z4fhgLG7anPHw1OTAwDPzazJudc2qvACmjbuWb2Fv5vex7wPHA+/n9iOkXGLCQ9c06BexEpnpntAswHvumcOy/s9oiIiNS6ILPuGOecJrQRERERySLIdp8FbJh80kVKQzWYRSRvZtac8ntyvas7Vn6GiIiIiIiIiEh5pStran6+q6OBRxVcLg+VyBCRQjxsZnfhL2GajL8E+oPAdRkmmxARERERERERKbe9zex84EZ8qZF24HPAaqRMhCmlowCziBRiNj6ofBT+c+Q54DvAD8NslIiIiIiIiIjUtWfwczB8DlgbP2/UfOBc59zcbE+UwqkGs4iIiIiIiIiIiIgURDWYRURERERERERERKQgNVEiY5111nHt7e1hN0NERGSMBQsWvOacaw27HaWkPldERKJG/a2IiEhlZOpzayLA3N7ezvz588NuhoiIyBhm1hd2G0pNfa6IiESN+lsREZHKyNTnqkSGiIiIiIiIiIiIiBREAWYRERERERERERERKYgCzCIiIiIiIiIiIiJSEAWYRURERERERERERKQgCjCLiIiIiIiIiIiISEEUYBYRERERERERERGRgijALCIiIiIiIiIiIiIFUYBZRERERERERERERAqiALOIiIiIiIiIiIiIFEQBZhEREREREREREREpiALMIiIiIiIiIiIiIlIQBZhFREREREREREREpCAKMIuIiIiIiIiIiIhIQRRgFhEREREREREREZGCKMAsIiIiIiIiIiIiIgWpaIDZzC43s1fN7NEMj5uZ/czMnjGzf5vZzpVsn4iISC1K1/+a2VpmdoeZPR3crxlmG0VERKqRxrgiIiKVz2C+Atg/y+MHAFsGt88Dv6xAm6RY8TjMm+fvRaT2VPP/eGrbn3gCrrzS36d7vJpfa3ZXsHL/+w3gTufclsCdwe8iUmHxONx+u79F4aOnEh+DiX088MDYj+RCtlFIO5OfO952SnE8El3PAw+U7tgm2vXEE/7+L3+BM87w+8i2frruMHVbuR6LdO/d5GXZtldj3e0VaIxbdeIDceYtnkd8IPpvwkq2NcrHpdxty2f7UT5O48m37aU8Lg88/wBn3H0GDzyfobMqYvul/puMt73kx1PXzee56X5/Iv4EVz58JU/Enyh7W0rKOVfRG9AOPJrhscuArqTfnwI2GG+bu+yyi5OQXHutc83Nzq2xhr+/9tqwWyQipVTN/+Opbd9vP+dgxa2zc+zjs2aV/LUC812F+9lMt9T+N7mPBTYAnsplO+pzRUrn2muda2xc8bHU1BTux2wlPvIT+2hqGvuRPGtWZdqZ/NymJn/8M22nFMdj1qyxr7Oxsfhjm2hXc7PfptnYfey3X/bX0dk5dv1VVlmxrcR2Mx2L1Vd3buJE52Kxld+7s2at/HdNt71yvM/C7m81xq0u1/77Wtd8TrNb49w1XPM5ze7aRdH9flvJtkb5uJS7bflsP8rHaTz5tr2Ux6Xzqk7HmSy/7XfVfhm2lP/2S/03GW97yY83fa/JNZ7VuHzdWbfMyvm56dbf76r9xhynzqs6c95e41mNrul7TTm3pVCZ+lzzj1WOmbUDf3HOTUvz2F+A85xz9we/3wl83Tk3P9s2p0+f7ubPz7qKlEM8Dm1tMDi4YllzM/T1QWtreO0SkdKo5v/xdG3PVwleq5ktcM5NL7wRpZPa/5rZEufclKTH33TOjVsmQ32uSGnE47DppvDee2OXh/UxW4mP/PE+mh9/HLbdtnztHG//ydspxfF44gnYbrvx95WPXLu3+++HPfcsvDsc71jkK7E9KM/7LOz+VmPc6hEfiNN2URuDIyvehM0Tmuk7uY/WydH6flvJtkb5uJS7bflsP8rHaTz5tr2Ux+WB5x/gA7/9wEr7uP+4+9lz0z2LajtQ0r/JeK8l3ePZFPPcUm+vVO/VTH1u1Cb5szTL0kbAzezzZjbfzObHa+TaqqrT2wtNTWOXNTb65SJS/ar5fzxd2/NVLa+1AtTnipReby80NKy8fJVVwvnoqcRH/ngfzf/6V2HbyLWd4+0/eTulOB7ZXk+hxzbX7u322/NbP1XqsZgwIf9tpNteNX+1KILGuBHSu6SXpoaxb8LGhkZ6l/SG06AsKtnWKB+Xcrctn+1H+TiNJ9+2l/K43P7f29PuI9PyfLZf6r/JeNtL93g2xTy31Nsr93s1agHmF4BNkn7fGHgx3YrOuV8556Y756a3Rj2Trla1t8PQ0Nhlw8N+uYhUv2r+H0/X9nxVy2st3CtmtgFAcP9qphXV54qUXns7jI6uvHzZsnA+eirxkT/eR/Ouuxa2jVzbOd7+k7dTiuOR7fUUemxz7d422cRnHhfaHY53LArdXjV/tSiCxrgR0j6lnaHRsW/C4dFh2qe0h9OgLCrZ1igfl3K3LZ/tR/k4jSfftpfyuOy3+X5p95FpeT7bL/XfZLztpXs8m2KeW+rtlfu9GrUA8xzg6GCm3d2BfufcS2E3SjJobYXubn9dW0uLv+/ujv6l8yKSm2r+H0/X9v1SvsDst9/Yx2fNqs7XWrg5wDHBz8cAs0Nsi0jdaW2Fyy/32ZsJTU3hffRU4iM/eR+pWayzZo1fHqPYdqY+t6nJH/902ynF8dh2W/+6kk2YUNyxTW7XpEl+maXkx5rBaaf5UhRz547fHa6yil8OfpvpjsVPf5q9XU1N/rWm/l1Tt1fNXy2KoDFuhLRObqV7ZjfNE5ppmdhC84Rmumd2R7K8QSXbGuXjUu625bP9KB+n8eTb9lIelz033ZP9Nhvb+ey32X45lcdIbD+2U2zMstjOMVont5b8bzLe9lIfb2pootEal687a9dZOT833frpjlOu22u0RpoamnJqSzlUtAazmfUAewPrAK8AZwCNAM65S83MgIvxs/C+Cxw3Xm0qUH2q0MXj/rq29vaa/3YoUpeq+X88te1PPOGvWd51Vz/yT328xK817JqQSe1I1//eBPwB2BR4Hvikc+6N8balPlektOJxWLjQ/7zTTuF/zFbiIz+xj6EheOaZFR/JhWyjkHYmPxeyb6cUxyPR9WyxhQ/AluLYJtq12mrwzjvwyitwzz1w8cWwdOmK9ZJrH2fqDtdZZ+y2MrXvssvgpJP8axgZgQsvhKlT/WOJ927y+3mTTTJvr9TvszD7W41xq1N8IE7vkl7ap7RHPjhYybZG+biUu235bD/Kx2k8+ba9lMflgecf4Pb/3s5+m+ceXE5sd7w6y6X+m4y3veTHgTHrjvfcJ+JP8K/F/2LXjXZl29ZtV1p/vMdL2ZZCZOpzKz7JXzmo8xURqVHVHNwmOgHmUlKfKyISTfPmQWcn9PevWNbS4rOYZ8wozT6i2i2rvxURqV3zFs+j8+pO+peu6OBaJrYw96i5zNioRB1chfQs6iE2J0ZTQxNDo0N0z+yma1pX2M3KS7VM8iciIuL19Pjrezs7/X1PT9gtEhERiaxK1DdubfXB6igFl0VEpLZVc+3rZPGBOLE5MQZHBulf2s/gyCCx2THiA7UxqasCzCIiEj3xOMRiMDjoU7EGB/3vmlFdREQkrTqtbywiIjWummtfJ+td0ktTw9jJChobGuld0htOg0psQtgNEBERWUlvry/yOLiizhaNjX65RsoiIiJpdXVBR0c0y1gki2qpDRERiaauaV10TO2o2trXUDuZ2Jkog1lEpBbF474YY7Vm/FbiOl8REZEaFPUyFqqAJSIihWid3MqMjWZUZXAZcs/Ejg/Embd4XtWVzlCAWUSk1tTCyE3X+YqIiNQcVcASkXpUrQFDKb2uaV30ndzH3KPm0ndy30oT/PUs6qHtojY6r+6k7aI2eh6tnrG8AswiIrWklkZuXV3Q1wdz5/r7ruqaXVdERETGSlTASpaogCUiUouqOWAo5ZEpE7vaJwFUgFlEpJbU2sit3Nf5VnspERERkSqiClgiUk+qPWAolVXtkwAqwCwiUks0cstdLZQSERERqSKqgCUi9aTaA4ZSWdU+CaACzCIitUQjt9zUUikRERGRKqIKWCJSL6o9YCilkWsN7lwnAYyqCWE3QERESqyrCzo6fFmM9nYFl9NJlBIZHFyxLFFKRMdLRESkrFpb1d2KSO1LBAxjs2M0NjQyPDpcVQFDKV7Poh5ic2I0NTQxNDpE98zulSb2S9Y1rYuOqR30LumlfUp7Vb1XFGAWEalFGrllp1IiIiIiIiJSZtUcMKxn8YF40X+z5BrcgyM+sSk2O0bH1I6s22yd3FqV7xOVyBARkfqjUiIiIiIiIlIBrZNbmbHRjKoMGtajnkU9tF3URufVnbRd1EbPo4XN1VNvNbgVYBYRkdoVj8O8eelrK6sIpIiI1Ils3aGIiEi1ybWucSHbTWQd9y/tZ3BkkNjsWEH7qbca3Aowi4hIberpgU03hX328fc9ac48t7bCjBnKXBYRkZrV0wNtbdDZ6e/TdYciIiLVolQZxumUMus406R9QFmC42FTgFlERGpPPA7HHAPvvQcDA/7+mGOUuiUiInUlHodYzM9p29/v72MxdYciIlKdSplhnE6ps467pnXRd3Ifc4+aS9/JfeBYKThermzsSlOAWUREas/ChX7SvmTDw365iIhInejthaaxiVg0NvrlIiIi1abcdY0zZR0XUz87UYMbWCk4fsyNxxSdjR2VAPWEUPcuIiISj/uRbnu7SlWIiEjdK2W32N4OQ2MTsRge9stFRESqTSXqGndN66Jjage9S3ppn9JesskZE8HxwZHB5cuG3TDDI8PLl8Vmx+iY2pHzPnsW9RCbE6OpoYmh0SG6Z3bTNS2cuYWUwSwiUu/CnPmnXIUhd9oJJqScQ50wwS8XERGJqFJ3i62t0N0Nzc3Q0uLvu7t1PldERKpTOTKMM+1nxkYzSrrddMHxVPlkY5e7XEi+6jaDefHixcydO5fXXnuNZcuWhd0cKZFJkyax1VZbsc8++9CUej2giKysp8cXY2xq8ilO3d3Q0VGZjOLkwpCDwVncWMzvvxT7Ncv+u4iISISUq1vs6ipP164LkEREJAzlyjAut0RwPDY7RmNDI0MjQyxj2Zigcz7Z2OkyohMB6jCOSd0FmJcsWUJXVxcPPvgg++23HxtuuCENDQ1hN0tKZHBwkJ6eHo488kjOPfdcPv/5z4fdJJHoSjeSPfZYWGUVmDhxRcC5q0yX2CQKQw6u6BCXF4YsdqTa2wurrupnNEpobi7NtqVgjz32GD09Pdx99928+eabOOfCbpLkaNKkSWy99dZ8/OMf52Mf+xgTJ04Mu0kiNaec3WJra2m7v3Tnp8v1dUFy9/jjj3P//ffz1ltvqY8toVVXXZVp06bxgQ98QLEDkYhondxaNYHlZKnB8bnPzV0ecB4eHc4rG7sS5ULyUVcB5uHhYT7ykY+w++67M3v2bGW41rAnn3yS/fffn0mTJnH00UeH3RyRaEo3kk0UanzvPX9fyoziVLkUhiw0PUpFJyPn97//PSeeeCLHHHMM55xzDuuuuy6rrKJKXdVicHCQBQsW8LOf/Yxf/vKX/OUvf2HVVVcNu1kiNaVauq7xMq2V2Vx5Tz31FIcffjhvvPEGnZ2drLnmmupjS8Q5x7vvvstvfvMbXn75ZS655BIOPfTQsJslIlUsOTheTDZ2akZ0vgHqUqurAPPcuXMxMy666CJMl0rXtG222YZrr72WWCzGUUcdpb+3SDrpRrKpSpU6lU6iMGQs5vczPDy2MGQx6VHjbVsq6oknnuDEE0/krrvuYtq0aWE3Rwq08847c/zxx3PMMcdwyimncNlll4XdJJGaUi1dV7ZM67lzldlcaS+88AL77rsvZ5xxBrFYTIHlMnrwwQeZOXMmq666Kh/5yEfCbo6I1IhisrGjVC7EauHSmenTp7v58+ePu94JJ5zA1ltvzVe+8pUKtErC5pyjra2N22+/nW222Sbs5ohEUyKImxjJjoz4+4TmZujrK38t5tRUp3jcz26UPHotpC0hp1GZ2QLn3PSK77iMcu1zk333u99lYGCACy64oEytkkpavHgx73vf+3j55Zd1NZhIGUQ9AzhTF71gAeyyS/FddyHqub8955xzePHFF7nkkksq0Cr5wx/+wK9+9Svmzp0bdlNEREKRqc+tq9Obzz//PFtvvXXYzZAKMTO22mornn/++bCbIhJdXV1+5Dd3rr8/4YSxj8di5R8VtrbCjBlj95NIj0qWSI8qdttScXfffTcHHnhg2M2QEtloo41oa2tj4cKFYTdFpGbE4zBvnr+PeteVyLRuboaWFn/f3Q3vvFOarlvyc9NNN3HEEUeE3Yy68dGPfpR58+axZMmSsJsiInUqPhBn3uJ5xAfiYTdljLoKMA8PD2tSmjozceJEhpOzMUVkZYmRLPgRYrLubj/azSZ5VFwq1VKIUnLy5ptvsu6664bdDCmh1tZWDa5FSqSnx2cEd3b6+56esFs0vtTz011d6rrD8vzzz7PVVluF3Yy6seqqq7LhhhuyePHisJsiInWoZ1EPbRe10Xl1J20XtdHzaHS+NNRVgFlERLIoJGu4XKPiTOlRUU3nkqycc6oJWWMaGhpYtmxZ2M0QqXrJE+b19/v7WKy052zLJTXTWl13OJREVXlKYhKRMMQH4sTmxBgcGaR/aT+DI4PEZscik8ms0V4BHn/8ccyM22+/veDn3nHHHWVoWbj+97//8YlPfII11liDlpYWDjvssJzLUzzwwAPst99+rLvuurS0tLDzzjtz+eWXj1nntttu48Mf/jDrr78+EydOZOONN+bwww/n8ccfL8fLEak/+aYelXtUnC49SmrSf/7zH8yMW2+9tSTbe+uttzjzzDN54oknSrK9MJW7b81nPREpXupFP6WqCBUV6rql1ArpB/fee2/MLO1t//33H7Pu3XffzQc+8AGam5tZa621OOqoo3jllVfK+ZJERArSu6SXpoaxXxoaGxrpXdIbToNSKMBcgIceegjws6kX+tzp02tqDgreffddPvzhD/Pkk09y5ZVXcvXVV/P000+zzz77MDAwkPW5//73v+no6GB4eJhf//rX/PGPf2TGjBnEYjF++ctfLl/vjTfeYJddduHiiy/m9ttv59xzz+Wxxx5j9913p6+vr9wvUaT25Zt6VIlRcdQLUUpJJPrGGYlSLUWaP38+Z511VtVnF1Wib811PREpXrqLfmqxrIS6bimVQvvBSy65hH/84x9jbj/5yU8A+NjHPrZ8vb/97W/st99+TJkyhT/+8Y/89Kc/5b777mPfffdl6dKlZX99IiL5aJ/SztDo2C8Nw6PDtE9pD6dBKSaE3YBqtGDBAtra2lhnnXUKeu7mm2/OmmuuWYaWpbd06dKyXzb161//mmeffZannnqKLbbYAoD3v//9bLnlllx22WV85Stfyfjc3//+94yOjvLnP/+Z1VZbDYDOzk4eeeQRrrrqKv7v//4PgK6uLrpS0iB23XVXttlmG2644QZOPfXUMr06kTrS1QUdHblNX1+Lo2IJxYIFC2hvby+oX01n4cKFTJw4ke22264k20unVvrWXNcTkeIkX/QzOOiXxWI+y7e72//c2Oi7UZWVkGoQ5X4wXf//61//mqampjETIp511lm0tbVx0003MWGCD41ss8027LrrrnR3d/PFL36xDK9KRKQwrZNb6Z7ZTWx2jMaGRoZHh+me2U3r5Gh8aVAGc4o77riDj370o2y00UZMmjSJTTbZhNNOO21MFtSCBQtWyl7eddddOfzww/nud7/L5ptvzqRJk3j/+9/PnXfeOWa9BQsWMGPGDK6++mp23nlnmpub2W677bj77rvzbkc6Z555JmbGo48+ykc+8hFWW201Dj/88CKPyvjmzJnD7rvvvrzjB5g6dSp77rkns2fPzvrcoaEhGhsbaW5uHrN8ypQp49Z3XHvttQFobGwssOUispJcU49UbFFKJNE3/upXv2K77bZj8uTJ7LHHHsszmxOcc3R3d7Prrruy6qqrsvHGG3PKKacwmIjWANtuuy2nnXYaS5cupbGxETPjE5/4BKC+FVbuW4vpg0Ukd9ku+lFZCSmXz372syuNW3fZZRfMjBdeeGH5stNOO42pU6cyOjqadjvV2A8mGxwc5Prrr+fggw9mrbXWWr78n//8J52dncuDy+Cvplp77bX505/+VJoXISJSQl3Tuug7uY+5R82l7+Q+uqZF50uDAswpHnnkEfbdd19+/etfc9ttt/GVr3yFSy+9lPPPPx/wg9uHH36YXXbZZflzRkZGWLRoETfffDP3338/F154IT09PYyMjHDYYYfx2muvjXnu3//+d6655hpOP/10/vCHP7Bs2TKOPvrovNoxnpkzZ7LXXnsxZ84cTjnllIzrOecYGRkZ95bpy0bCY489xrRp01Zavv32249bI/nYY48F4MQTT+TFF19kyZIl/PrXv+bOO+9M2/bR0VGGhoZ4+umnOeGEE1h//fXHnIkWkQrSqFhKYOHChTzwwAP8+c9/5sc//jFXXHEFr7zyCgcffPCYS2A/+9nP8qUvfYmOjg5mz57NN7/5TS6//PIxGUxXXXUVm222GQcffPDyy2IvuOACoDJ9a6n6VahM35pvHywihRnvop8wykqk1oOW2rPmmmvy9ttvL//9rrvu4qmnnsLMePPNNwFfhuLyyy/nxBNPpKGhIev2qqkfTHbjjTfy9ttvc8wxx4xZ3tDQQFPqmR/8JH6PPvpoztsXEamk1smtzNhoRmQylxNUIiPFaaedtvzn0dFR9txzT+655x4eeOABwE9E9Pbbb48JMD/++OO89957fOhDH+KOO+5Y3jGvtdZa7L333vztb3/j0EMPXf7czs5O/vjHPy5//v/+9z++9KUvMTg4uDyDaLx2jOfEE0/kpJNOGne9e++9l3322Wfc9fbaay/uueeejI+/8cYbact+rLXWWsu/vGQybdo07rnnHg499FAuueQSwGckX3rppWkDx7vtthsLFiwAYIsttuCuu+5i3XXXHfc1iEiZtLYqa1kK9t///pclS5aw2267MWfOHMwMgNVXX50DDjhg+QR0V111FZdffjl//OMfOeywwwBfyuG9997jW9/6FhdffDENDQ3ssMMOvPDCC3z5y19m9913H7OvSvStpepXoTJ9a759sIgUJnHRT9ilMOJxnzX90ENwyik+q3poyLdF54hrT2qA+YILLuD444/niiuuWN6PXHXVVYyOjhKLxcbdXjX1g8muuuoq1l13XQ444IAxy7feemv++c9/jlnW19fHSy+9pCtkRUTypABzkpGREX73u99x2WWX8fTTT/P6668vfyxxCVAisJl8qVFi2Q9+8IMxZ3232WYbgOXbSV4v2WuvvUZLS8vy4HIu7RjPoYcemtN6u+yyC/PmzRt3vdVXX33cdRJBgWTOuXGf9/TTT/Pxj3+c7bffnksvvZTm5mZmz57NF77wBSZNmsSnP/3pMetfffXVvPXWWzz77LP8+Mc/prOzk/vvv5921X0VEak6ib7x+9///ph+5H3vex8A8SC17pxzzuFDH/oQH/vYxxgZGVm+3nbbbcfQ0BAvvvgim2yyCY899hhDQ0MrXRJcqb61lP0qlL9vzbcPFqkFiSDreFMNlFo+0xyUQ0+PD3BPmACJmGNyPeiODp0vrjVrrrkmb731FgBPPPEEt99+O0899RQ33njj8gDtxRdfzGc/+1laWlrG3V419YMJL774InPnzuWkk04aUwoD4KSTTuIzn/kMp59+OieeeCJvvPEGn//851lllVVYZRVd7C1ST+IDcXqX9NI+pT1ymcFRblsyBZiTHHnkkdx6663MmjWL008/nXXWWYf33nuPffbZZ/mlOQ899BAbb7zxmIzZhx56iA033JA999xzzPZefPFFADbeeOPl67W3t7P11luPWW/hwoW8//3vz6sd49lggw1yWm+11VZjxx13HHe9dB17sjXXXJM33nhjpeVvvvnmuBMafutb36KxsZG//OUvy88U77vvvrz++uucdNJJdHV1jengt912W8BnMh9wwAG0t7dz3nnncemll477OkREJFoeeughNt988zFXBgG89NJLgO9Dn3vuOZ5++mmefvrpjBlFa6yxxvLtmdlKfVul+tZS9atQmb413z5Y8hNWIFMySwRZw8rcTVz0kyhPUan3RvIkg+kk6kHrfVpb1lxzTQYHBxkdHeWCCy5g5syZbLbZZqyxxhq8+eabzJ07lyeffJKbb745p+1VUz+Y8Lvf/Y5ly5atVB4D4NOf/jRPPvkkP/7xj5ef6P7Upz7FgQceqBIZInWkZ1EPsTkxmhqaGBodontmd2RqG0e5bak0Ygg8/PDDXH/99Vx22WWce+65HHTQQey2227E43Gcc+y0006Az7RKHQQ/9NBDbLTRRitt87rrrmPVVVflgx/84PLnpmZUgQ8wJ5bn2o7x5NJhg7+EqbGxcdzbvvvum3U722+/PY899thKyx9//PG0s/gmW7RoETvssMNKQYNdd92V119/nVdffTXjc6dMmcIWW2zBM888k3UfIiISTQsWLGDDDTdcafl1113HGmuswR577MHixYsB+O1vf8u8efNWui1YsGB55tXChQvZfPPNx2RiVbJvLVW/CpXpW4vpgyW7nh5oa4POTn/f0xN2iyQ5yNrf7+9jscrXIA7jvZFuksFkyfWgpXYkgrDPPPMMv/vd7zj11FMBaGlp4c033+RnP/sZhx12GG1tbTltr5r6wYSrrrqKHXbYgR122CHt49/73vd47bXX+Pe//81LL71ET08PTz/9NB/4wAdy2r6IVLf4QJzYnBiDI4P0L+1ncGSQ2OwY8YHwJyiIctvSUQZz4PnnnwcYk108MDDA6aefDviSGM45Fi5cOGYyoWXLlvHII48wefJkRkZGll928+KLL3LJJZcwa9YsJk+evPy5yfUfwZ997evrWz64zaUdpVSqS5g+9rGPcdppp/Hss8+y2WabAdDb28sDDzzAeeedl/W566+/Pg8//DBDQ0NjJll48MEHmTRp0piZflO98sorPPnkk7qEVyTKlMInWTz00ENMmjRpTB/6wgsvcOmll3LqqafS1NS0/CTuxIkTmT59etbtpRt0VrJvLeWlwZXoW4vpgyWz5ECmShBERyLImpzFW+nM3bDeG+kmGQRYbTUYHQ2nHrSUXyLA/IMf/ICdd96ZPfbYA/BX/SxYsICbb74553kIchWVfhBg/vz5PPbYY/zkJz/Jut7kyZOXl+b661//ypNPPkl3d/e42xeR6te7pJemhiYGR1Z8OWhsaKR3SW/o5Sii3LZ0FGAO7LTTTjQ1NfHVr36Vb37zm7z88sv86Ec/YmhoiHXXXZcNN9yQZ555hv7+/jEZzE8++SQDAwOstdZaHHvssRx33HG88MILnH322WyzzTacddZZgJ/EqL+/f6VB7MKFC4EVg9tc2lFKq6+++riD9Vx87nOf4+KLL2bmzJmcc845mBnf+c532GSTTTjhhBOWr3fvvfey7777cvnll3P00UcDMGvWLD75yU9y8MEH88UvfpHm5mbmzJlDT08Pp5xyyvIB76GHHsrOO+/M+9//flpaWvjPf/7DhRdeyIQJE5afjReRHFUq6Bv2tcgSab29vbzxxhtMnTp1TB965plnssMOO/Ctb30LgPb2dvbZZx9OOukkXn31VXbYYQfeffddnnvuOe644w5uvPHG5WUcpkyZwkMPPcRtt93GGmuswZZbblnRvrVU/SpUpm/NdT3JTxQCmbKydEHWSmfuhvXeSDfJ4IUXws476/xvLUsEmK+55hquu+665ctbWlr4/e9/z2677bbShLjFiko/CD57ecKECRx55JFpt79w4UJuvfXW5WPx+++/n/PPP5+vfe1r/L//9/9K8hpEJNrap7QzNDr2y8Hw6DDtU9rDaVCSKLctHZXICGyyySZcc8019PX18bGPfYxf/OIX/OhHP2KLLbYYUx4DxmY6PfTQQwDccsstLFmyhIMPPpivfe1rHHjggdx5551MmjQp43PBd2oTJ05cnm2VSzuiaPLkydx1111stdVWHHXUUXz6059m6tSp3HXXXay22mrL13POMTo6yrJly5Yv+8QnPsEtt9zC0qVL+exnP8vHP/5x7r//fn7xi19w/vnnL19v991356abbuKYY47hoIMO4ic/+Ql77bUXDz/8MFtttVVFX69IVavUtbmFXIucKEpZ6euVJRSJvvEvf/kLg4ODHHzwwXzjG9/gsMMO47bbblse3DQzrr/+eo444gh++tOfsv/++3Pcccfxhz/8gX333XdMjeCzzz6b9dZbj0MOOYQ99tiDJ554Qn1rlr411/UkP1EIZMrKEkHW5mZoafH3lc7cDfO90dUFfX0wd66/P+EEmDFDweValggwb7rpphxyyCHLl6+xxhqMjo6OuTI3iorpB4eHh+np6WH//fdnvfXWS7v9pqYmbrnlFo444ggOPfRQbrnlFi699FJ++MMflv21iUg0tE5upXtmN80TmmmZ2ELzhGa6Z3ZHIkM4ym1Lx/KZgTWqpk+f7ubPnz/ueh0dHXzjG9+go6OjZPs+5ZRTuOGGG/jf//5Xsm1K6Rx00EF88Ytf5KCDDgq7KSLREI/7oHJy6lRzsx9plnqEOW+eD2L3969Y1tLiR7YzZqy8fg1mO5vZAudcadJ4IiLXPjfZ9ttvzx/+8Ae23377MrVKKu2AAw7gxBNP5IADDgi7KZGV+EhLZIvWwEdazQi7cpPeG6VXz/3tmmuuybPPPpvzpHdSvB133JErrrgip8kMRaIuPhCnd0kv7VPaIxu4rIQoH4eotS1Tn6sSGUV66KGHVpr0T0Qksip5bW4+aVoqWCoiNaary3+EqQR94coVCG5tDffvofeGiIiUSjHBx55FPcTmxGhqaGJodIjumd10TavPM56tk1sjEbxNJ7VtUQs4J6hERhGcczz88MMKMItI5RVaSqKS1+bmcy1yb+/Ky5xLv1yqziqrrMLo6GjYzZASGh0dHVOaRNJrbVUJgkJVqppTWPTekFKqhauSq4mOt0RFz6Ie2i5qo/PqTtouaqPn0dw7y/hAnNicGIMjg/Qv7WdwZJDY7BjxAZUqjLJi/ublppFBEcyM/v5+vvOd74TdFBGpJ8WMultbfWZwslisfCPc1IKPma4BXm21sVnVAO+955dL1Vt77bV5+eWXw26GlNBLL73E2muvHXYzpEYVUsJfpF6tuuqqDAwMhN2MuvLOO+8wefLksJshZRQfiDNv8bxIB1uLDRD3LumlqWHsZM6NDY30LuktQ2u9ch7XavibFSvqJwXqLsCss431RX9vqTnFjrrjcZ9FnKy7u7yj9lzStN55x2c4J2tu9svzoUkCI6mzs5M///nPYTdDSuS5557jlVdeqerak/qoiLZENadkiWpOIjLW+9//fh544IGwm1E3Fi9ezJtvvsmmm24adlOkTKKcIZqs2ABx+5R2hkbHXtk6PDpM+5T2ErVwrHIe12r5mxUrjJMC+airALPO7tafgYEBnV2W2lLsqDuqo/ZMJTryKd1R69dTV7EjjjiC3//+9/zjH/8IuylSpKGhIU466SQ+85nPMGFCdU7loY+K6KtkNSeRanf44Ydz2WWXMTIyEnZT6sKll17KwQcfzMSJE8NuipRB1DNEkxUbIG6d3Er3zG6aJzTTMrGF5gnNdM/sLktN33Ie10K3XY0Zz5U+KZCv6hwZFGiHHXbggQce4JBDDgm7KVIBg4ODPPLII2y33XZhN0WkdIoddUd11J6o1xyL+YD38HDmes3paJLASNt88825+uqr+ehHP8rBBx/MQQcdRGtrKw0NDSXdz5Il8NJLsMEGMGVKSTdd15xzDA4OsmDBAq699lq22morzjvvvLCbVRB9VFSHYrsEkXpy5JFHcv311/Pxj3+c008/nenTp2NmYTer5vT29nLZZZdx3XXXcc8994TdHCmTRIbo4MiK0n2JDNEoTaYGKwLEsdkxGhsaGR4dzjtA3DWti46pHWWfMK6cx7WQbYc9uWGhk/SV4m9eTnUVYP7kJz/JgQceyDe/+U3WWmutsJsjZXbVVVexyy67sO6664bdFJHSKXbUHeVRe1eXj/L09vqAdz5tSmRmJ9dxTmRmR+G1Cfvvvz+LFi3ihhtu4JprruHNN99k2bJlJdt+PA5PPw1mfn7ILbfUn76Umpub2WqrrfjFL37BBz/4waqd4E8fFZURjxf2UZ6smC5BpJ5MnDiRG2+8kXPPPZfPfOYzxONx1lxzzar9nI4a5xwDAwOMjIxw6KGHcs8996g8Rg2LeoZoqlIEiFsnt5Y9QFmq45ouMJvvtpMznhNB6djsGB1TOyoSqC02uF2pkwKFsFqoUTt9+nQ3f/78nNb9+te/zm233caPfvQj9tlnHxobG8vcOqm0l156iauvvpoLL7yQ22+/nfe9731hN0mk9IodvZdi9B8l8ThstJEPmCc0NsLixaG+PjNb4JybHloDyiCfPrdS4nFf6iA5aNjc7OeVrIW3t5SO3ivl19Pjz2E2NfkLZrq7M8/vWs9qrRsOm/pbzzlHPB6nv7+/TK2qT5MnT2bdddet2tJQkp+eR3tWyhCtZHZrrfryLV/m4nkXL/991q6z+PkBP8/5+dkCs/n8zeYtnkfn1Z30L13xOdkysYW5R81lxkYzCnx1uYkPxGm7qG1MtnXzhGb6Tu6LVKB4PJn63Lr7hDzvvPPYbLPN+O53v8tjjz3GOuuso46iRiQu4X333Xc5+OCDufPOO1UeQ2pH6kg0cStUsc+PotTLQXV5aN1QVqrkKsoXcdQClSDJjYLwUi5mxrrrrqsrOEWKEOUM0WoVH4jTvXDsRPPdD3Xz3Q99d8zxzVQ6Yrys43z+ZmFmqVdTCZZC1F1k1cw44YQTOOGEE1iyZAmvv/56SS/RlXBNmjSJ9dZbj6bUScxEqplGouPr7fVpiMn1pSdNUoSxTkS1tLhEk0ovlI9O9oxPQXgRkeirRNmIepJLYDVbhnIuz8/1bxZmHeNqK8GSr7oLMCebMmUKUzQLkIhEmUaiuVGEsa4pK1XyVYsXcUSBPorHly4I39AAt9wCBx4YzfelynmIVFahE4CJRNV4gdXxMpRLHZgNK0s96pP0FUuV/0VEoiwxEk2WSAerB/E4zJvn77NJRBibm6Glxd8rwlhXurp8Hd25c/29kvxFKk8fxeNLF4R/5x348pd9ffCenlCalVFPj29XZ2c02ydSa3oW9dB2URudV3fSdlEbPY/qn06qXyKw2jyhmZaJLTRPaB4TWE1kKCdLZCjn8vxC2zRjoxkVD+52Teui7+Q+5h41l76T+2qqvnfdTfInIlJVojgjVaVSmQopDRKxNCtNOiQi9aiUH8UR+1gviUT31tDgg8vJwu7ik0XxK0gm6m+lFtTKBGAimWSrsZzLe1/Z/dGQqc9VBrOISJRVIh0s1yxhqFwqU3JpkP5+fx+L5ZbJPGNG9Ea+IiJ1pFQfxbWaPZu44uLii2H11cc+Nt5FSvl02cWq94uoRCptvCxOkWqXKWs41wzlsLKO04kPxJm3eB7xgQp0yFVCAWYRkagr57X/+YzeCw36FkKjWhGRulbJLicMra2+5vLIyNjl2WpWVzrgrpraIpVV6xOAiWRTTaUjVMomPQWYRUSqQTkyc/MdvVcy6NvePvaaXID33ht/VFvJ1C4RESmbejjPmM9FSmEE3FVTW6SyylFnVqSaRClDOZPkCQn7l/YzODJIbHZMmczAhLAbICIihFNkMt1U9onRe7o2VDqVKXWOgPHmDCikZrOIiERSvWTPdnVBR8f4XwHy7bIr3T4RKY2uaV10TO1QnVmRiEqUskmuF50oZVPv/6/KYBYRCVtYRSbzHb1XMpWptxdWXXXssubmzKlrtX4ttYhInamn7NlcLlIKM+Cu6Q1EKqsasjhFClELdYtVyiYzBZhFRMIUZmC0kNF7OetBJ8t3JF0P11KLiNSZSnU51aCeAu4iIlJ7cqlbXA0BaJWyyUwlMkREwhTWNa8JhVz72tpa/rYlRtKxmD8ew8PZR9L1ci21iEidKWWXE0Y1qlJSuQoREalGyXWLE6UlYrNjdEztWB6Y7VnUQ2xOjKaGJoZGh+ie2R3Zif5UyiY9ZTCLiIQpCoHRqF77mk/qmlK7RERqVinmbw2rGlWpRbXLFhERySRRtzhZom4xVOfEeSplszIFmEVEwhRWYLQUo/VKyGckrWupRURqTikCw6WqRlUtXaeIiEiUjFe3eLwAtFQHBZhFRMJW6cBoraRxpaPULhGRmlGqwHApyvTXctcpIiJSTuPVLdbEebVBNZhFRKKgEnWNYexoPVH3ORbzRR0VlBURkQgp1TQFxVajUtcpIiJSnGx1ixMB6NjsGI0NjQyPDmvivCqkALOISD0Je1JBERGRHJVqmoJ8541Npa5TRESkeK2TWzMGjTVxXnnEB+IVO6YKMIuI1JMoTCooIiKSg2IDw8m6unzGcW+v7/IqmQEtIiLpVTL4JdGXLQAt+etZ1ENsToymhiaGRofontlN17TyleNUDWYRkXoS1qSCIiIiBSjlNAWFlulX1ykiUno9i3pou6iNzqs7abuojZ5HVdxepFTiA3Fic2IMjgzSv7SfwZFBYrNjxAfKN1OxMphFROpBPL4ibauYNC4REZEKq9Q0Bdmo6xQRKZ3k4NfgiK8/FJsdo2NqhzJYRUqgd0kvTQ1Ny/+/ABobGuld0lu2/zFlMIuI1LqeHj/lfWenv+/pKTyNS0REpELicZg3z99HgbpOEZHSSAS/kiWCXyJSvPYp7QyNjq3vNTw6TPuU9rLtUwFmEZFaFo/74pWDg9Df7+9jseiM1kVERNJId25UoiVqJwBEpHqEEfwSqSetk1vpntlN84RmWia20Dyhme6Z3WW9QkABZhGRWtbbC01jswNobPTLRUREIkjnRqNPJwBEpBhhBL9E6k3XtC76Tu5j7lFz6Tu5r6wT/IFqMIuIZJZct7har4dtb4ehsdkBDA/75SIiIhGUODc6uKJs4PJzo9XaHdeS5BMAib9RLOZrVOvvIyK56prWRcfUDnqX9NI+pV3BZZEyaJ3cWrH/LQWYRUTS6enxo6WmJh+g7e4ubur6sLS2+rbHYn50Pjzsf9cIUEREIkrnRqOtVk4AmNlzgMvw8DKgH1gA/Mw592jFGiZSRyoZ/BKR8lKJDBGRVLV2bW5XF/T1wdy5/j7sQLmKNoqIlE0tfMQmzo02N0NLi7/XudHoqKETAPcCDcAGwHPAP4P7DfGJWH3AwcA8M/t/YTVSRESkGijALCKSqhbrFre2wowZ6UfnlYxGqGijiEjZ1NJHbNTOjcoKNXQC4G/4LOWpzrl9nXNHOuf2BaYCbwG3AlsAjwBnhddMERGpJfGBOPMWzyM+kNv4O9/1w6IAs4hIqiik5lQq6FvJaEQumeG1kHonIhKCWrv4BrKfG5Vw1cgJgK8DZzvnXk5e6Jx7CTgH+LpzbgD4KbBrCO0TEZEa07Ooh7aL2ui8upO2i9roeTT7+Dvf9cOkALOISKqwU3MKCfoWEpitdDRivMzwWkq9ExGpsFJdfKPzfJKrGjgBsAmwNMNj7wEbBT8vBpoyrCciIpKT+ECc2JwYgyOD9C/tZ3BkkNjsWMbM5HzXD1vFA8xmtr+ZPWVmz5jZN9I8voaZ/dnMHjGzx8zsuEq3UUQktNScQoK+hQZmK10KJFtmeC2m3lUJMzsl6G8fNbMeM5sUdptEJH+luPgmquf5MgW9FQyXIj0BnGpmE5MXBv3gacHj4Gsyv5JtQxrjiojIeHqX9NLUMHb83djQSO+S3pKsH7aKBpjNrAH4BXAAsB3QZWbbpaz2JeBx59wOwN7ABWamM8YiUnlhpObkG/QtJjBb6VIg2TLDa7HudRUws42AE4Hpzrlp+MmOjgi3VSJSiGIvvonqeb5MQe+oBsOlqnwN2B143sx+a2Y/NLPf4if32w34arDe/wNuz7QRjXFFRCQX7VPaGRodO/4eHh2mfUp7SdYPW6UzmHcFnnHOPeucGwJ+D8xMWccBq5uZAasBbwAjlW2miEhI8g36FhOYTUQjJk2CyZP9fSGlQPJJIcuUGR6Futf1awLQbGYTgFWBF0Nuj4gUqJiLb6J4ni9T0PuJJ6IZDJfq4pybC+wM3Al8CPhycD8X2NE5d2ew3onOuc9n2ZTGuCIiMq7Wya10z+ymeUIzLRNbaJ7QTPfMblonpx9/57t+2CZUeH8bAf9L+v0F/NnhZBcDc/AD3NWBTznnllWmeSJSdeJxP/ptb6/qIoDLJYK+sZgf2Q8PZw/6liIwazb2Ph89Pb6tTU2+Hd3d40c0WlvHvp7E3/DCC+GUU3J73VISzrnFZvZj4HlgELjdOZcxS0tEoi/1IzZXUTzPlwh6Dw6uWNbYCP/6V/rlvb3qNiQ/zrnHgSOL3IzGuCIikpOuaV10TO2gd0kv7VPaxw0W57t+mCqdwZwueuFSfv8I8DC+1tWOwMVm1rLShsw+b2bzzWx+XOkKIvWpVq+PzScFrZhropNTwwYG8k8BK8X11Ml/w1NO8UHmKp+SvpqY2Zr4LKup+H53spl9Js166nNFalzY89umkynoveuu0QuGS13TGFdERHLWOrmVGRvNyDlYnO/6Yal0gPkF/Gy9CRuz8qW4xwE3Ou8Z4Dlgm9QNOed+5Zyb7pyb3qpUBZH6E9VikaWST/3ndAHpXMpWFHs9dLHPT/c3POWU2slGrw4dwHPOubhzbhi4EV9rcgz1uSL1Iaz5bTPJFPTedtvoBcOlOpnZXmZ2qZndYmZ3pdzuzHEzGuOKiEjdq3SJjHnAlmY2FViMn0go9ZKk54F9gb+Z2XrA1sCzFW2liERfputm6/X62ORronMtW1Hs9dDFPl9/wyh4HtjdzFbFl8jYF5gfbpNEJEyFltgol64u6OhYuRpWpuUiuTKzE4BfAq8DTwNLU1fJcVMa44qISN2raIDZOTdiZrOA2/Az1V/unHvMzL4QPH4p8D3gCjNbhO/Uv+6ce62S7RSRKhDFYpFRkJwVnAjcxmJ+FJ46+s633nOqYp+vv2HonHMPmtkNwEP4yYYWAr8Kt1UiImNlCnpHLRguVedU4Frg+GByvoJojCsiIlL5DGacc7cAt6QsuzTp5xeB/SrdLhGpMsUGN2tVvlnBxaaAFfN8/Q0jwTl3BnBG2O0QEalGtTbXcJ3ZCPhtMcHlBI1xRUSk3lU8wCwiUjK6PnZlhWQFF5sCVszz9TcUEZEqlWtFKomsBcBmQK61lkVERCSDSk/yJyJSWvlMhlcPMs2IFOXjo7+hiIhUmVqfa7hOnAicbGYfCrshIgLxgTjzFs8jPqAPUpFqpAxmEZFao6xgERGRstI8tTXhz0ALcLeZvQu8mfK4c861Vb5ZIvWnZ1EPsTkxmhqaGBodontmN13TdEmISDVRgFlEpBZp5iMREZGy0Ty1NeFOwIXdCJF6Fx+IE5sTY3BkkMERf9YuNjtGx9QOWidrPCNSLRRgFhERERERyYPmqa1+zrljw26DiEDvkl6aGpqWB5cBGhsa6V3SqwCzSBVRgFlERERERCRPqkglIlK89intDI2OvSRkeHSY9int4TRIRAqiALOIiIiIiEgBVJGqupjZ0cDNzrnXg5+zcs5dVYFmidS11smtdM/sJjY7RmNDI8Ojw3TP7Fb2skiVUYBZRERERERE6sEVwO7A68HP2ThAAWaRCuia1kXH1A56l/TSPqVdweU8xAfiOm4SCQowi4iIiIhIzYrHa7OMRa2+rjKbCryU9LOIRETr5FYFSPPUs6iH2JwYTQ1NDI0O0T2zm65pXWE3S+rUKmE3QEREREREJBfxOMyb5+9z0dMDbW3Q2enve3rK27508m1zLqLwuqqRc67POTeU9HPGGz7LWUQkkuIDcWJzYgyODNK/tJ/BkUFis2PEB0rY2YjkQQFmEREREREpqSgEVeNxiMVgcBD6+/19LFbaNo2nHIHgKLyuWmBmP8vy2GrAbRVsjohIXnqX9NLU0DRmWWNDI71LesNpkNQ9BZhFRERERKRkohJU7e2FprFjbxob/fJKKFcgOOzXVUOOM7NvpS40s8nAX4FNKt8kEZHctE9pZ2h0aMyy4dFh2qe0h9OgGhMfiDNv8TxlhOdBAWYRERERESmJKAVV29thaOzYm+FhvzxXxWRilysQXIrXJQB8EjjDzI5LLDCzVfHB5anAPmE1TERkPK2TW+me2U3zhGZaJrbQPKGZ7pndqmNdAj2Lemi7qI3Oqztpu6iNnkdVhyoXCjCLiIiIiEhJRCmo2toK3d3Q3AwtLf6+uzv3CfGKzcQuVyC42NclnnPur8DngEvN7KNm1gzcCmwO7O2c+2+oDRQRGUfXtC76Tu5j7lFz6Tu5TxP8lYBqWxduQtgNEBERERGR2lDuoGos5gPWw8O5BVW7uqCjwwe429tzD8ImZ2IPDvplsZjfVq7bKLTNuSj0dclYzrmrzGx94A/Ao8Cm+ODy0+G2TEQkN62TW5W1XEKJ2taDI4PLlyVqW+s4Z6cAs4jUj3hcIzEREZEyimJQtbU1//0nMrEHV4wvl2di57OtbG0u9mtJIa+r3plZuit4fwxsDBwB7Av8J7Gec25ZBZsnIiIhU23rwqlEhojUh3LMOCQiIiIr6eqCvj6YO9ffd5Xwit3WVpgxo/yB1VJmYqdrs76WhGYEGE5z+xKwDvBw0rKh9JsQEZFapdrWhVMGs4jUvlJc5yoiIiI5q/bs2nJmYutrSajOBlzYjRCRyosPxOld0kv7lHYFCyWrrmlddEzt0PslTwowi0jtK9V1riIiIlI3ylXnWF9LwuOcOzPsNohI5fUs6iE2J0ZTQxNDo0N0z+zWhHiSlWpb508lMkSk9pVrxiERERGpaeUoyaGvJSIilRMfiBObE2NwZJD+pf0MjgwSmx0jPhAPu2kiNUUZzCJS+8p5nauIiIhIHvS1JDrMrAk4ANgamJTysHPOfa/yrRKRUupd0ktTQxODIysuG2lsaKR3Sa8yVEVKSAFmEakP5brOVURERCRPlfhaEo/nv/1CnlOtzGxD4H6gHV+X2YKHkms0K8AsUuXap7QzNDr2spHh0WHap7SH0yCRGqUSGSJSPyo19Xw1iMdh3jx/L5FnZuuY2UfN7BgzWytYNsnM1I+LiFSpcn4t6emBtjbo7PT3PT3leU6VOx+IA5vig8u7AZsB3weeCX4WkSrXOrmV7pndNE9opmViC80Tmume2a3sZZESUwaziEi96enx1+U2NfkikN3dPpVKIsfMDPgR8GWgCZ9VNQN4A5iNz7xSdpWIiCwXj/tufnBwxUSCsZjPmM4UzC7kOTXgg8BpwIvB78ucc73Ad82sAfgZMDOktolICXVN66Jjage9S3ppn9Ku4LJIGSjzSUSkniSPIPv7/X0spkzm6PomMAs4G59ZZUmP/Rn4aBiNEhGR6Ort9eeQkzU2+uWlfE4NWBt40Tm3DBgA1kx67C5g7zAaJSLl0Tq5lRkbzVBwWaRMFGAWEakndTqCrGKfBc52zv0AeCjlsWeAzSvfJBERibL2dn+BUrLhYb+8lM+pAS8A6wQ//xfYL+mxXYH3Kt4iERGRKqUAs4hIPanTEWQV2wj4Z4bHhoDJFWyLiIhUgdZWX/2quRlaWvx9d3f2UheFPKcG3A3sFfx8GXCamd1uZjfjy0/dEFrLRKSqxQfizFs8j/iArhKV+qEazCIi9SQxgozFfOby8HBdjCCr2GJgGn4QnGoH4LnKNkdERKpBV5evn9zb688h59LNF/KcKnc6sBaAc+6XZjYB+BSwKn7+g7NDbJuIVKmeRT3E5sRoamhiaHSI7pnddE3TfDdS+xRgFhGpN3U4gqxi1+MnG3qIFZnMzsy2Ak4FfhVay0REIiweVzfX2pr/ay/kOdXKOfca8FrS7z8Hfh5ei0Sk2sUH4sTmxBgcGWRwxM+YGpsdo2NqR8lqP8cH4pqsUCJJJTJEpLrF4zBvniapy1drK8yYEf1RpP6+ZwJPAvcBTwfLrgcWBb+fF06zRESiq6cH2tqgs9Pf9/SE3SIREakHvUt6aWoYO99NY0MjvUt6S7L9nkU9tF3URufVnbRd1EbPo8V3cCrnIaWiALOIRFu2AKNGkLVNf1+cc4P4WeyPBf4OzAXmAZ8HOp1zQxmfLCJSh+JxXwVqcBD6+/19LFbP5yklGzPb38y6g9rL96Xc7g27fSJSXdqntDM0Ovbr+fDoMO1T2ovednJ2dP/SfgZHBonNjhUVGC5HwFrqlwLMIhJd2QKMGkHWNv19MbMmM/sTsKdz7mrn3Gecc/s557qcc1c650bCbqOISNT09kLT2OQxGhv9cpFkZvY14Bbgo/hJc0dTbsvCa52IVKPWya10z+ymeUIzLRNbaJ7QTPfM7pKUssg1OzrXjORyBKylvqkGs4hEU3KAcdDXryIW87WDW1tXjCATj8GKEWTUyz7I+PT3xTk3ZGYdwE/DbouISLVob4ehlGs7hof9cpEUs4DLgFnOudGwGyMitaFrWhcdUztKXie5fUo77w69O2bZ4NDgmOzofCYYTASsE7WiYUXAWrWdpRDKYBaRaBovBUkjyNqmv2/CA8DuYTdCRKRatLZCdzc0N0NLi7/v7q6bc5OSnxbgegWXRaTUWie3MmOjGSUP1NoqlvH3fDOSy1nOQ+qTAswiEk3jBRg1gqxt+vsmnArEzGyWmW1sZg1mtkryLewGiohETVcX9PXB3Ln+vit98pbIbegkbqRp8jHJpt7eH71Lemme0Dxm2aQJk5aXyMh3gsFylvOQ+qQSGSISrnjcZyW3t48NHiYCjLGYz1weHl45wNjV5UtmpHu+VD/9fQEWBfc/JX2pDIf6chGRlbS21mu3IXmYBfzJzBxwO/Bm6grOuWcr3ioB8rvUX+pPPb4/xss4LiQjuVzlPOpdfCBel8dUg1IRCU9Pjw8gNzX5bOXu7rFpRrkEGDWCrG36+56NDyKLiIhIaTngbeD7wDkZ1mmoXHMkIflS/0R92NjsGB1TO+oqWCPp1ev7I5FxHJsdo7GhkeHR4TEZx+M9nm27tXzcKq0eT34kKMAsIuEYbxK/BAUYpY45584Muw0ikp9MF+bIWDpOEgFXAP8PuBB4EhjKurZUjCYfqx+FZHrW8/tjvIxjZSSHq15PfiQowCwi4UhM4je44ovB8kn8ojrS1GhYRESyGO/CHPF0nCQi9gZmOeeuCLkdkkKTj9WHQjM96/39MV7GsTKSw1PPJz9Ak/yJSDrxOMyb5+/LZbxJ/KKmpwfa2qCz09/39ITdIqkDZvbdcW7fCbuNIuIlX5jT3+/vY7HydqXVSMdJIuQ14JWwGyEr0+RjtS8507N/aT+DI4PEZsdymrBP7w+Jqno/+aEMZhEZq1JpRblM4hcVuZbzECm9M7M8lqjN/L0KtENExlGNF+ZA5S/OqdbjJDXpZ8AXzew259yysBsjY+lS/9pWbKan3h8SRYXWwa4VCjCLyAqVDqTmMolfFGQaDS9cCGuuGe22S1Vzzq10pZGZrQkcDJwKHFLpNolIetV2YQ6EU6qiGo+T1Kw1gWnA42Z2B/BmyuPOOXdG5ZslCbrUv3aVItMzyu+PQmpLS22o55MfCjCLyAphpBVVwyR+6UbDg4NwyCEqIJkL1a4uKefcm8BVZrY28AvgwJCbJCJU14U5EN7FOdV2nKSmfTvp563SPO4ABZhFyqCWMz0LrS0ttSPKJz/KSQFmEVlBaUXppY6Gh4Zg2TKVzMiFZnIqp0dQeQyRSKmWC3Mg3FIV1XScpHalu0pIRCqnFjM9k2tLJ8p/xGbH6JjaUfDrUza0VAt1qiKyQiKQ2twMLS3+XmlFXlcX9PXB3Lkwe7Y/NskSo3JZQTM5ldtHAR1MkYhpbYUZM6LfdYZ9TrlajpOIiJRP6+RWZmw0o2YCp4na0skStaUL0bOoh7aL2ui8upO2i9roeVQTzUt0KYNZRMZSWlFmiXIe8bgyvXOhmZyKZmaXp1nchK8Z+T506a6IFEilKsZSNScRESlWKWpLg89aXvjSQo6ffTzvjb6XVza0Mp4lLAowi8jKqqEucpiqYVQehZFy2OlxteHD+BqQyd4D+oCLgCsr3SARqR06p+ypmpOIiJRCKWpLJ2o4r2Kr8N7oe2MeS2RDZ9pe1Os/K/hd28y51HFr9Zk+fbqbP39+2M0QkXoThSBuOvmOlMv5OhJtSQ7E19Go3cwWOOemh92OUlKfKyK1JB6HtraxF9s0N/uqWFHq2iU79bciEiWFBlLjA3HaLmpbnrGcqnlCM30n96XdZrrnZlu/0qIe/JbcZepzVYNZRKRQUSwgmW/d454eP7Lu7PT3PSWu65Vcu7qvr66Cy6VgZkeb2doZHlvLzI6udJtERMIUj8O8eaUr55+o5pRM0yqIiEgxUmtLxwfizFs8j/hA9s4rXQ1ngMmNk2me0Jw1G7rU9Z9LKXnyw/6l/QyODBKbHRv3eEh1UYBZRKSW5DNSrtQkfFEMxFeP3wKbZ3hsavC4iISk1MFOya4c50RVzUlERMopn4n60tVwbp7QzI2H30jfyX1ZM35LVf+5HKIc/JbSUYBZRCSTaowc5DNSVtpWNbAsj00GRirVEBEZq9wXgFSTQrvLfJ5XrnOiiWkVmpuhpcXfR21aBRERqU75Zu4majg3T2imZWLL8qzl/bbYb9wyF5meG4XyGFEOfkvpaJI/EZF0qnXGn3wmIFTaViSZ2Y7AzkmLDjazaSmrNQNHAE9Xql0iskJysDNRu/f44/2EefUWmCy0u8z3eYlzosm1khPnRIs95prssD6ZWRPwTaAL2BSYmLKKc85pvCwiBUtk7ibXRR5vor6uaV10TO0oqIZzMc8tp1JMfijRp0n+RERSVWLGn3JPEJjr9ut8Er5yK2TSITM7Azgj+NWROYv5dSDmnJtTRBPzpj5XxGfddnb6TNpk3/senH56OG0KQ6HdZSHP02R8kk2B/e1PgS8BtwKLgKWp6zjnzipNC/On/lak+kV94r1KK3TyQ4mWTH1uzmdkzWwj4FTgQ8BawMecc4+a2cnAP5xzD5aqsSIioSpnmhRUJju6tTW3tiptK4ouAq7AB5afBQ4DFqassxR4xdXCWWKRKtTeDktXCkXBD34AJ5xQPx+lhXaXhTwvnwt0RHL0CeAM59z3w26IiNQmZe6O1Tq5tW5fez3IKcBsZtsDfwNGgX8AOwGJwp1twK7AkeVooIhIxZWzdES666pjsXCvq841GC0V4ZzrB/oBzGwq8JJzbij7s0Skklpb4dvfhu98Z+zyUp6LrAaFdpeFPk/nRKXEVsOPbUVEyiaqZStkLGVXFy/XSf4uAJ7Az1h/GGMv1/07sHuJ2yUiEp5EmtSkSTB5sr8vVZpUpon1Fi6svgkFpeycc30KLotE0wkn+BINyeqtjH2hE+QVM7FeayvMmKHgspTEn/FX54qIlFXr5FZmbDRDgcuI6lnUQ9tFbXRe3UnbRW30PFrHMzcXIdcSGR8Aupxz75hZQ8pjrwDrl7ZZIlJS5a73W6vMxt6XQrq0rcFBOOSQ6ptQUCrCzD4P/B+wNStPQIRzLrVfFpEKUMkGr9CsYmUjSwT8HLjKzJYBtwBvpK7gnHu24q0SEUmizNryig/Eic2JMTgyuLxWdmx2jI6pHTreeco1g3lZlsfWAQazPC4iYerp8bPidHb6+54qOxsXj1c+sze5jMXAgL+PxUrThtS0rUmTfAB7cNDPFlXKfUnVM7Oj8QPgecAk4LfA74C3gP8CZ4fXOpHaUWhX09XlJ5mbO9ff1+u5wUKzipWNLCH7B7AlcCbwIPB0mpuISNnFB+LMWzyP+MDYLyLKrC2/3iW9NDWMvcK4saGR3iW94TSoiuUaYP4XcFyGxw4HHihNc0SkpJIDpdUYvAwrOJ6pjEVvb2m2nxyRmD175WusS7kvqXYnA+fiM5gBLnHOHQNshj+5+3pI7RKpGcV2NQqSilSt4/Fj3OOCn9PdRETKqmdRD5teuCn7XLkPm1646fIgcnJmbf/SfgZHBonNjq0UhJbitE9pZ2h07BXGw6PDtE9pD6dBVSzXEhnfA+aa2e3AtYADOszsJOBQVLtKJJoKnd49CsKcDK+ck/wlJCbWi8fLvy+pZlsC9+GvJFpGMMGuc+5NM/s+8H3g4vCaJ1LdojjvajVTRS6pJs65K8Jug4jUt/hAnGP+dAzDbhhG/bJjbjxm+aSATQ1Ny8s2wIrM2lop3RCF8h+tk1vpntlNbHaMxoZGhkeH6Z7ZXTPHuJJyymB2zt0LHIKf5O9y/CR/5wEfBA5xzj1YrgaKSBEqESgtl3JnEWdTzOxDUd6XVKNBYBXnnANexmcuJ7wDbBhKq0RqRJhdTdQUW5Gq2itySf0yb3sz+6CZbWdWysk3REQyW/jSQh9cTjLshln40sKqyKzNVNojF1Eq/9E1rYu+k/uYe9Rc+k7uo2tandY8K1KuJTJwzt3snNsS2Ao/6d+2zrnNnHO3lq11IlKcag5ehh0cr2RhTRXxlMwWAVsEP/8N+JaZ7WFmM/A1I58Mq2EitSDsriYqig0OV3tFLqlfZvZZ4CXg38A9+H73RTOLhdkuEZFEZm3zhGZaJrbQPKE5Upm1xQSIo1j+o3VyKzM2mhGZ41uNciqRYWbfBX7jnHvROfcM8EzSYxsAn3POaaIhkSiq1mnaE8HxWMynkw0PVz44nihjUWv7kmryK1ZkLX8HmAvcH/z+Nv7qIhEpUBS6mrCVokxINVfkkvplZp/G97N34ifQfRlYH/g08Csze9c5p1x8ESmbnTbYiaaGpjGZyk0NTey0wU6Az6xNlMsIs4xEquQAcaKER2x2jI6pHTm1sR7Kf9SjXGswnwH8FXgxzWMbBo8rwCwSVdUavKzW4HguVKhScuCcuy7p52fMbHtgD2BV4O/OuddCa5xIjajlriYXpQgOKxNcqtTXgGucc0elLL/SzK4Gvg4owCwiZdM6uZUrDrmC2OwYq9gqLHPLVspSbp3cGrmga7EB4moo/yH5yzXAnK0O1ZrA0hK0RURk5cBrtQbHs+np8elhTU1+RN7drbIYshIzawJ+CFzrnJsH4JwbwGcxi0gJ1WJXk6tSBIeVCS5Vamt8kDmd3wE3Va4pIlKvopqlnE2xAWJNrFebMgaYzWxv4MNJi04ws4+mrNYMHAQ8VvKWiUj9qYfAaymuRZa64JwbMrMTgD+F3RYRqV2lCg7Xeya4VKW3gY0zPLZx8LiISNlFMUs5m1IEiKsxsC7ZZctg3gs4PfjZAcelWWcIeBw4scTtEpF6Uy+BVxWqlPwsBN4H3Bd2Q0SkdpUqOFzPmeBSlW4FfmBm/3HO/S2x0Mz2AM4JHhcRkTS6pnWx43o78q/F/2LXjXZl29Zt895GtQXWJbuMAWbn3FnAWQBmtgzY3Tn3r0o1TETqTL0EXlWoUvJzKtBjZn3Azc45F3aDRKQ2KTgsdehrwO7APWa2GHgJP8nfxvhJ7TOVzxARqXs9i3qIzYktn6Swe2Y3XdNq7OpjycsquazknFtFwWURKat6CbwmrkVuboaWFn+vQpWVE4/DvHn+vjpcD6wNzAbeM7P/mdnzSbe+kNsnIiJSlZxzLwM7AicB/wDeAv4JfBnYyTn3SnitExGJrvhAnNicGIMjg/Qv7WdwZJDY7BjxgaoZY0kZ5DrJ33Jmti4wKXW5c+75krRIROpTPc0QpEKV4ajOGt934stUiYiISIk5594FLg5uIiKSg94lvTQ1NDE4suLq48aGRnqX9KrkRQXEB+KRrF2dU4DZzFbB16E6AZiSYbWGErVJROpVusBrPF6bgVhdi1xZVVrj2zl3bNhtEBERERERSWif0s7Q6Nirj4dHh2mf0h5Og+pIlEuT5FQiAzgZ+BJwAWDAD/AB5+eA/wKfK0fjRKQOtbbCjBn+vqcH2tqgs9Pf9/SE3broqL5SD+FK1PhOlqjxLSIikaCuTcrNzJ41sx2Cn58Lfs90+2/Y7RWR+hAfiDNv8byqKTHROrmV7pndNE9opmViC80Tmume2R2pbNpaFPXSJLmWyDgOOBu4CB9Y/pNz7iEzOwe4Hdi0PM0TkbpVpRmnBcsnU7s6Sz2Eq4prfJvZTsB3gA/hryLaNeiDfwDc55z7a5jtExEpBXVtUiH34mstJ35WGSoRCVWUM1Kz6ZrWRcfUjkiWaqhVUS9NkmsG82bAfOfcKDACNAM454bxQefjy9I6EalOpUhBqqeM03wytZMD7/39/j4WU7rXeKp0ckUz+wB+4qFtgGsZ228vA74QRrtEREpJXZtUinPuOOfcc8HPxwa/Z7yF3V4RqW1Rz0gdT+vkVmZsNCMSwc16EPXSJLkGmPtZMbHfi8DWSY9NANYqZaNEpIqVqqxFFWec5iXfUXU9Bd5LrasL+vpg7lx/Xx2pcecBtwHbA19JeewhYOeKt0hEpMTUtUmUmNnaYbdBROpDIiM1WSIjVSRV1EuT5FoiYyGwHX6QextwlpkN4rOZv48f5IpIvStlWYtExmks5keZw8NVkXGat8SoenDFZS7LR9XpXmu9BN7LpfomV9wZOMw558ws9TLe14CiX4yZTQF+A0zDXyp8vHPuH8VuV0QkV+raJAxm9jlginPu/OD39wG3AhuY2ULgo865l8Nso4jUtqhnpEr0RLk0Sa4ZzBcB7wY/nwG8DFwDXAc0ArNK3jIRqT6lTkGqzozT/OQ7qq7SUg9SsPeAVTM8tgH+CqNi/RT4q3NuG2AH4IkSbFNEKqASk+JVYh/q2iQkXwaSzvDzE2AJfoL7NfBzEImIlE3UM1IlmqJamiSnDGbn3B1JP79sZrsCm+MHvU8EtZhzYmb74wezDcBvnHPnpVlnb3xQuxF4zTm3V67bF5EQlSMFqfoyTvNTSKZ2V5fPCs91UkCpZvcDJ5vZ7KRliUzmGHBXMRs3sxb85IHHAjjnhoChbM8RkWioxKR4lZx4T12bhGBT4EkAM1sD2As4xDl3i5m9Dpyb64Y0xhWRQlU6IzU+EI9k9qtUv3EDzGbWhM9UvtA5dx+Ac84Bz+S7MzNrAH4BdAIvAPPMbI5z7vGkdaYAlwD7O+eeN7N1892PiISkXspalFoho+paD7xLwneAB4BHgBvwweVjzOwnwC7AjCK3vxkQB35rZjsAC4CTnHMDRW5XRMqolBWpwtxHKnVtUmEN+AlzAT6A72PvCX7/H5DTOFRjXBEpVuvk1ooEe3sW9RCbE6OpoYmh0SG6Z3bTNa0GrxKWUIxbIiPIZurIZd0c7Ao845x7Ntju74GZKescCdzonHs+2P+rJdiviFRKPZS1EKkQ59wj+AzjV4BvA8aKslR7OeeeKnIXE/B1nn/pnNsJGAC+kbqSmX3ezOab2fx4Oa+TF6lT+ZahqMSkeJp4T+rA08BBwc9HAH93ziXKQm4IvJHjdjTGFZHIiw/Eic2JMTgySP/SfgZHBonNjhEf0Hd7KY1cg8YPALuXYH8b4c8GJ7wQLEu2FbCmmd1jZgvM7OgS7FdEKqm1FWbMUBpSrnp6oK0NOjv9fU9P2C2SCHHOPeSc2xdYHdgYaHHO7eOcW1iCzb8AvOCcezD4/QZ8wDm1Db9yzk13zk1v1f+1SEkV0gVUYlI8TbwndeDH+DJUr+EDwD9Pemwf4N85bkdjXBGJvN4lvTQ1jD1z3NjQSO+S3nAaJDUn1wDzqUDMzGaZ2cZm1mBmqyTfctyOpVnmUn6fgL/s9yDgI8B3zGyrlTakbCqJmkrMghPFfUtxkq9B7u/397GY/payEufce8BwUnZVKbb5MvA/M9s6WLQv8HiWp4hICWXrArJ17ZWYFE8T70mtc85di6+7fC6wj3PuxqSHX2FswDkbjXFFJPLap7QzNDr2zPHw6DDtU9rDaZDUnFwDw4vwk/r9FOjDTwA0nHTLdUKgF4BNkn7fGHgxzTp/dc4NOOdeA+7Dz2o/hrKpJFLCzEBV9mt1y3Stsa5BloCZ7WVm95rZIPCymQ0GGVAfKtEuvgxcY2b/BnYEflCi7YrIODJ91F922fhdeyUqUqnqldQ659z9zrkLEnMNJS0/wzl3S46b0RhXRCKvdXIr3TO7aZ7QTMvEFponNNM9s1sT/UnJmJ+vb5yVzM5k5bOwYzjnzsphOxOA/+AzpBYD84AjnXOPJa2zLXAx/sxuE/Av4Ajn3KOZtjt9+nQ3f/78cV+HSFnE4370l5gBB3yaT19ffmk+8Xj+U6dn2veCBfDOO5qGvRo88QRst93Kyx9/HLbdtvLtkZIyswXOuelFPP+T+FqO/wGux2dUrQ98AtgS3z/eUIq25kp9rkjpZOoCJk2C995b8XshXytE6kmu/a2ZbQq85JwbDn7OKlEzeZxtaowrIlUjPhCnd0kv7VPaFVyWgmTqcyfk8mTn3JmlaIRzbsTMZgG34Wftvdw595iZfSF4/FLn3BNm9ld8zatlwG+ydbwioUvMgpMc5E3MgpPrSLCnx18T29TkCx52d+eWJpRu387BTjv50Wk+2wpLIYH1WvLOOz5ykPw3nDTJLxeBs4GbgUOcc4mZ7jGzM4A5wPfwdZNFpAql6wKamqChYex6+X6tEJGMngP2wAd4exkniQo/Zs1KY1wRqSatk1sVWJayyCnAXErBpUa3pCy7NOX384HzK9kukYIVOwtOcgHGxAgzFoOOjvFHkun2nUh5Wro0v22FodDAei1J9z4x0yxKkjAV+EpycBnAObfMzC4B/hhOs0SkFNJ91K+yCixbNnbZ8DCstpqvyVyv52NFSuR44L9JP49/OW8ONMYVEZF6l2sNZhHJpNhZcBJZyMkSqUr57nviRP9zIduqNE1u52kWJcnuaSDTm6EVeKaCbRGREkvXBVx++crLYjHYZRdNtyBSLOfclc6514Ofrwh+z3gLu70iIiLVouIZzCI1qavLZwkXUuqh2Azo5H2vtpofgRa6rYRKlK0oRWmRWlHM+0dq3beBn5rZE865eYmFZrYbcCZ+gj4RqWKZuoDUrr2QC51EJDMzawXWdM79J81jWwFvBBPyiYiIyDiUwSxSKq2tMGNG/qO9UmSwJva97bbFb6unZ/yp60uh2MB6rSn0/SO17qvAJOCfZtZrZg+aWS/wd2Ai8DUzuy+43RtmQ0WkcOm6gMSyd95Z+UKnhoZoXpwkUmUuAU7N8NgpweMiIiKSAwWYRaKgq8tPDz93rr8vpg5xMduqZNmKeiwNEY/7Apr1VgZEijEKPAnch5+Y6N3g/j7gqeDxxG1Zhm2ISBVLdz72nXfgoYdCaY5ILfkAfmK+dG4H9qxgW0SkhsQH4sxbPI/4gMZ9Uj9UIkMkKlpbSxdcLXRblS5bUU+lITShoRTAObd32G0QkXC1tsKFF8IXvjB2+SmnwGGH1XbXKVJmawL9GR57C1i7gm0RkRrRs6iH2JwYTQ1NDI0O0T2zm65pGvdJ7cspwGxmR2d5eBm+Y17onHuhJK0SkXCEUbailIH1qErODFcBTRERydPOO8Pqq8Pbb69YVq/TFoiU0AvAbsCdaR7bDXipss0RkWoXH4gTmxNjcGSQwRE/7ovNjtExtYPWyeqwpbblmsF8BeCCny1pefKyZWZ2HXCccy4lQiUiVSFRtiIW8yPX4eHaL1tRCZrQUIpgZhOAPYBN8PWYx3DOXV7xRolIRbW3w8jI2GWlOv9biXl9q6ENUpduAL5lZo84525OLDSzg4BvAL8MrWUiUpV6l/TS1NC0PLgM0NjQSO+SXgWYpeblGmDeE7gG+DO+I34FWA84HPgo8EVgGnAW0Ad8q+QtFZHKqKeyFZWiCQ2lQGa2M/AnYGPGnuBNcIACzCI1LtP5X/Cl/QvtrqNQvSkKbZC6dTbwIWCOmb0MLAY2AtYH/okf24qI5Kx9SjtDo2PHfcOjw7RPaQ+nQSIVlOskf6cBv3fOneSc+5tz7j/B/ZeBHuDzzrnzgQuAI8rVWBGpkHTT2YclyhPj5dq2epzQUErlUuAd4BBga2Bqym2z0FomIhWVOocvQFsbdHb6+56e/LZXyXl9o9wGqV/OuXeBvYDP4SfPXQLcC8SAvYLHRaQOFTpJX+vkVrpndtM8oZmWiS00T2ime2a3spelLuSawdwJXJLhsbuAWcHP9wFfK7ZRIpKDerieNMppTfm2TZnhUpjtgMOdc7eE3RARCV9i2oJSlPaPQvWmKLRB6ptzbhh/JZCuBhIRoPhJ+rqmddExtYPeJb20T2lXcFnqRq4ZzEPALhke2yV4PLG9gWIbJSLj6OkpLm2pGkQ5ranQtuWTGR7lzG2ppP8Ak8NuhIgUrhwf54nAbLJEYDZXUajeFIU2iJjZ+81slpmdYWbrB8u2MLPVw26biKxQaFZxvvtITNLXv7SfwZFBYrNjBWUyz9hohoLLUldyDTBfD5xlZqeaWZuZNQf3pwFnAtcF6+0IPFX6ZorIclEOvJZSKUbP5VLuttXDCQTJ1beA081s07AbIiL5K9fHeSkCs1Go3hSFNtQDnbNOz8wmmtn1wELgZ8B3gQ2Dh38EfDustonIWD2Lemi7qI3Oqztpu6iNnkfLMz5KTNKXLDFJXzEqERwXCVuuAeavAH/Ed7TP4utBPgv8ED/p36nBeo8CXy9xG0UkWZQDr6UU5bSmcratXk4gSE6cc38FbgWeNrNFZnZfyu3esNsoEqYoB87K+XFeqsBsal3nMKpQRaENtUznrLP6PtABHIWfwD55Mt1bgY+E0SgRGatUWcW5KMckfeUOjit4LVGRU4DZOTfonPsMsC1wLPDN4H5b59xRzrn3gvVuds7dV6a2ighEM/BajhF+ptEzjN1XGNGFcqZc1csJBMmJmX0DP7fBEuAtYDTltiy0xomELOqBs3J/nOcTmM3WVUZhXt8otKEW6Zz1uLqA051z1wJvpDz2HNBe8RaJyErKlVWcTqZJ+oCCgrjlDo5XKrNbJBe5TvIHgHPuP/h6kCISltZWPzq4+OIVy2Kx8EZl5ZyIL3VivLlzfRQhsa9YzO8vjEkAyzVpXxRPIEiYTgYuA2Y550ZDbotIZJRikrtyq8THeWLSv2yiPF+ulJcmURzX2sATGR5bBZhYwbaISAblyCrOJnWSvrnPzqXtoraCJv1LBMcHR1Z8ECeC48XWZ04OXie2H5sdY8f1duSdoXc0waBUXK4lMgAws/XNbFcz+1DqrVwNDFWUr7uU+hWPr8jkTejuDud9WonUmERaE6y8r4svDjctpxwpVypIKWOtClyv4LLIWNVwsUcUPs6VwVrfdM56XM8Be2R4bFc0t5BIJGTKKi5n8DQxSR9QVAZyrsHxQspcpMvsBtjpsp2U0SyhyCnAbGYbmdndwGLgH8A9wN3BLfFzbYn6dZdSv6I0qq5kW9LtK1XUoguFUkFKWeFWMg9+RepWtQTOKv1xnpobEaWvDFJ5UTjJEXFXAd8ws08Dif8UZ2b7AKcAl4fWMhEZo2taF30n9zH3qLn0ndyXcwZxsYotz5FLcLzQMhfpgteDI4MsHV1a9lrVIunkWiLjl8A0fB3IRcDSsrUoCqrhukupX7mMquPx0pduKLQt5dxXqihGFwqVy3XPUg8uAq4wM4C/Am+mruCce7bCbRIJXSJwFov5gOnwcOUCZ/l2sZX6OE9XCqOjozoC8VI+5aroVSN+BOwAXA38Jlh2PzAJ+L1z7udhNUxEVtY6ubXiJR9KUZ4jteRG8mvIVOaiY2rHuK81EbyOzY7R2NDI0pGlrMIqDI6WvhyHSC5yLZHxQeArzrkLnHO3O+fuTb2Vs5EVp3QPibLx0lEKyb4vtBxMJVNj0u1r1iyl5UitewDYEvge8C/g6TQ3kboUxsUeUb3ALVMpDKiODFZVpSsvTaKYnnNu1Dl3BLAXcAE+yPwz4MPOuU+H2jgRKblCylCUqjxHouRG6vOKzZBOzuxeeMJCsLGPl7NWtUgqc86Nv5LZi8Bxzrnbyt+k/E2fPt3Nnz+/dBuMx/2oIXlGjOZmP3rRNzOJinQpVIW8d0sx+0+lMqbT7auS+xbJk5ktcM5NL+L5xwJZO2rn3JWFbr8QJe9zRapElL8ezpvng979/SuWtbT44PuMGb7tCxf65TvtFH57k2kSQimFYvvbKFJ/K1I6PYt6iM2JFTRRH/jgdLoM5GLFB+K0XdQ2ZhLA5gnN9J3cV9B+eh7tWZ7RPDw6nPfrFMlFpj431xIZvwaOAiIZYC65MK+7FMlVumtu850uvFTlYCpZziF1XyolITXMOXdF2G0QES/fLraSxqtYNXduNIO4qkonIiIJ5QziFlqGIqFc5TlSy1wkgsKF7itbOY56U673k2SWa4B5MXCUmd0F3AK8kbqCc662JkFQwTKpRvnWRI7yaDlBGcoimNkqwHbA2sB859xAyE0SqTtRnlgwW25ElIO41fA1RGqLmS1jnCuDkjnnGsrYHBEJFJthnE2iDEVylnCUahOXOigcRq3qqCnn+0kyyzXAfGlw3w7sneZxRy3OsqvMSKk2+WbfR3m0DLpuVgQwsy8BZwDr4PvbGcBDZnYTcJdz7mchNk+kbkT9ArdMuRFRDuJG/WuI1KSzWRFgNuB4oBn4M/AKsD7wUWAQ6A6jgSL1phQZxtmUYqK+clNQuHTK/X6SzHKd5G/qOLfNytI6EclfPrMeVXKSvnxlmrFIMwBJHTGzzwE/BW4CDmfs1B1/Az4eQrNE6lYYEwvmI91kblEO4kb5a4jUJufcmc65s5xzZwEjQB/Q5pw73jn3TefccfikqueDx0WkzIqd6G48rZNbie0UG7MstnOsZoKNhUxeWMttKff7STLLKYPZOddX7oaISAnlk30f1XIwUU65EqmcrwAXOOe+bmapl+k+CXw1hDaJ1LVqu8CtWjOvRSrgBOBLzrl3kxc65wbM7MfAz4Hvh9IykTpS7gzj+ECc7oVjL0jofqib737ou1UfZI5SKYiotKUaMtZrVa4ZzCJSy9KlPIUtyilXIpUzlcwT7A4AUyrXFBGpVtWYeS1SAesATRkea8LPeyBS06KQcZqY6K55QjMtE1tontBc1ER3qcLIaK3EcU0uBdG/tJ/BkUFis2Oh/C2j1JZyv58ks4wZzGb2LHCoc+4RM3uO7JMhOOfc5iVvnYhEQxgT7UU95UqkMl7DX6qbztb4SXhFJGTVMB9ttWVei1TAfOAsM/uHc255f2pmGwFnAvPCaphIJUQl4xRKP9FdskpntFbquEZp8sIotQXK+36SzLKVyLgXeCvp55xn2xWJrGoYASZEpa1hTrSn62ZF/gx818zuwdeJBHBmtg5wCr42s4iEaLxuMirduYis5ETgLuC/ZvZP/CR/6wG7A+8CR4bYNpGyiuJEaOWa6C6R0RqbHaOxoZHh0eGyZbRW8rhGqRRElNqSoIkTKy9jiQzn3HHOueeCn48Nfs94q1yTRQrU0wNtbdDZ6e97esJuUWZRaWs5JtqLx2HevNy3oetmpb6dDiwFHgXm4k/2/gx4AhgFzg6vaSL1I1PXNV43GZXuXERW5pxbCGwBXIDvU98X3P8Y2NI593B4rRMpr3qbCK1rWhd9J/cx96i59J3cV7ZM7Uoe1yiVgihVW6JQskUKl9MkfyJVL3kEmJg0Lhbz2bFRC1xGqa2lnmgvzGzoaqOUNwGcc6+b2XTgZOAjwH/xfffFwIXOubeyPF0qSP+ytStb15Wtm4TodOcikp5z7nXg22G3Q6TSophxWqz4QDxrSYRKZLRW+rhGqRREsW2JUskWKUzOk/yZWYuZHWFmXzOz76bcvlPORooULTECTJY8AoySKLW1lBPtlSMbulYp5U2SOOfeds59zzn3AefcVs65PZxzZym4HB36l61d43Vd2brJKHXnIiIiyaKU/VoKPYt6aLuojc6rO2m7qI2eR8P5MhbGcW2d3MqMjWZE4m9XaFuiNEmgFC6nDGYz2xNfB3JKhlUc8L0StUmk9EoZKC23KLW1lBPtlTobOlfVllYYpQx2CV3yhLtpHpsGzHHObVb5lkmC/mXzU20fyeN1XeN1k/l059V2bEREpLpFKfu1GOWoezxeNnQ2tXJcKylqkwRKYXLNYL4I6AVmAJOcc6uk3BrK1UCRkkiMAJuboaXF3xcaKC23/9/evYfHdZX3Hv++kSVbKLiGMFzqJFJSKIT6QB0sCqUEAnK4tgZOoQgISRhKUwgQyjnlViBcCqW0xZRwRyEXWsHhUuxCSoKBBEIKOCS0TkiBQKQQA2FIYhMLxZad9/yx9kSj8Yy0Z2bPvsz8Ps+jZzR7z2Vpz2jWrHe/6115a+vkJMzOwo4d4bLdkhZZBM6LmFaolDdZagxY3WTfGmA0vaZII/qXja+IH8lxuq5m3WQr3XkRj42IiBRf2tmv9TV2k6i5m3Td4ySyodM8rr1Qt7gXS7b0I3P3lW9ktg94jrtf3P0mtW7Tpk1+1VVXZd0MKYIipQd1s61ZHYdqIcvaNK9u1WCuVMIovTbtbHg4jP7z/NoXtd3SkJl91903dXD/u4A/cPedDfadCfytux/VSRtbpT53Kf3LxlPk49Rp17VSl5vVsSnSVyKRlXTa3+aR+lvpNfU1dssby0xdM9Vxzd3KXIXRraNLsl+HVw0ze/ZsW6UaknqsVp+3nYznXqpbPH3tNOVtZQYHBlk4tFDov6XXNetz42Yw30TzDCqR4iiVYHy8GCOpbrU1yzSppLKh4yhqWmHeMtgldWb2KjO7ycxuIpSg+vfq9ZqfCvB+4EvZtlb0LxtPUT+SofOua6XuPItjo4xpERFJU6Mau+fuPDeRmrtJ1j1OOhs6jnYzpnutbvHkhklmz55lx6k7mD17VsHlAopVgxl4C/BaM/uKFhUSKbA8FAutFq3stjzVsm7V5GR4TZRa1q9+Anwl+v004Cqg/pvifuD7wMdSbJc0oX/ZlRX5Ixm623WlfWzy8FVARET6S6Mau/U6qbmbVN3jtEs1dFI/uhfrFpdGSoVtu8QPMD8duB9wo5n9J3Bb3X5399MSbZlI3hVxbmlWC+1lIckFCrOQViBecsfdtwHbAMwM4K3ufmOmjZIV6V92eUX/SO6mtI9NP30VEKlnZue1cHN393LXGiPSRxoFbut1GshNIjhZzYauL9XQraBnJ0Fi1S2WvIkbYH4sYZrur4Hfa7B/5ULOIr2kWpBxaCikHXWzlnCSip5C1iqlFUrBufsZWbdBJCn6SG4uzWPTb18FROo8gaVj13XAbwEHgVuBowhj5L3A7Wk3TqTomtUSbhS4LZ9YZurqqVQCua1IKhs6jk6CxGkHw0VWEmuRv7zTAgiSqiKvVATpLrQn0ue06JCI5JG+Ckivaae/NbM/AqaBVwOfdfdDZjYA/CnwD8CfufuVybc2HvW3UjRxFpyrD0C3u7hdL+l0cTsdQ0lbsz53xQCzmQ0B3wJe6+6Xdql9HVHnK6nauTOsirN37+K2tWvD6j/j49m1qxVFLO8hUkAKMItIXumrgPSSNgPM3wHOd/cPNNj3MuA0d39kUm1slfpbKZLKXIXRraNLSj0Mrxpm9uxZBT1jSDJIrICzdFuzPnfFEhnufsDMjiNMGxKRXphbqmKhIiIifU1fBUT4X8ANTfb9CNiQYltECq0XF5xLU1KL28XJIhfpliNi3u7LwCndbIhIYVRX4xkeDpnLw8NaqUhERES6rlIJE6kqlaxbItITfgE8p8m+5wK3pNgWkULTgnPZq8xVKG8vM39wnr379zJ/cJ7ytjKVOX1pkHTEXeTvfcAnzGwV8Hng59Qt7OfuP0m2aSI5ppWKukdzdkVERA5T1PWFRXJsK/AeM3sA8GlCQPl+hKDzk4CzM2uZSMFowbnsKYtcshY3wHx5dPlXwKua3Gag8+aISF/T6FnkMGa2EXgjcBJhtftHuvvVZvYO4Ovu/qUs2ycirWnnPGqlErrH+fnFNYbL5XCuW+diRdrj7u81s33Am4Gn1Oz6KfDn7n5eNi0TKabJDZNMHDeh+r8ZURa5ZC1ugPmMrrZCpGgUCE2eRs8ih4lWuN8B/AT4V+Csmt13AWcCCjCLFES7Xx9mZsJ95heTkhgcDNvVRYq0z92nzOw84GjgAYSZuje7uy9/TxFpJKlawtI6ZZFL1mIFmN39gm43RKQweiEQmscyFBo9izTyd8AlwDMIM4VqA8xXAy/MoE0i0oZOvj70wvrCInkVBZN/Gv2IiBSWssglS3EX+RORqmogtFY1EFoE09MwOgqbN4fL6emsWxRo9CzSyInAB6PBb3021a8AfWsUKYhOvj5ofWGR7jCzjWb2OTP7lZkdNLMTo+3vMLMnZ90+EZFWlUZKjK8fV3BZUhe3RAZmdl9gEngwsKZut7t7OcmGieRWkQOhec6+ro6ey+Uw4l5Y0OhZBO4E7tFk3wOAvSm2RUQ60OnXB60vLJIslaESERFJTqwAs5k9GPgWYXruCCFr6t7R9dvRAFf6SRECoc1KYCRVhqJbJTY0ehapdwVwtpltq9lWzWQuA19Nv0ki0o4kvj6USuoaRRKkMlQiIiIJiVsi493Ad4D7AUZYZXcYeDHwG+CZXWmdSF5NTsLsLOzYES7ztMDfciUwksi+7naJjVIJxsc1ghYJ3kgok/Ff0e8OnGZmXwMeBbwlw7aJpK5SgZ07w2UR5fnrg0gfUhkqkYRV5irs3L2TylxBO2oRaVvcAPM48AFgf/V+7n7Q3c8D3gds7ULbRPItj4HQ2hIYe/eGy3J5cSTeaRHHlR5fRBLl7v8FnATcAryBcJK3mmH1OHf/QVZtE0lbXpcQaFXcrw9FD6aLFIDKUIkkaHrXNKNbR9l80WZGt44yfW1BO+ouUOBd+kHcAPORwG3ufheho71Pzb6rCAFoEclanBWEOkmfKvoChyIF5O5Xu/sTgXsCRwNr3f1kd78m46aJpKbfzm/2SjBdJOeqZagGarapDJVIGypzFcrby8wfnGfv/r3MH5ynvK2sgCoKvEv/iBtgngHuH/3+A+DZNfueDuxJrkki0ra4JTDazb4u8gKHIgXn7ne6+8+ANWb2CDNbnXWbRNLST+c3uxlMV1a0yBIqQyV3U4ZpZ2b2zDA0sLSjHhwYZGbPTDYNygkF3qWfxA0wfxnYHP3+T8AZZvYDM7sOeCVwXjcaJyIt6rQERtaPLyJLmNnfmNk7a66fRDjp+x3gR2b2oKzaJpKmPJzfTCs4261gurKiRZZSGSqpUoZp58bWjXHg0NKOeuHQAmPrxrJpUE4o8C79JG6A+XXA/wFw9/8HbAF2ErKZ/xJ4c1daJyKt6/YKQlqhSCRNLwB+UnP97wmZVs8gDIjflkGbRFLX7fObKwWP0wzOthJMjxv07rcSIyJxqQyVKMM0GaWRElNbphheNcza1WsZXjXM1JYpSiPdS0QqQtZ5GoH3IhwH6Q+r4tzI3fezuMAf7v7vwL93q1Ei0qFSqbtZxd1+fBGpWg/8CMDMSoQ1D57o7peZ2RDwz1k2TiRNk5MwMREyecfGkuuGpqdDsHVoKAR2p6aWnjutDc7Oz4dt5XJoSze6wmowvVwOmcsLC42D6Su1u1Y1K7rafljMilZ3LhLKUAE/M7N7m9kJwLXRGFh6XDXDdP7g4gdkNcO0m8HRXjS5YZKJ4yaY2TPD2Lqxrh6/6V3TlLeXGRoY4sChA0xtmWJyQ/4Sn6qB9/K2MoMDgywcWkg08F6U4yD9IVaAucrM7kOoR3UU8O/ufpuZrQEORAsAikhclUryo2QR6TWHgOq8upMIK95/M7peAe6dRaNEspL0+c1GweMzzoDf/3044YRwPYvg7ErB9FaD3nkoMSKSN2b2N8CIu78uun4S8AVgBNhtZk909x9l2UbpPpV2SFZppNT1wHxt1nn1xEB5W5mJ4yZyeVKgW4H3oh0H6X2xSmRY8G7gZmA7oebyWLR7G6FmlYjEVfRCiFolSCQt1wIvMLMjgRcBl7v7QrTvGOCXmbVMpAc0qne8fz9s3LjYNY+NwW9+s/Q28/PdD84utx5vq3WatYSCSEMqQyWZlHaQzhSxrnFppMT4+vFE31dFPA7S2+JmML+OsODBWwkL/n27Zt+/A6eiDlgknrTn2iatlTm5ItKptxFO5D4fWACeVLPvqcDVWTRKpFc0yuyFEGSuds0AZkv3119PWzsZyd0qMSJSYCpDJUC6pR26oTJXKWzb26Gs80DHQfIm7iJ/Lwbe6u7v4PDB7A3A7yTaKpFe1izt6Jpr8p8VrFWCRFLl7pcAJwDPAX7P3S+v2f114F2ZNEykYJpNvKlm9q5effh9qhnBMzMh47fWmjXNs4XT0G5G8nJZ0SJ9SGWo5G7dyDBNw/SuaUa3jrL5os2Mbh1l+tqCzYxtg7LOAx0HyZu4GczrgW812XeAUKdKROJolHY0Pw9btsDAABw6BOedl8+sYK0SJJKaKHvqU8B73P2z9fvd/cPpt0qkeFaaeDM5GWoub9wYMperajOC81i/WBnJIh2rlqG6EpWhkgLq5xq8Rc86T4qOg+RJ3Azm3cCGJvseDtyYTHNE+kB92tGaNXDXXXDnnTA3Fy5POy2fWcFaJUgkNe5+AJggfl8tInXiTrw54QT4+McbZwTnuX6xMpJFOvI2wgyhvcATWTorSGWoJPf6vQZvfdZ5Za7Czt07qczlcBzdRUXNvpfeE3fQ+mngTWb2mJptbma/C7wa+GTiLRPpZZOTMDsLO3bABReErOVaCwuhZEZc9XN/u7UIX55H2SK96ZvAo7JuhEhRtbIYXm3XPDt7eJZzs30iUkwqQyVFpxq8i5IuFdKvwWqRTsQtkXEO8IeEjnY22vZpwtShK4G/S7xlIr2umhZ1++2dPU517u8RR4RM6HI5BH27tQif5uSKpOnVwOfNbB/weeDngNfewN3vyqBdIoXQ6sSbatfc6j4RKSZ3v5EGs3FVhkqKoFqDt7ytzODAIAuHFrpegzePCwomXSpketc05e1lhgaGOHDoAFNbppjcoDPLIiuJFWB293kzezzwPMIK9jcAtxKmFf2Lux/sVgNFet7GjYvB4KqhobB9JZUKnH760vuee264rNZJLpdDQDjJUbFG2SJp2RVdvjf6qefEP1ks0neqE2/K5ZC5vLCQ7sSbSqX5+djl9knrdDylXWZ2L+BBwJr6fe7+9fRbJBJfmjV48xp4rZYKqQaXYbFUSKvHo5/rWot0Kvag1N0PARdFPyKSlFIJzj9/aRZy3NHvNdccnppVT4vwiRTZW6nLWBaR1mQ18Wa5xQVXWnhQWqPjKe0wszXAeYQSGdbkZgPptUikPaWRUtvBz7gZyXkOvCZZKiTJYLVIv1HWk0gedHP0q0X4RArL3c/Jug0ivSDtiTe1iwvWTyiq/t5on84Ft265Y63jKSt4I/B44DRCEtXLgDuB04EHAK/MqmGSvjyWfui2VjKS8xx4TbJUSLfqWvfj+0v6T9MAs5ndSPysKXf330mmSSJ9qp3R78aNi3N+qwYGwrahofTnAotI15jZkcBRwM/cfWGl24ukQWUJGqsuLji/OA5fsrhgs306hq1b7ljreMoK/jdhptAnCQHmb7v71cDHzezTwJOB/8iwfZKSvJZ+6KZWM5LzvqBgUqVCulHXuh/fX9KflstgvhxNyxXJt1IJLrgAXvSiEFg+dAjOO0+L8In0EDN7OmEA/PBo0zhwtZl9DPiqu/9rZo2TvqayBM2ttLhgKwsPyvJaXchRpMaxwHXufsjMFoCRmn3nAR9HWcw9L8+lH7qp1Yzk0kiJ8sYy5+489+5t5RPLuTpGnZQKqZVkXet+fX9Jf2oaYHb301Nsh4i0q1l5DQWWRQrPzJ4BfBb4CvAa4O9rdt9ImNarALOkTmUJlqrP5F5pccEsFx7sNVkv5CiFditwZPT7Twkncr8RXb8PMJxFoyRdeS790E2tZiRX5ipMXTO1ZNvU1VO86aQ39eRxSipY3a/vL+lPR2TdABFJQKkE4+MaTYn0njcDH3f3U4CtdfuuBTak3iIRFssS1KotAdFPpqdhdBQ2bw6X09Nh++QkzM7Cjh3hsja7e7l90jodT2nTt4CN0e+fBd5mZq8zs/8LvBu4IrOWSWryXvqhW6qlIIZXDbN29VqGVw0vWwqiGiitVQ2U9oPKXIWdu3dSmau0dL8k3l/tPrdI2rTIn4iISH6dAPx19Ht92arbCTWZRVKnsgTBSpncaS8u2M90rKUN7yKUyQB4O/BAQkmqAULw+S8zapekqBs1d4uilVIQ/RqIh85qKHf6/lL9ZikSBZhFRETy69eEabqNjAFKZZBMqCxB0O4Cc6pfLZI9d78KuCr6/Q7gf5vZamC1u/8608ZJqpKsuVs0cUtB9GsgPokayu2+v1S/WYpGAWYREZH8+jLwOjP7D+COaJtHA+CzSGh1ezMbIAyyd7v705N4TOl9zZYA6CX1tZXrtZPJrfrVxbHS6y+9x933A/uzboekL6mau72sHwPxSdVQbuf9pfrNUjSqwSwiIpJfbwDuD/wA+BihTMZrge8BRwPnJPQ8rwSuT+ixpI/08hIAzWor16pmcg8Pw9q14XKlTG7Vry6GOK+/FJ+ZPcjMLjCzH5rZXHR5vpk9MOu2icSVZo3e0kiJ8fXjfRPgzLI0SD+XJZFiUoBZREQkp9x9BjgR+AKwGTgEnESoDfkH7v6zTp/DzI4GnkYIYIv0nUoFdu4Ml9Xrl14KL3pRyDDeuzdclsuLt6nV6gJzql+df7VZ5iu9/lJcZvZ44L+ApxP61Q9El38M7DKzx2XWOJGYpndNM7p1lM0XbWZ06yjT1xbrbFjeF7BrdTHEXnlukXakXiLDzJ4MvJeweMLH3P3vmtxunNDB/5m7fybFJoqIiOSGu98MlLv4FFsJCwnes4vPIZIbtWUPduxYWgu5XA4ZyEccAXfeufR+y9VWbmWBOdWvzr92a2tL4fwjcA3wJHffV91oZvcELo32b4rzQBrjShbyWKO3MleJXUKjKAvYZVkapB/LkkhxdRxgNrMFwNx9xceKajy+n5CFdTOw08y2u/v3G9zuXcAlnbZPRPqcCihKgZnZK4Bpd+9KWoeZPR34pbt/N8rkana7lwAvATj22GO70RSRxDX6+K9dXG//frjrrhBYrgYSzz23+eMlmWXcD/Wri0xZ5n3joYRA777aje5+h5m9C4iVCqoxrmQlbzV6WwkY5zE4vpwsa3SrPrgURRIlMi4ELop520cCN7j7T9z9APBJYEuD270c+CzwywTaJyL9SgUUpfj+EdhtZl8ws2dHi/sl6THAn5jZDKFPfoKZfaL+Ru7+EXff5O6bSoqESQE0+vivL3tw552HBxEbGRmJV1u5Vb1cv7ro2qmtLYV0MzDUZN8QsDvm42iMK5nIU43e2oDx3v17mT84T3lbuWnpi2pwvFY1OC4ixdRxgNndy+5+Rsybrwd+WnP95mjb3cxsPfBM4EOdtk1E+pgKKEpvOAZ4HaGv/BRwi5l91Mwem8SDu/vr3P1odx8Dngt81d1fkMRjS++qr1mcN80+/q+55vDF9VYyPAyf+1y82srSW1qtrS2F9C7gLdH4827R9TcD74j5OBrjSibyVKO31YBxnoLjIpKMWAFmMzvJzI5ssm/EzE6K+XzWYJvXXd8KvMbdD63QppeY2VVmdlUlryMcEclOtYBirWoBRZGCcPdfuPs/uvtG4OHAR4AnAZeb2YyZvS3bFkq/KcLEkGYf/3B4xvLg4NIs1bPOOjxr9ZRTlLnar5Rl3nvM7MLqD2HR3HsCPzazy8zsU2Z2GfBj4Egg7iJ/GuNKZiY3TDJ79iw7Tt3B7NmzmdUwjhswri7qBzC1ZYo1A2sYGRxhzcAaLWAnUnBxM5i/RqhR1chDov1x3EzIxqo6GvhZ3W02AZ+Mpuv+KfABM3tG/QNpuq6ILEsFFKXHuPsud/9rYJSwwv0A8PoEH/8yd396Uo8nvacoE0Oaffxv3Hh42YMLLliapfq+97WftZr3zG4RAUJQ+bE1P4eAnxP61kdGlz8H7or2x6ExrmSqNFJifP14psHZONnU07umGd06yuaLNjO6dZQrb7oSs3B+pnopIsUVd5G/5f7bVxM65jh2Ag8ys+MINa2eCzyv9gbuftzdT2p2PvAFd/98zMcXEQmqBRTL5ZCitrCgAopSeGb2OOAFhMHpbwFXZdsi6SfVzOD5xbWE7p4YkqeP1uU+/pstrlfb/lKp9b+ndvHAAwfC86mkgkj+RCWhkqYxrgghm3riuAlm9swwtm5sSXC50aJ+5+5curJunhf5K5LKXKXhayDSbU0DzGY2Bhxfs2lTgzIZw8CLgJviPJm7HzSzswgr5w4A57n7dWZ2ZrRfNalEJDnNIgkiBWJmDwFOBZ4PHEvocz8AXOjuP8iybdJfijQxZLmP/3YCyMupzeyuBt/L5fD86nZEep/GuJK0IgcISyOlhm2u1miuBpcbqdZsLtrfnCfTu6Ypby8zNDDEgUMHmNoylVnZFOk/y2Uwn0ZY3MCjn/exNJPZo+sHgZfFfUJ3vxi4uG5bw07X3U+P+7giIg0lHUkQSZGZXQVsBO4grDx/obtfnm2rpF8VbWJIWh//RcnsFpHmzOwI4Abgj939ulbvrzGuJKVXA4SNajTX0yJ/nWmUJa6scEnTcjWYzwdOBp5ICCSfFV2v/jwB+EPg/u7+0e42U0REpC/dQphme393Lyu4LFmbnGy/RnEedKNOcpEyu0WkKQPGCOUfRTJRGyDcu38v8wfnKW8rU5krfnH/RjWaz3rkWcvWbJbWVLPEa1WzwkXS0DSD2d1ngVkAMzsZ+K6770urYdJDKpXilCgoUltFpOe5+9OyboNIvaJODOlWneSiZXaLiEjnulHGolEZiV4qG9GoRvObTnpTYcuB5E2jLHFlhUua4i7ydx1wb+DuALOZ/QWwAbjE3b/QhbZJLyjSqjdFaquI9BUzuz+h/vKa+n3u/vX0WyRSLN2uk6yS/yIi/aNbZSz6IUBYX6O5Wc1maV01S7y8rczgwCALhxaUFS6pihtgPg+4GXgpgJm9EXgLcDvwUjN7nrt/qjtNlMIq0qo3RWqriPQNM1sPfAI4qdFuwnoIA6k2SqSA0qiTXNTMbhEBdz9kZmcAN2bdFsm3bta5VYBQOtUoS1wkLXEDzJuAC2qunwm8w93/xsz+GfgrQAFmWapIq97MzBy+zT2fbRWRfvJBwmyhvwZ2AfuzbY5IMalOsoisxN0vWPlW0u+6XcZCAULplLLCJStxA8z3Jiw0hJltAO7PYsD588ALE2+ZFF+RRnNHHrk0EA5w551he16oPrRIP3os8Ap3vyjrhogUmeokiwiAmbU0bnX3C7vVFimmNMpYKEBYPN2oyS1SNHEDzLcCR0e/PwH4mbv/KLo+CByRdMOkBxRpNLdvHwwPLw0yDw+H7Xmg+tAi/Woe+GXWjRDpBaqTLCLA+S3c1gEFmGUJlbGQet2qyS1SNHEDzDuAc8zsPsCrCVnLVQ8BZhNul/SKoozmmmVV5yHbWvWhRfrZR4FTgUuybohIL1CdZJG+d1zWDZDiUxmLYupGlnE3a3KLFE3cAPNfExYZeiewk7DAX9XzgSsSbpf0kiKM5vKcbV2kWtYikrTdwKlm9lXgYuC2+hu4+3mpt0pERKSA3F2JUZIIlbEolm5lGXe7JrdIkcQKMLv7LcDmJrsngDsTa5FIVvKabV2kWtYikrQPRZdjwOMb7HdAAWYREZE2mdnDgJOAo4APu/svzOyBwC3ufke2rRPpbWnULu5mlnEaNblFiiKJ2sn7gBythCbSgVIJxsfzE1yGxezq4WFYuzZc5iW7WkS67bgVfo7PrmkiIiLFZWarzezTwDXAPwNvAn472v33wBuyaptIP5jeNc3o1lE2X7SZ0a2jTF873ZXnqWYZ16pmGXeqWpN7eNUwa1evZXjVsGpyS99qmsFsZrcBE+5+dXTdgG3A2e7+k5qbjgNXAgPdbKhIX0s7u7pSyV8mt0gf0lRe6SXqWkQkZ/6WMBv3VODLwC01+/4DeCnw2gzaJdLz0qxd3O0s4yRqcqeRyS3SbctlMK9jaQD6CODp0XYRqVepwM6d4bIb0squnp6G0VHYvDlcTnfnTLKIiPSPbnct3e6CRaQnTQJ/4+7/yuFrHNxIKE8lIl3QzaziemlkGZdGSoyvH2/rMdPK5BbptiRKZIhIrwRlK5Ww0OH8POzdGy7LZY3Ye50iM7liZj8xs4dHv98YXW/28+Os2yuykm53Lb3QBetjWCQTRwHXN9l3BLA6xbaI9JW0axdPbphk9uxZdpy6g9mzZxNZ4C8JtZnce/fvZf7gPOVtZSpz+kIgxaMAs0ineikoOzMDQ0vPJDM4GLZLb+qFyEzvuRz4dc3vy/18PYsGirSim11LL3TB+hgWycyNwKOb7Hsk8IMU2yLSV7KoXdxJlnG3pJnJLdJtTWswi0hM1ZHz/PziturIuWhFJsfG4MDSM8ksLITt0ntqIzPV92+5HOp9F+2920Pc/Yya30/PsCkiieika1mpbnPRu2B9DItk6kLg9WY2A3wu2uZmdjLwKuCcjNol0heSqF1cdGlncot000oZzOvN7HgzO57Flerv3hZtP7q7TRTJuV4KypZKMDUFw8Owdm24nJrSKLddeZ/zrIx1EUlBu11LnMzeonfB+hgWydTfA18ELmKxBvMVwA7gS+7+vqwaJtIv8phVnKYsMrlFumWlDObPNNj2+brrBngirREpourIuVwOo8KFhWIHZScnQ+rUciljsrLp6fCeGBoK0Y+pqXBs86TokRmRAlspM7fXtNq1xM3sLXoXrI9hkey4+yHguWb2fuBJwH2BWwnB5cszbZyI9A1lckuvWC7AfMYy+0SkVq8FZUul4v8NWSrKnOeiR2ZECqoI55+6oZWuZaXSF7UB+iJ3wfoYFsmeu38D+EbW7RCR/lUaKSmw3EBlrqLAe4E0DTC7+wVpNkSk8BSUlaoiFQUtcmRGpICKcv4pa8tl9jYL0Bf1+OljWERERGSp6V3TlLeXGRoY4sChA0xtmWJyQx9kZBTYSjWYRUSkVUWb81wqwfi4ohoiKVDN3Xia1W2GxQD93r3hslxuvdR93krkl0qhi5iZyU+bRHqRmd1lZofi/mTdXhGRflSZq1DeXmb+4Dx79+9l/uA85W1lKnP6kpRnCjBLseRtRCjSiBZLFJEminb+KUuTkzA7Czt2hMvJyWQC9HEWD0xbHtsk0qPeWvPzNmA3YYG/CwiL/l0YXd8d3UZEcqoyV2Hn7p0KOvagmT0zDA0s/cI3ODDIzJ6ZbBoksay0yJ9IfqRRtLLfVl2S7tGcZxFpoNOau/3WTdVXn+o0QJ/HEiV5bJNIr3L3c6q/m9nfALPAk9z9NzXbR4BLgIOpN1BEYlH5hN42tm6MA4eWfuFbOLTA2LqxbBoksSiDWYqhdvRVOyf2+uuTy2hW+pAkTaUnpIvMbMHMNPgtoEaZuXGom+p8gkgeS5TksU0ifeIvgHfXBpcB3H0O+AfgzExaJSLLUvmE3lcaKTG1ZYrhVcOsXb2W4VXDTG2Z0kJ/OacMZimGRoumAWzcCGvWdJ7RrPQhESmeC9GJ4sJqdV1YdVOLOpkgkscSJXlsk0ifuA8w1GTfEHBUim0RkZiq5RPmDy7GBqrlExSA7B2TGyaZOG6CmT0zjK0b02tbABqYSvK6USe50ehrfh727+9slZ8qpQ+JSMG4e9ndz8i6HZIOdVNLtTtBJI8l8vPYJpE+cRXwFjNbX7sxun4OsDOLRokkrddqFat8Qv8ojZQYXz+u4HJBKMAsyerW/N360dfq1eH3Wp2MtJU+JCI5ZGYnmdmRTfaNmNlJabdJsqFuKjntlijptzaJ9IFXAL8N/NjMLjOzT5nZZcCPgfsDr8yycSJJmN41zejWUTZftJnRraNMX1v8+loqnyCST+buWbehY5s2bfKrrroq62ZIpRKCyrVlLIaHw0gpqTSc6upGRx4JD394GF1XDQ7C7t3tP1d1EcHaVZc0wmtPv61CJdKEmX3X3Td1cP9DwKPd/TsN9j0C+I67D3TSxlapz81O3ropfdSLSF6029+a2VHAXwGPAh4A/Bz4T+A97n5rsq1sjfpb6VRlrsLo1tElpSSGVw0ze/Zs02BsZa5SmJIERWqr9Jdef28263NVg1mS06hOcjWrOKmRZ7VoZaUCZkv31V9vVSdFHWVRNQIyNNR5bWwRWe6DbTVwKK2GSPby1E2t9FGv4LOIFEEURH5D1u0Q6YZWaxVP75qmvL3M0MAQBw4dYGrLFJMb8juOK42UejJ4J8VWtP+jJKlEhiQn7vzdJGo0z8wcXiJjzZr+LUaZF7WrUCVRG1ukD5nZmJk9wcyeEG3aVL1e8/M04NXATRk2VTLQbu3hJK30Ud+talkiIt1gZvc2s6eZ2alm9hQzu3fWbRJJQiu1iitzFcrby8wfnGfv/r3MH5ynvK3cM3WbRdLQ7/9HCjBLcuKsUpPUqLMbxSg1Iu6cVqESScJpwA7gy4AD74uu76jZ/u/AHwPvyqiNPakba9T2opkZWFU3B676Ua/zjCJSJGb2dmA3oV+9APgisNvM3pZpw0QS0Eqt4mq2c61qtrOIxNPv/0cqkSHJWm7+bu2os1pGo1wOt293Gfj6YpTtpnS10zbN/z2cVqESScL5wGWE8hhfBV4GfL/uNvuBH7r7bam2rIepuk98V18Nd9yxdFv1oz6NalkiIkkws7OB1wNTwCeAXxAW93sB8Hozq7j7P2fXQpHOTW6YZOK4iRXrwbaS7SwijfX7/5ECzJK8ap3kekmPOpMsRtlq2xSJaCzpwL9IH3L3WWAWwMxOBr7r7vuybVVvS/L8Z6+rVOBVrzp8+3ves3isdJ5RRAriTOC97l77qfYD4HIz2we8FFCAWQovTq3iarZzeVuZwYFBFg4tNM12FpHG+v3/SAFmSU83slvrg9ntZhW3Uj/6mmsUiVhOnlahEim+64B7A3cHmM3sL4ANwCXu/oWsGtZLipR1m/XkmUbH6p73hBNPDL/rPKOIFMgYoSRGI18E/jK9pohkL262s4g018//R6rBLMlrVsQyTo3mTnRSQ7mV+tHPetbSkTWoznC9PKxCJdIbzgNeW71iZm8EPgg8D9hmZn+WVcN6SVGq++RhqYBGx+rgwaXHanISZmdhx45wqQk+IpJTtxJO2Dbye9F+kb5SGikxvn68r4JiIknr1/8jBZglWSuNfrs16kxiVaHl2lb7+HNzh983j5EIEekFm4Cv1Fw/E3iHux8FvB/4q0xa1WO6ff4zCXlZPC/usdJ5RhEpgH8D3mZmp5rZIICZrTKzSeCtwGczbZ1ITJW5Cjt376QypxV1RSQ7KpEhyYlbxLJZjeZOJDW/uZX60QAjI3DXXfmLRIhIr7g3cAuAmW0gLD50QbTv88ALs2lW78l7dZ88lfHI+7ESEYnpdcDDCf3qeWZ2G6HfHQCuICwAKJJr07umKW8vMzQwxIFDB5jaMsXkBk0dEpH0KcAsycly9Nvu/Oa4xSwbPf6aNfC5z8HGjRpdi0i33AocHf3+BOBn7v6j6PogmomUqG6c/0xK3sp45PlYiYjE4e53mNlJwNOAxxKCy7cBlwP/4e6eZftEVlKZq1DeXmb+4DzzB8MYvLytzMRxE303NV9EsqeBqSQny9FvO/ObWylm2ejxzzsPTjlFI2wR6aYdwDlmdhbwakLWctVDgNksGiXpK0IZDxGRovHgC+7+Gnf/8+jyYgWXpQhm9swwNDC0ZNvgwCAze2ayaZCI9DVlMEtysl46vpU5u3HLebT7+CIiyfhr4BPAO4GdwFtq9j2fMIVXcizuRJk41A2JiCTPzAx4ALCmfp+7/yT9FonEM7ZujAOHliZ4LRxaYGzdWDYNEpG+pgCzJCvr0W/cObvtlvPQnGARSZG73wJsbrJ7ArgzxeZIi6anw7nLoaEwwWdqqvO1bdvthpIKdCcZMBcRyZKZVRfMfSbNx8UD6bVIpDWlkRJTW6YobyszODDIwqEFprZMqTyGiGRCAWZJXhGCsHkrZiki0rp9wDpCvUjJmXYmynRLUoHubgTMRUQyNAWcDJwL/A9wYPmbi+TP5IZJJo6bYGbPDGPrxhRcFpHMKMAsvSVualXW5TwaUVqYiADRKvYT7n51dN2AbcDZdVN1x4ErUXZVLmW57m2tpALdaQXM1RWKSIpOBl7p7udn3RCRTpRGSgosi0jmtMif9I5WFu2DkHY1Ows7doTLLNOwWm27iPSydSw9AXwE8PRouxREXibKVAPdtaqB7iweZznqCkUkZbcBt2TdCBERkV6gALP0htrUqr17w2W5HLYvp1SC8fHsM5fbabuIiORWdaLM8DCsXRsus5gok1Sgu9sBc3WFIpKB9wFnRjOFRHpGZa7Czt07qcypExWR9KhEhvSGvMxFbkeR2y4iIk0tt+5tWqUgkqoI1e3KUuoKRSRt7v5PZvbbwPfNbAdw++E38Tdn0DSRtk3vmqa8vczQwBAHDh1gassUkxvSm6lbmauoHrRIn1KAWXpDt1Kr0ogA5GUetYiIJK7RurfNFsvrpMtZ7r7LBbpbkdTjNKKuUETSZmZPBV4GrAYe3OAmDijALIVRmatQ3l5m/uA88wfDGdvytjITx02kEuzNOrgtItlSiQzJv0oFdu5cfp5sN+Yip1UMMi/zqEUkT9ab2fFmdjxwfP22aPvRGbZP2tSsFMSHP9x+lxOnu0qqIlS3KkupKxSRDPwTsBN4OLDa3Y+o+9EiulIoM3tmGBpYumDC4MAgM3tmuv7ctcHtvfv3Mn9wnvK2ssp0iPQRZTBLvjVL82okydSq2ghAdb5uuRwevxuj3W6mhYlIEX2mwbbP1103QnaVFEijUhADA/DKV8L+/a13OddfD2ec0d5980ZdoYik7FjgFe6+K+uGiCRhbN0YBw4tnQ60cGiBsXVjXX/uanC7mjkNi8FtlcqQuFRipdgUYJb0xZ0D3E6Qt9Fc5HZkUQwyqbaLSNGdkXUDZKkkqyU1KwUxNBSCxFVxupzp6cXgcq0i1y5WVygiKboG+O2sGyGSlNJIiaktU5S3lRkcGGTh0AJTW6ZSCdRlGdyW3qASK8WnALOkK05GcnUkf/vt2a34o2KQIpIRd78g6zbIolYm0sTRaLG897wHXvWqpbdbqcupnoOtDy7Hua+IiADwCuACM/uRu38z68aIJGFywyQTx02kngWaZXBbii/r+uGSDAWYJT1xMpLrR/IHDy59jLRGzY0iACoGKSLSV7pVLalRKYi1a1vrchpNtAFYvVrdlYhITJ8H1gJfN7M5YE/dfnf30bQbJcno9lT7PE/lL42UMmlTVsFtKT6VWOkNCjBLelYqO9FoJD80BGvWhMu0g7ydFoOsn1Od5BxrERHpum5WS6ovBdFql9Noos3q1XDNNXDCCZ21rRvUBYpIDn0FrWXQk7o91V5T+ZvLKrgtxaYSK71BAWZJz0plJxqN5NesgU9/Gu51r2xGpa0Ug6wdPe/YsTQTu1wOwfGk5liLiEjXpV0tqZUup9lEmzwGl5MsM6JAtYgkxd1Pz7oNkrxuT7XXVH6R5KnESm9QgFnSs1LZiWYj+Y0b8z+KrB09798Pd90V/pZqsPzcc8NlknOsRUSkq/JeLanTiTZxVSohMxpa75KTLDOSdD1sERHpPd2eaq+p/CLdoRIrxacAs6RrudFw3kfyzTQaPa8krcUKRUT6QDezWtMK4rarlazndkxPw2mnhS4ZQnD3/PPjB3aTKjPSrXrYIiLSW7o91V5T+UW6RyVWiu2IrBsgfahUgvHxxiPCyUmYnQ0lJmZni5GaVB09tyKtxQpFRHrc9DSMjsLmzeFyejr551iu2+pllQq86EWLwWVYrPpUqcR7jKTKjDTqaquBahERkarqVPvhVcOsXb2W4VXDiU617/bji4gUlTKYJX+6nY6VtEaj58FBWLVqMRO7WoO5SJnZIiI5p6zW7pqZgYGBw7cfcUT8DOTayUkDA6ELfM97Wn990q6HLSIixdXtqfaayi8icjhlMIt0qjp6Hh6GtWvD5QUXLM3Eft/7ipeZLSKSc1lmtVYqsHNn/EzetB8vCWNjcOjQ4dvvuqu1wO7kZAgqLyyE1+tVr2o907xRV6tztSIi0kxppMT4+vGuBX+7/fgiIkWjALNIEhqV9qifU92vc6xFRLokq6zWpMtypFHmox2lEpx3XgjaVw0NtR7YrVRCUHn/frjjjpBt3kqZjaoiVtESERHpJZW5Cjt376Qyl6Mz4iKSCwowiyRFAWQRkVRlkdVaW5Zj7972g6XderykTU7C7t1wySXh5+abWw/sJplprq5WREQkG9O7phndOsrmizYzunWU6WtzckZcRHJBNZhFRESksCYnQ83lmZmQudztwGM1WFqt+QyLwdJ2njvpx+uGUglOOaX9+6t+sogUjZktAObuGi+LEDKXy9vLzB+cZ/5g+NJS3lZm4rgJlQkREUABZimCSmUxcgDpRRFERKQQ0lwbNulgaT8EX2sX+tNatyJSEBei2b4id5vZM8PQwNDdwWWAwYFBZvbMKMAsIoA6Tcm72sKU69fD0Ufnr0iliIj0jaTLcvTL4nWqnywiReLuZXc/I+t2SH8oQl3jsXVjHDi09Iz4wqEFxtaNZdMgEckdBZglv+oLUy4shDSvPBapFBGRvpF0sLRfgq+qnywieWJmJ5nZkU32jZjZSWm3SfpPUeoal0ZKTG2ZYnjVMGtXr2V41TBTW6aUvSwid1OJDMmvRoUpa+WtSKWIiPSNpMtypFnmQ0REAPga8GjgOw32PSTaP5Bqi6SvFK2u8eSGSSaOm2Bmzwxj68Zy2UYRyY4CzJJfjQpT1uq1IpUiIiIiIpIWW2bfauBQWg2R/lTEusalkVJu2yYi2VKAWfKrflWg+XkwgzVrtEKQiIiIiIi0xMzGgONrNm1qUCZjGHgRcFNa7ZL+pLrGItJLFGCWfJuchImJUApjbAx+9Sv4znfgkY+EE07IunUiIiK5UKksdpU69yoi0tRpwJsBj37ex9JMZo+uHwRelnrrpK9U6xqXt5UZHBhk4dCC6hqLSGEpwCz5Vy1MOT0dspmHhkLpjKmp3l0JSUREJCZ1jyIisZ0PXEYIIn+VEET+ft1t9gM/dPfbUm2Z9CXVNRaRXqEAsxRDpRJGz/Pzi4v+lcshu1mpWiIi0qfUPYqIxOfus8AsgJmdDHzX3fdl2ypJQ2Wuktsgruoai0gvOCLrBojEMjMTUrNqDQ6G7SIiIn1K3aOISNuuA+5du8HM/sLM3mdmT8+oTdIF07umGd06yuaLNjO6dZTpa6ezbpKISM9RgLlfVCqwc2e4LKKxsTDvt9bCQtguIiLSp9Q9ioi07TzgtdUrZvZG4IPA84BtZvZnWTWsSCpzFXbu3kllLp/jzMpchfL2MvMH59m7fy/zB+cpbyvntr0iIkWlAHM/mJ6G0VHYvDlcTid8xjaN4HWpFIpKDg/D2rXhcmpK839FRDpgZseY2dfM7Hozu87MXpl1m6Q16h5FRNq2CfhKzfUzgXe4+1HA+4G/yqRVBVKEzOCZPTMMDSyd6jM4MMjMnplsGiRL5P0EhYjEpwBzr6stzrh3b7gsl5MLBnc7eF1rchJmZ2HHjnCpFYxERDp1EHi1u58APAp4mZk9NOM2rajok3KSpu5RRKQt9wZuATCzDcD9gQuifZ8HHpxNs4qhKJnBY+vGOHBo6VSfhUMLjK0by6ZBcrcinKAQkfgUYO51cYsztjNabxa8vv76ZEf+tW0rlWB8XKlZIiIJcPefu/vV0e93ANcD67Nt1fLSPK9ZNLffDtdco8C7iEhMtwJHR78/AfiZu/8ouj6IxsrLKkpmcGmkxNSWKYZXDbN29VqGVw0ztWVKi+plrCgnKEQkvtQ7TTN7spn9wMxuMLPXNtj/fDP77+jnSjN7eNpt7ClxijO2O1pvFLx2h40bkxv5K5Ig0vuUjpoLZjYGbAS+nXFTmur2pJxO25bV23h6Go4+Gp70pPCzfr26SxGRGHYA55jZWcCrCVnLVQ8BZuM+UC+NceOWLChSZvDkhklmz55lx6k7mD17lskNmuqTtaKcoBCR+FINMJvZAKGe1VOAhwKTDabi3gg8zt0fBrwN+Eiabew51eKMa9bAyEi4rC3O2EkWcqPg9Z13wv79yYz88xxJEJFk6CRSLpjZkcBngbPd/dcN9r/EzK4ys6sqGX4Gx52Uk7ZGb+O0As7VrrK2O15YgBe9SN2liMgK/hr4KfBO4MfAW2r2PR+4Is6D9NIYt5WSBUXLDC6NlBhfP57b9vWbIp2gEJF40s5gfiRwg7v/xN0PAJ8EttTewN2vdPfbo6vfYnHaknTCbOllVaPROsTLQq5fWWj16vB7rU5G/nmNJPQjZZhKN+gkUi6Y2SAhuPwv7v65Rrdx94+4+yZ331TKsERRnEk5aWv0Nj7ttPTOm8zMwBENvs0NDKi7FBFZjrvf4u6b3f2e7v4Ed/9Vze4JIO7Ctz0xxm2nZIEyg6VdRTtBISIrSzvAvJ5wlrjqZpav9VgG/qOrLep1tSPfubnDAziNRuvz8/GzkGtXFrrmmsP3dzLyz2MkoR8pw1S6RSeRMmdmBkwB17v7P2XdnpXUn9ccHl46KScLjd7GCwvpnTcZG4O77jp8+6FD6i5FRDqwDzgy5m17YozbbskCZQZLu3SCQqS3pB1gtgbbvOENzU4mdL6vabI/F9N1c2+lAE4SWcjVhfdOOCHZkX8eIwn9Rhmm0k06iZQHjwFOBZ5gZt+Lfp6adaOWU3tec3Y2XM9So7dxvW6eN6l2lbVd/eAgnHeeuksRkXpmdpuZnVhz3cxsu5kdX3fTcSDuF96eGOOqZIFkQScoRHpH2gHmm4Fjaq4fDfys/kZm9jDgY8AWd7+10QPlZbpu7sUJ4CSZhZz0yD9vkYR+owxT6SadRMqcu1/h7ubuD3P3349+Ls66XSupntfMw1ul/m28Zk3jjOZunjeZnISbb4ZLLgk/u3eruxQRaWIdsKrm+hHA06Pt7eqJMW7ckgVxFwEUEZH+smrlmyRqJ/AgMzsO2A08F3he7Q3M7Fjgc8Cp7v7DlNuXX5VKCOqNjbU2oq6OfMvlEBhcWGgcwCmVFrfFuf1Kz5nkF6KkH0/iU4apdNvkJExMtPf5JpIT9W/jHTs660bbUSrBKad09zlERKShnhnjTm6YZOK4CWb2zDC2buyw4PL0rmnK28sMDQxx4NABprZMqayBiIgAKQeY3f2gmZ0FXAIMAOe5+3Vmdma0/0PAm4CjgA+E0pAcdPdNabYzd6anw0h1aCgE+6amWktNajWAo4CPVMU9QSHSCZ1Ekh5Q+zZupRtt9/yxiIjkQ6+NcUsjpYblCmoXAZw/OA9AeVuZieMmVN5ARERSz2Ammnp7cd22D9X8/mLgxWm3K7dqa+DOh46ccjmMXLuZVayAj1TphIOISMvidKOdnj8WEZF86IcxbnURwGpwGRYXAYwbYK7MVZpmR4uISLGlHmCWFlVr4M4vduR318DtJNCnlClphU44iEiPyEv3l9T5YxERadn6mkX9Bmq27am5zdHpNin/Ol0EMInyGgpQi4jkV9qL/EmrulEDd3oaRkdh8+ZwOT19+G0qFdi5M1yKiIj0gDjdX1q0hqqISGY+A/wo+vmfaNvna7b9CPh0Ji3LsbiLADZSW15j7/69zB+cp7yt3NJCgdO7phndOsrmizYzunWU6Wsz7MRFROQwymDOu6Rr4MZJmdKcXRER6TF5yxjWGqoiIpk4I+sGFNlKiwA202l5DdV/FhHJPwWY86p2Dm+SNXBXKrmRxAg8L/OPRUREIt2qONUuraEqIpI+d78g6zYUXbNFAJfTaXmNJOo/i4hId6lERh41msNbKsH4eOcjz5VSphrN2R0YgIsvXiyXsVz5jDzNPxYREYnkMWN4chJmZ2HHjnCZxWQhVcQSEZFua7e8RmWuws7dOzly6MiOAtQiItJ9CjDnTW0G8d694bJcTm7kV02ZGh6GtWvDZW3KVKMR+L598PKXh4Bx9bJRALnbbRcREWnTSt1flu1K4vxxO3ROWERE0jK5YZLZs2fZceoOZs+eXXGBv9qay4/4yCMon1huq/6ziIikw9w96zZ0bNOmTX7VVVdl3Yxk7NwZRnp79y5uW7s2pDeNjyf3PMuVsajWYB4YCMHl5QwPh7SrUim9touIFISZfdfdN2XdjiQVvc9VFaegUglB5dqSIbVduohIkai/7S2VuQqjW0eXlMQYXjXMd1/yXfYd2NdS/WcREUlWsz5XGcx5k9Yc3uVSpqpzds89F+55z+Ufp3bJ+zzOPxYREamRZcbwctIuVdGoIlZtly4iIv2pWpaiMpfdLNRqzeVagwOD7Duwj/H14woui4jkkALMeZOXObylEjz1qXDw4PK3qw0g56XtIiIiGWslYJxFqQqdExYRkXq1ZSlGt44yfW02tZM6XRRQRETSpwBzHuVh1R9oHDA+66zlA8h5abuIiEhGWgkYN1u+4NJLu5vNrHPCIiJSqzJXoby9zPzBefbu38v8wXnK28qZZDK3uyhgkvKQyS0iUiSrsm6ANFEqdTbKS6rI5OQkTEwsfaw3vWn5x+607SIiIgVVGzCu1jcul0NX2qhrrJaqqK2FPD8Pz3oW3HVXCPp261xtoy5eRET6U7UsRW3d48GBQWb2zGRSkmJywyQTx00ws2cm9ZrL07umKW8vMzQwxIFDB5jaMrXiooQiIv1OGcy9KOm5tvUFK/NawFJERCRjrdY2blSqAmBubjGbuduZzOrSRUQkj2UpSiOl1Gsu5ymTW0SkSBRg7jXN5tpef326qweJiIj0oVZrG9eWqhgZOXy/Ft4TEZE05KEsRR40W2BwZs9MNg0SESkIlcjoNY3m2gJs3Ahr1oRRbzfn24qIiPSxasC4XA7B4YWFlWsbV0tVXHMNbNkCd965uE8L74mISFqyLEuRF3nM5BYRKQJlMPeaRqlT8/Owf//SjOZOMpkrFWVDi4iINNHOerelEpxyCpx3nhbeExGR7GRRliJPlMktItIeZTD3mvrUqf374YgjlmY0V+fbtjNinZ4Ojz00pGxoERGRJtpd71YL74mIiGRLmdwiIq1TgDlPKpVkRpS1o9Mjj4RHPGLp/nbn29bWd64GrMvl8FxpjYCTOkYiIiI51W5wWkRERJJRGikpsCwi0gKVyMiL6Wk49lg4+eRwOT3d2eNVl4U/4YTF1YM6nW9bre9cK83Vh6anYXQUNm8Ol50eIxEREREREREREemIAsx5UKnAaaeFVX3m5sLlaaclV+O4nWKQjTSq75zW6kO12dNJ1ZIWEZGepKUCRERERERE0qMAcx5cc00I1NZaWAjbk1LNaO5kzm21vnMWqw9lnT0tIiKFkPRkFwWrRURERERElqcAs7QmqWzoVmWZPS35omiPiDSR9GSXarD65JPD5Yc/rI8fERHpb5W5Cjt376Qyp85QREQWKcCcBxs3Hp6dOzQUtudREtnQ7TxnVtnTkh+qwy0iy0hyskulAqefHoLUc3Ph8swz4YlP1MePiIj0p+ld04xuHWXzRZsZ3TrK9LXqDEVEJFCAOQ9KJTj/fFizJgRO16wJ1xU8XSqr7GnJB9XhFpEVJDnZ5ZprDn8sgDvu0MePiIj0n8pchfL2MvMH59m7fy/zB+cpbysrk1lERAAFmPPnCL0ky5ZAyCJ7WvJBdbhFpIlqtwGdTXZppQKPPn5ERKSfzOyZYWhg6XfxwYFBZvbMZNMgERHJFUUz86CamXnnnWEe7p139m9qlEogSDOqwy0iDdR3G9DeZJf6x7nxxhBEbkYfPyIi0k/G1o1x4NDS7+ILhxYYWzeWTYNQPWgRkTxRgDkPlJkZqASCLEd1uEWkTrNuA1qb7NLocV71Knjf+0LVqpERWLUqdNX6+BERkX5UGikxtWWK4VXDrF29luFVw0xtmaI0kk1nqHrQIiL5sirrBgjKzKyqBtrn5xe3VQPtGsULhFTEiYnwnhgb0/tCpM8l1W00e5wTT4Sbblr8yIFQmxnyuw6viIhIt0xumGTiuAlm9swwtm4ss+BybT3o+YOh8y5vKzNx3ERmbRIR6XfKYM6DdjIzWykUWRQKtEscqsMtIpGkuo3lHqf2I2fHDnjGM+A5z1EVJxER6U+lkRLj68djBXK7VcJC9aBFRPJHAea8mJyMXzSyV+sUqwSCiIi0IKluI87jqIqTiIhIfN0sYZHHetD1VB9aRPqNAsx5Eiczs9dHuK0E2kVEpC8sN2knqW5jpcfRcgkiIiLx1Jaw2Lt/L/MH5ylvK68YbI0blM1bPeh6qg8tIv1INZiLJqmCk5VKfuvYlkr5a5OIiGRiejqcRx0aCmUspqYOD/622m006wKXexxVcRIREYmnWsKiWh8ZFktYNAsCT++apry9zNDAEAcOHWBqyxSTG5qfNc5LPeh6qg8tIt1Wmavk7rMPlMFcPEmMcHu1xIaIiPSUbkzaabcLVBUnERGReFotYdFuxnMr9aDTovrQItJNeZ4hoQBz0XQ6wu31EhsiItIzki5L0WkXqCpOIiIizVVLXAAtlbDopaBsEepDi0gxtXsyLi0qkVFEk5MwMdFeiYukSmyIiIh0WdJlKZLoAlXFSURE5HCNSlzMnj277DTu6jTvI4eObBiUPXLoSHbu3pm7aeDLqdaHLm8rMzgwyMKhhVzVhxaR4mqn/FCaFGAuqnZHuCoiKSIiBVGdtFMuh0DwwkJnZSnUBYqIiCSvWd3h2bNnGV8/3vA+9QHp8ollpq6eujsoW95Y5hEfecSSgHUeay43ktf60FV5rd8qIsvL+wwJBZj7TbPROsDOnflc9E9EiiXPi4hK4XQyaadeqwFrvZVFRERW1mpWXaOA9NTVU3z3Jd9l34F9HDl0JI/4yCOW7D/tc6examBV7EUAs1YaKeUyeNvqYooikh95nyGhGsz9qL6IJGjRPxFJhhYRlS4olWB8PJkgb9w6ynori4iIxNNqVl2zmsv7DuxjfP04+w7sO2z/gi/ktu5oUeS9fquIrGxywySzZ8+y49QdzJ49m6sTRAow96vqaB2SX/SvUgnZ0Fo4UKS/aBFRKYiVAtZ6K4ssT1/1RKRWNasu7qJ+KwWkG+2vV9RFALPUS4spivSz0kiJ8fXjuclcrlKAud9VVzyqVV3xqB1K+RLpX0l/nohkRG9lkeb0VU9EGmklq640UqJ8YnnJtvKJ5buDJfUB6zUDaw7PaM5R3dGiyHv9VhEpNgWYs1Sf/pFFOsjYGPzmN0u3zc+3t+KRUr5E+ptWUJOcarV71VtZpDF91ROR5cTNqqvMVZi6emrJtqmrp5aUaqgNWN/0qps4/xnnx86QlsZazTQXEWmFFvnLyvR0+EY+NBRGseVyWGmoen1qqnlhyKSZLX89rmrK1/zi4g53p3xpdSSR3tfqCmrS19JaQK++u43TveqtLNKYvuqJSBLiLgpYu1De5IZJJo6bYGbPDGPrxhQUbZOOo4h0iwLMWahN/6h+Qz/33HBZvV4uw8RE97+tz8zA8PDSVK01a9obKSjlS0QmJ8NnVxqRQymsdoK+7WjU3cbtXvVWFjmcvuqJSBLaLdVQG3CW9uk4ikg3qERGFhoVd6yXVrHHJEcK1ZSv4WFYuzZcKuVLpP+stIKa9LU0p9h3WktZb2WRpfRVT0SSoFINIiK9RxnMWWgU1K2XVjpI0vOAlfIlUgxp1ScQqZPmFHtlW4okT1/1RCQJKtUgItJblMGchUbpH2edlV06yOQkzM7Cjh3hstN5ykr5Esm36WkYHYXNm8Pl9HTWLZI+kmbQV9mWIt2hr3oikoS4iwKKiEj+KYM5K43SP176UvjOd+CRj4QTTki3PaVS/FGCMh9FiquTorQiCUh7AT1lW4qIiLSvMldRlrGIiKxIAeYs1QZ101rxqFNFaaeINJZmfQKRJtIO+rZyDlVERESC6V3TlLeXGRoY4sChA0xtmWJyQzHGfgqMi4ikSyUy8iDNFY86UZR2ikhzKkorOaEp9iIiIvlVmatQ3l5m/uA8e/fvZf7gPOVtZSpz+R/7Te+aZnTrKJsv2szo1lGmr822HFxlrsLO3TsLcexERNqlAHMedLrMfVqK0k4RaU5FaUVERERkBTN7ZhgaWDr2GxwYZGbPTDYNIl6gNm+B8bwFu0VEukUB5jwoSkZhUdopIstLemFPEREREekpY+vGOHBo6dhv4dACY+vGMmlP3EBtngLjeQt2S/coS11EAeZ8KEpGYVHaKSIrU30CEREREWmiNFJiassUw6uGWbt6LcOrhpnaMpVJPeNWArV5CoznKdgt3aMsdZFAi/wlqVJpf8WioixzX5R2ioiIiIiISNsmN0wycdxE5ovlVQO18wcXF6muBmrr21QNjJe3lRkcGGTh0EJmgfE8BbulO2pPflTfn+VtZSaOm9DiktJ3FGBOyvR0WPBuaCiUkZiaan3aeVGWuS9KO0VERERERKRtpZFS5oGyVgO1eQmM5ynYLd3RyskPkV6nAHMSKpUQXJ6fDz8Qrk9MKBArIiIiIiIi0qZ2ArV5CIxDfoLd0h3KUhdZpABzEmZmQuby/OJZKwYHw3YFmEVERA7TSVUpERER6S9FDtTmJdgtyVOWusgiBZiTMDYWymLUWlgI20VERGSJJKpKiYiISPoqc5XMgrwK1EoeFfnkh0iSjsi6AT2hVAqj4+FhWLs2XE5NKSVLRESkTm1Vqb17w2W5HLaLiIhIfk3vmmZ06yibL9rM6NZRpq+dzrpJIrlQGikxvn5cwWXpawowJ2VyEmZnYceOcKlULBERkcNUq0rVqlaVEhERkXyqzFUoby8zf3Cevfv3Mn9wnvK2MpU5nSEWEREFmJNVKsH4+GLm8vXXwwUXhEsRERFRVSkREZECmtkzw9DA0jPEgwODzOyZuft6Za7Czt07FXQWEelDCjB3y8tfDg99KJx+erh8+cvD/N+dOzUPWMdBRKRvdaOqlLoVERGR7hpbN8aBQ0vPEC8cWmBs3Rig8hkiIv1OAeZuuP56OPfcpdvOPReOOQY2b4bR0bDCUT+ang5/f78fBxGRPpZkVSl1KyIiIt1XGikxtWWK4VXDrF29luFVw0xtmaI0UlL5DBERYVXWDehJ3/lO4+3794cfCCsaTUz010KAtSs7zc+Hbf14HEREhFKp849+dSsiIiLpmdwwycRxE8zsmWFs3djdC5pVy2fMH5y/+7bV8hla9ExEpD8ogzlJ1Tm6D3zgyrftdEWjIs4H1spOIiKSIHUrIiIi6SqNlBhfP74kcLxS+QzJF9XKFpFuUIA5KbVzdDdvhlNOWbr/iLpD3cmKRkWdD6yVnUREJEHqVkRERLK3XPkMyRfVyhaRblGAOQm1c3T37g2X3/gGXHEFnH8+fP/78IlPJLOiUaPnKpeLkcncjZWdRESkr9RO4FG3IiIi0l1xs10nN0wye/YsO07dwezZs0xu6GCBBekK1coWkW5SDeYkVOfozi/WnGJwMGw77bRw/YQTQlHImZmQWtXu6LfZc83MFGNEPTmZzHEQEZG+Mz0dzqkODYXM5akpdSsiIiLdMr1rmvL2MkMDQxw4dICpLVPLBo5LIyVlLeeYamWLSDcpwJyEuHN0k1jRqBfmAydxHEREpK+stKCfuhUREZHk1Ga7VgOS5W1lJo6bUDCyoLKolV2Zqxy2KKSI9CaVyEhCmnN0NR9YRET6kBb0ExERSU8127VWNdtViintWtmq9yzSX5TBXKtSWTrHtv76ctKco6v5wCIi0md6YQKPiIhIUWSR7SrdN7lhkonjJrqeVawMeJH+owzmqulpGB2FzZvD5ctfDsceCyefHC6nY5xtK5VgfLz9gG/tykXdfi4REZEC0QQeERGR9KSd7SrpKY2UGF8/3tXXUhnwIv1HGczQuLDjuecuvc1ppy0WeuyGZisXiYiICKAJPCIiImlKK9tVeo8y4EX6jzKYoXFhx3oLC3DNNd15/toA99694bJcjpfJLCIi0kc0gUdERCQ9aWS7Su9RBrxI/1EGMzQu7JimaoC7mj0NiysXaQQtIiJdZGZPBt4LDAAfc/e/y7hJIiIiIlJwyoAX6S/KYIbGhR0HBpbeZmgIjjkmfo3kVmjlouSsVMe6lTrXkk96DfNPr1FhmNkA8H7gKcBDgUkze2haz5/GWyWtt2Onz6N/GxER6YbKXIWdu3dSmQsdzPWV67ngexdwfeX6WPu/edM3efPX3sw3b/pm4m1Z6Xqnj59XeWpn0m2pfzxlwIv0D2UwV9UXdtyxI5SpOOIIuOuu8PsjHtGdGsmlUnj82rrP5bKyl1u1Uh1r1bkuPr2G+afXqGgeCdzg7j8BMLNPAluA73f7idN4q6T1duz0efRvIyIi3TC9a5ry9jJDA0McOHSAPzr2j/jyT7589/5Tjj+Fb9z0jab7j7nnMfz0jp8C8Navv5VTjj+FS069JJG2lE8sM3X11OL1jWWmrlm8PrVliskN8TvD+sdv9f5pyVM7k25Lnv42EUmfuXvWbejYpk2b/Kqrrkr+gSuVEHA+8sgQXK4tYTE8DLOzyQSBKxUYHe3e4/eDlY6hjnHx6TXMP71GhzGz77r7pqzb0YyZ/SnwZHd/cXT9VOAP3P2sZvdJos9N462S1tux0+fRv42ISOfy3t+2o9P+tjJXYXTrKPMH51e+cQuuOOMKHnPsY7reluFVw8yePRsr87XR47dy/7TkqZ1JtyVPf5uIdFezPjf1Ehlm9mQz+4GZ3WBmr22w38zsn6P9/21mJ6bdxrtVVxLat+/wRQCrNZKT0GiRwSQfvx+sdAx1jItPr2H+6TUqImuw7bAzz2b2EjO7ysyuqiRQwyGNt0pab8dOn0f/NiIixZfHMe7MnhmGBlZYyL4Nl/740lTaMjgwyMyembYfv5X7pyVP7Uy6LXn620QkG6kGmGPWenwK8KDo5yXAB9NsY0PdrpGsGsydW+kY6hgXn17D/NNrVEQ3A8fUXD8a+Fn9jdz9I+6+yd03lRJIq03jrZLW27HT59G/jYhIseV1jDu2bowDh5JfyP6U3zkllbYsHFpgbN1Y24/fyv3Tkqd2Jt2WPP1tIpKNtDOY76716O4HgGqtx1pbgAs9+BawzswekHI7l2q0CODUVHJzV7v9+P1gpWOoY1x8eg3zT69REe0EHmRmx5nZEPBcYHu3nzSNt0pab8dOn0f/NiIihZfLMW5ppMTUlimGVw2zdvVahlcNc8rxS4PDpxx/yrL7j7nnMYfdvtXyGM3actYjz1r2+tSWqdilFRo9fiv3T0ue2pl0W/L0t4lINlKtwRyn1qOZfQH4O3e/Irr+FeA17t60AFXXajDXq9ZkHhvrzsiv24/fD1Y6hjrGxafXMP/0Gt2tCDUhzeypwFZgADjP3f92udsn2eem8VZJ6+3Y6fPo30ZEpH1Z9rd5H+NW5irM7JlhbN0YpZES11eu5zu7v8Mj1z+SE0onrLj/mzd9k0t/fCmn/E57weXl2rLS9U4fP6/y1M6k25Knv01EuqNZn7sq7XY02FYf4Y5dD5IwvYhjjz2285bFUSp1d9TX7cfvBysdQx3j4tNrmH96jQrF3S8GLs7iudN4q6T1duz0efRvIyJSWLke45ZGSksCfSeUTuCE0gmx9z/m2Md0HFhu9lwrXe/08fMqT+1Mui15+ttEJF1pl8iIU+sxk3qQIiIiIiIiIi3SGFdERPpe2gHmOLUetwMvjFbafRSw191/nnI7RURERERERFaiMa6IiPS9VEtkuPtBMzsLuITFWo/XmdmZ0f4PEabpPhW4AfgNcEaabRQRERERERGJQ2NcERGR9GswN6z1GHW61d8deFna7RIRERERERFplca4IiLS79IukSEiIiIiIiIiIiIiPUIBZhERERERERERERFpiwLMIiIiIiIiIiIiItIWBZhFREREREREREREpC0KMIuIiIiIiIiIiIhIWxRgFhEREREREREREZG2KMAsIiIiIiIiIiIiIm1RgFlERERERERERERE2qIAs4iIiIiIiIiIiIi0RQFmEREREREREREREWmLAswiIiIiIiIiIiIi0hYFmEVERERERERERESkLebuWbehY2ZWAWY7eIj7AL9KqDn9QsesPTpu7dFxa4+OW3uSPG6j7l5K6LFyIYE+t0rvz/bouLVOx6x1Omat0zFrnfrbZXTY3+r92B4dt/bouLVHx609Om6tS/qYNexzeyLA3Ckzu8rdN2XdjiLRMWuPjlt7dNzao+PWHh23dOg4t0fHrXU6Zq3TMWudjlnrdMy6R8e2PTpu7dFxa4+OW3t03FqX1jFTiQwRERERERERERERaYsCzCIiIiIiIiIiIiLSFgWYg49k3YAC0jFrj45be3Tc2qPj1h4dt3ToOLdHx611Omat0zFrnY5Z63TMukfHtj06bu3RcWuPjlt7dNxal8oxUw1mEREREREREREREWmLMphFREREREREREREpC19E2A2syeb2Q/M7AYze22D/WZm/xzt/28zOzGLduZNjOP2/Oh4/beZXWlmD8+inXmz0nGrud24mR0ysz9Ns315Fee4mdnjzex7ZnadmV2edhvzJsb/6G+Z2b+b2X9Fx+yMLNqZN2Z2npn90syubbJffUIXxf2MlMDMjjGzr5nZ9dH/8SuzblNRmNmAmV1jZl/Iui1FYWbrzOwzZvY/0Xvu0Vm3Ke/M7FXR/+a1ZjZtZmuyblPeNOp3zezeZvZlM/tRdHmvLNtYRBrjtkdj3PZojNs6jW/bozFu6/Iwvu2LALOZDQDvB54CPBSYNLOH1t3sKcCDop+XAB9MtZE5FPO43Qg8zt0fBrwN1cOJe9yqt3sXcEm6LcynOMfNzNYBHwD+xN1/D3h22u3Mk5jvtZcB33f3hwOPB/7RzIZSbWg+nQ88eZn96hO6JO5npCxxEHi1u58APAp4mY5ZbK8Ers+6EQXzXuBL7v4Q4OHo+C3LzNYDrwA2ufsGYAB4bratyqXzObzffS3wFXd/EPCV6LrEpDFuezTGbY/GuK3T+LY9GuO27XwyHt/2RYAZeCRwg7v/xN0PAJ8EttTdZgtwoQffAtaZ2QPSbmjOrHjc3P1Kd789uvot4OiU25hHcd5vAC8HPgv8Ms3G5Vic4/Y84HPufhOAu/f7sYtzzBy4p5kZcCRwGyFY1dfc/euEY9GM+oTuifsZKRF3/7m7Xx39fgch4Lc+21bln5kdDTwN+FjWbSkKM1sLnARMAbj7AXffk2mjimEVMGxmq4B7AD/LuD2506Tf3QJcEP1+AfCMNNvUAzTGbY/GuO3RGLd1Gt+2R2PcNuRhfNsvAeb1wE9rrt/M4QOzOLfpN60ekzLwH11tUTGseNyibJdnAh9KsV15F+f99rvAvczsMjP7rpm9MLXW5VOcY3YucAJhsLsLeKW735VO8wpNfUL36Nh2wMzGgI3AtzNuShFsBf4a0GdefMcDFeDjUWmRj5nZSNaNyjN33w38A3AT8HNgr7tfmm2rCuN+7v5zCCfSgPtm3J6i0Ri3PRrjtkdj3NZpfNsejXG7o+v9Qb8EmK3BNm/jNv0m9jExs5MJne9rutqiYohz3LYCr3H3Q91vTmHEOW6rgEcQMtKeBLzRzH632w3LsTjH7EnA94DfBn4fODfKUJPlqU/oHh3bNpnZkYSsoLPd/ddZtyfPzOzpwC/d/btZt6VgVgEnAh90943AHCpbsKyobvAW4DhCXztiZi/ItlXSJzTGbY/GuO3RGLd1Gt+2R2Pc7uh6f9AvAeabgWNqrh/N4VPX4tym38Q6Jmb2MML00y3ufmtKbcuzOMdtE/BJM5sB/hT4gJk9I5XW5Vfc/9Mvufucu/8K+DqhPmS/inPMziBMu3J3v4FQU+4hKbWvyNQndI+ObRvMbJAQXP4Xd/9c1u0pgMcAfxL1s58EnmBmn8i2SYVwM3Czu1cz5D9DCDhLcxPAje5ecfcF4HPAH2bcpqK4pTo9N7rU1PDWaIzbHo1x26Mxbus0vm2Pxrjd0fX+oF8CzDuBB5nZcVHh7+cC2+tusx14YbSy4qMI09t+nnZDc2bF42ZmxxK+SJ/q7j/MoI15tOJxc/fj3H3M3ccIg7eXuvvnU29pvsT5P90GPNbMVpnZPYA/oL8XH4pzzG4CnghgZvcDHgz8JNVWFpP6hO6J876VGlF9uSngenf/p6zbUwTu/jp3PzrqZ58LfNXdlVW6Anf/BfBTM3twtOmJwPczbFIR3AQ8yszuEf2vPpH+/m7Siu3AadHvpxG+50l8GuO2R2Pc9miM2zqNb9ujMW53dL0/WJXkg+WVux80s7MIK5kOAOe5+3Vmdma0/0PAxcBTgRuA3xDOiPS1mMftTcBRhLOTAAfdfVNWbc6DmMdN6sQ5bu5+vZl9CfhvQk3Nj7n7tdm1Olsx32tvA843s12EaTGvic6O9zUzmyasOHwfM7sZeDMwCOoTuq3Z+zbjZuXdY4BTgV1m9r1o2+vd/eLsmiQ97OXAv0SDup+gz79lufu3zewzwNWEBYauAT6Sbavyp0m/+3fA/zOzMiFY8OzsWlg8GuO2R2Pc9miM2zqNb9ujMW578jC+Nfd+L8EkIiIiIiIiIiIiIu3olxIZIiIiIiIiIiIiIpIwBZhFREREREREREREpC0KMIuIiIiIiIiIiIhIWxRgFhEREREREREREZG2KMAsIiIiIiIiIiIiIm1RgFn6kpmdbmZe83PAzH5sZu8wszUpPO/YCre7zMwu61Y7klCENjZiZuvM7BwzOzHrtoiIpEX9Xr5Fx+lFWbcjL8xsLOqrj8+6LSKSX3nv27JkZueb2Uwb93t89Lc9PsZt3cze3kbzMhG195w279vW8Yz52OeYmbd4n5793hQd65tXuE3s96mkZ1XWDRDJ2LOBm4F7As8EXhf9/vIsG1UQL826AW1aB7yZ8LpfnW1TRERSl9d+r6h9SlJOJ3wvPy/jduTFGKGvvgL4SbZNEZECyGvfJhLHx4AvtXiffv/edDXwaOD7WTdEFinALP3ue+5+Q/T7l83sQUDZzF7p7ndl2bC0mdlqd98f9/bunpsP81bbLiLSx3LZ7+WpT6mnPiYZeTiOeWiDiHRFLvs2kTjc/WbCCZJW7pPb701pcPdfA9/Kuh2ylEpkiCx1NTAM3Ke6wczuYWbvMrMbo2lXN5rZG8zsiJrbrDGz95jZtWa2z8x+YWb/bmYPSaphZnYfM/ugme02s/1m9j9m9pK625TM7MNm9kMz+42Z/dTM/tXM1tfd7pxoSskGM7vEzPYB/y/a52b2djN7RfS33mFml5vZ79U9xpJpOTXTVP7EzM41s1+ZWcXMPmFm6xq0c9rMfm1mt5vZx6P7rTjNpTplxswebWZXmtk88PfRvuea2Vej591nZteY2Wk19x0DboyufrRmOt3pNbd5lpl9Kzp+e8zs02Z27HJtEhEpsFz0e+32KWZ2nZl9tsHj/UF0/2fUbHu4mW2P+p15M/ummT227n7L9THPi/qVfWa218x2mdlf1N3/cWb2lajvnIv62A0r/e3A44DH1PRLtcfiODP7l+jv329m3zOzZ9Y9RrVff0j0nHNmdpOZnRHtP9XC94Z9ZvY1M/uduvvPRMf2z83sBjO708yuNrOTG7R3xb8xej2vMLM/jo7ZfqJsKzM7y8z+08xui/rZb5nZ02ru+3jga9HVL9cck8dH+w+b4myhpEZ9f77ca7nidyoRKbQ89W1XmNmTo8/u+egz8Q/MbJWFUh4/jz4Pzzezkbr7P8DMLrTQB+43s/82sxc0eJ4nRp/Zd1ooEfIX9beJewza+zPtDdHn7byZfd3Mfr/uBqeY2cXR3/qb6Pi+2swG6m6XSD9rZgMWxrPV57vM6sayK/xBiR5PC2PfD1gYm++PLi8ys9XR/sNKZJjZK83s+uiY3m5mV1lN328NSmSY2YPN7N8s9K3zFvrXJ9fdpvp94UFm9sXoWM+a2ZtWeh/YYl97ppm9M/r/uMPC94d7mNkDo9djn4XvEqfV3f+B0d99Y9S+n1joi++13PNG9z3DzBbM7LXR9cNKZNji/9tE9PpV32vPaPB4kxb6/juj99mfNDqm0hplMIssNQbsBW4FMLNVwCXAQ4G3AbuARwFvBO4NvDq632rCNKy3Az+P9r0U+JaZPcTdf9FJo8xsLfBNwhelcwhB0icBH7SQjfO+6Kb3Bu4kTAurAL8dtfGbUTvurHvobcAU8C6g9uz+C4AfAK8EhoB3A9uixzi4QnPfC3wBeB7wYMJg7hBQ28F8DvhfUTtvAP438D7i+y3gk8A/AK8H5qPtxwOfAf4u+ntOAj5mZsPu/iHCa/Os6PnfCWyP7vdjADM7E/gg8HHgrYTX9BzgcjN7mLvf0UIbRUSKYIwc9ns1VupTLgLeYmb3cvfba+73AuA24OLo7zoR+AZwDfDnwG+AM4EdZvaH7v7dmvse1seY2R8BnwD+Gfi/hCSNhxDKLhE9x9MI/eoXo+cHeA3wjagP+WmTv/Gl0WMPANVB7K+jxzwG+DbwS+BVhL79z4DPmtkz3H173WN9Gvho1PaXAudZyOR7PPBaYDA6pv8K/EHdfR8HPAJ4A7A/avt/mNnD3f0HbfyNvxsdr7cRylzcFm0fI0wHniGMRf4Y+IKZPdXd/4MQGHoZ8H7gFcDO6H7tZGs1ei3jfqcSkeIaIz992wMJY6m/BfYR+rHt0c8qQomkE6Lb/BL466jNI8DlwL0In18/JXzuXmRm93D3j0S3O4HQ110FPDf6G84BjiT0l7R4DFr1QuAm4Kzoud8KfMXMHuTu1c/944GvEMZ7dwKbojaWCH0TCfez5xCO2T8Bl0bPV99fNpT08YyCp1dG294O/DdwX2ALYZx92KwaM3s+8I+EY/kNQn/1sOgxmrX7twllpe4gvBZ7CX3pF83s6VH/WuvfCGPe9xD64bcQ3mMfX+EQQRjDX0b4LvZQwnv6LmAji99B/hL4uJld5e7XRff7bUKm9tnA7YT3xesJx/vRy/xtr4va9+fufv4KbfsdwvecdwK/IrwOn4n+f2+IHm8z8C+E98SrCSeitgJrgB/G+PulGXfXj3767ofQkTthsLqK0HG/CDgInFVzu1Oj251Ud/83AAeA+zZ5/AHgHoQP+Fc1eN6xFdp3GXBZzfU3EjrjB9Xd7qOED85Vy7TjmOg5n1mz/Zxo2ysb3MeBHwGDNdv+NNr+h8u08fHRbS6oe7xzo7ZbdP2U6HbPqbvd9mj741c4NudHt9uywu2OiF7bjwL/VbN9LLr/i+tufyShIz6vbvtY9FqfnfX7Vj/60Y9+2v0pYL8Xt085hjDg+4ua2wwSArEfqNn2FeB6YKiuzdcDn6/Z1rCPAf4PcNsKf8MNwFfqtq0l9NNbY/z9VzTYPhX9LUfVbf8yYUp49fo5UbtfWLPtXtHreyuwtmb7K6LbjtZsm4le32Nrtt2TEBS+qNW/Mfp77gJ+f4W/u9pXXwpsa/D6TzS4jwPn1G0bi7afHuO1bOs7lX70o5/8/VCMvm0BOL5m259E991Rd9vPATfWXD+LBmMjYAchED0QXf+X6LNrpOY2x0R/10yrx6Dm8/fxy/1t0W29wXOPRX/z25rcx6LX6g2EIOMR0fZE+tnoPbAP+FDd7V7TqP9o8BxJH8+3Er6nbFzmOc8BvOb6ucDVMd5bl9Vc/4foff/AuvfvD2ofi8XvC2fUPd4u4NIVnnMsuu9XG7x3HXhBzbbqd5A3L/N4q4A/iu67sWb7+YRA9BGEkxJzwNPq7nvY+5TF/7cH1Wy7b3T8X1+z7UrgWqLvktG2E6PHu2y5Y6Cf5X9UIkP63f8QPoRuIwziPuzu59bsfzIwC1xpYRrTquhs5aWEAeyjqjc0s+eY2bfNbA/hw3SOELR8cALtfDIhg+nGunZcAhxFOHNYbcdfmtl/WSh7cZBwRpkm7fi3Js/3ZXdfqLm+K7qMUyrii3XXdxHO/N4vuv4owod8/XN/JsZjVx0kZLQtEU31mTaz3YTXdQF4MfFeg0cTvqD8S90xvpnwPjmphfaJiORVUfq9qmX7FA/ZSpcTBnq1f8N9gAujdg4TsnM/DdxV8zcZYaBe//neqI/ZCdwrmgb6dDu89NODCFkz9X3Ib4D/bPAccT2ZkNmzt0H///AoG7fW3RlKHjK6fwl8y0Otwqr/iS6Pqbvvt9y9+p0BD7N2vkiUVdTG3zjj7t+r/4PM7BFm9gUzu4VwrBeAzST7vqlq9FrG/k4lIoWR577th+5eu1hp9TP4kgZ/w9FmZtH1k4Dd7n5Z3e0+Qcj8rX5WPRq42N3nqjeI+sZv1t0v9jGoZWZH1N7e6spaNHjuGUJt3EfXPMYDLJRxnCUEXxcI2bzrCAFASK6f/V/ACFH5xxqfbPT3NZD08TwF2Onu18R8fgjH4vfN7H0Wyj3cI8Z9TiL049Va5Lj7IWA6eqz67wv136+uJd5YH2q+a0QOe0/XfAe5+7uGmQ2Z2estlKaYJ7wPvhHtrv//WkV4zZ5HONlc395mfuTuP6ppxy+jdhwbtWGAkNH+WY8iy9HtrmaxlKa0SQFm6XfPBMaBpxIGmS81sxfW7L8vMMpisLL6851o/1EAZvbHwKcImVDPI0w7HSdkHa1JoJ33JXQa9e34dF07Xg58IPpbngU8ksXOrVE7ft7k+W6ru16duhPnb1npvg8Abq8LYAPcEuOxq34ZdZh3M7MjCRldDydMtXos4TU4jxCMWEn1y80ODj/O/4voGIuIFFxR+r2qOP3RhYT6xcdF108FbnD36uIv9yZk8byRw/+uswgD2trvxIf1Me5+OfBswkDp34CKme0ws4dFN6n2IVMNnuPptN+H3Jcw/bj+Md8d7a9/3Nvrrh9osg0Of50a9cO3ANV1HFr9Gw/7jmGh5MdXCK/Jy4E/JLxvvtSgPUk47LUk5ncqESmUPPdtzT6DG21fReivIHxONhqr/aJmP4SxVbPP71qxjkEDb6q7/VdWeJ7qtvUQAtSEmapPJwSVn0A4pn8b3XYNJNrPPqBJu+KONZM+nkfR4gJ+hO81f0l4/10C3GZmn7OwnlAzy71fjJBRXKvR96u47/FW3tO1j/lOQgb1J4CnEWIVz4r21T/32ug2V7J4TOOo/7tg6d92H8IJgF82uF0r8QhpQDWYpd9d64u1eL5KqIn0bjP7bHTW8lbCmaznNLn/THT5XMJg9vTqDjMbZJk6SS26lfAh+Mom+39Q046vuPvdNbRqBtyN+DL7uuXnhMH8YF2Q+X7N7tBAo3Y/mtDJP9bdr6hujM4kx3FrdHk6cF2D/aq/LCK9oCj9Xis+S6jX+wIzey+hluA7a/bvIZRreD9RVnM9d69dh6Bh3+junyHU8TuSMDXzXcCXzOxoFvuQ1xGCG/UONNgWx62E7J53Ndn/szYft5FG/fD9gN01bYH4f2Oj4/hkQl3k57j73QPumNlZVfsJdStrNQuMNGpD3O9UIlIcvdi33UbjrOn7R5fVz+Sf0/zzu1bcY1DvIyydCVI/Jlqp7/gdQsboqe7+ieoNomD+Egn1s9Ug6/1YOqaLO9ZM+nj+isUTtbFEmbUfBj5soYbzKYSazJ/i8PUTqm5j8b1R6/6EvrBR4DVtzwUudPe3VzdEr3UjtxHqbH8BmDaz5/nKa0HF8SvCiYD7Nth3PxZnf0sbFGAWibj7fjP7v4SFA15KyA76EmEBun3u/j/L3P0ehClUtU5l8Qx0p75EyPS5KZrmsVw7fl237YyE2pCUbxGOyzNZOnXp2R0+bnVwenfQOuqQt9Tdrpr9Nly3/UrCF6YHuvsFHbZFRCT3ct7vxebud5jZtuj5f0bIUrmoZv+cmX2DMMPl6rpgcjvPt4+wKN3xhIVkjiIEJWeA33P3v2vjYfcTah7X+xLhBOp17j7fYH+SHmVmx0RTgTGzexKyh6rTUjv9G6FxX/27wGNYmuHVrK+GMCV5Q922p7XQhrjfqUSkgHqlbyOUf3q2mT3G3WvLMzyPcJLs+uj6fwJPNbORalmHaLbIY1h6EjLuMVjC3X/G8icz6597jDCDttpPNPrcHwSev8xzdtLP/jehrMlzgK/WbH/uMveplfTxvBT4GwsL5v5XzDbcLSo18Skz+wMWFwJu5HLgbDMbi8qUVMtB/Blwjedjsfp7UPM+iDSNVbj7ZWb2FEKpsE+a2XM7DTK7+yEzuwr432Z2TrVMhpk9AjgOBZg7ogCzSA13325mO4H/Y2bnEor8n0FYCfcfgf8iZM38DmGBhme4+28IHcwzzOw9hLNsjyAsorMnoaa9h9A5fCN6jh8Qaks9hJCxWw2ifgl4jZm9njCV5AmEBfpyw90vNbMrgI+Y2X0IizX8KWHgDyHLrB1XEoLr7zezNxOOz98QzlL+Vs3tbiGccX6umVW/gNzo7rdGX0bfb2YlQm2pvYQzzo8jFPz/1zbbJiKSSznu91p1ITBJWGX8Cnevr6P3V8DXgUvMbIqQoXQfwqIuA+7+2uUe3MzeSshs+RphgHk04e/9nrtXotu8DNhmZkOEE6i/iu7zh4Rg5j8t8xTfJ0zp/jPgx8Ad7v4DwtTk7wBfj16fGcI01w2ERaNetNKBacEtwKVmdg4hwPsaQl/6NggZVR3+jRCyzg4CF0bvrwcQXrObWFq674fR7V5kZrdF7flBNED+JGGw/gbCSevHEl77uOJ+pxKRguqRvu18wkyLz0WfdzcTgrKbCQvbVsv/vJ2QqHOpmb2b8He9hcOn+8c9Bq2ar3nu1dFz/5rwWQshED4L/K2ZHSIEGF9V/yBJ9bPuvid6/d5gZncQArzjQDnm35P08XwP4aTADjN7O2EtifsQkqDObBT4NbOPEBKf/pNwMuF3CSc6Ll2m3e8hzMT9cjQW/jXhBMvv0tpJ2G76EnCame0ixACeRXjtmnL3b5jZkwlj809FQeb6IHWr3kw4lv8WHev7EEp3/IL2YxGCAswijfwNodbRme7+HjN7EqGm70sIZ7XmCIO/L7I4FeejhHpRLyKcWdxJmKLbbBG9lrj7XjP7Q8JA8zWEoOcewqDoszU3fSthsYRXETK4LgeeBNQuLJEHzyKsCPsuwoJ/2wm1Mc8nBHVb5u4VM3smYfrQZwhfTN5LmNL25prb3WVmLwbeQRjoriJ8OTjf3T9sZj8F/i/hi8AgYXrX14HvtdMuEZECyF2/14YvEwYG6wl94RLufrWZjRP6g38mnHisAFcDH4rx+N8mDHTfQ+hXfkkYnLyx5jkuNrOTCCvIf4yQffsLQhD0Uys8/rsIU6E/RlhM6nLCyug3mdkmwsDnHYSFnW4lLMaT9GybywkrsL+DMLD/PvAUd/9h9QYd/o24+3Vm9nzCa7Sd8L56LaF0xuNrbnermZ1F+M5zOSF78OSofe8kfNc5K7rvxYSB97fj/JEtfKcSkWIrdN8Wzb55HPD3hGzgexI+p5aUmnD3683sqYRM7U8Rxi7vIsx+eXzN7RZiHoNWXRg9zrmEQN1O4Lnuflv0vAfM7BnR/gsJpQ/OI5xY/GjN4yTZz55DqDv8YkJf8W3C69ioDOISSR/PKOD9GELgJDSJ6QAAAWxJREFU+rWEbOxbCNnVzY75Nwnj01MJ31d+Rqhb/OYmt8fdf2ZmfxS19YOEYP/3gKe5+5dW+rtT8nLC61Ktv30x4QTxsjWW3f2b0bH+EvBpM2tWliQWd/9y9F3kzYT/7RuAVxO+F7QVi5DAahZOFBHJjJm9n3DW9d7uvn+Fm4uIiEhCzGyGkPn9gqzbIiIiIpKmqM73DcDfuvvbsm5PUSmDWURSZ2anE87GXkeYSvRk4Ezg3Qoui4iIiIiIiEjSzGwY+CfCbOZfAccDfw38hpAZL21SgFlEsjAHnE2oUbWasALv6wlTkUREREREREREknYIuD+hbMtRhNjEN4Bnu/vPs2xY0alEhoiIiIiIiIiIiIi05YiVbyIiIiIiIiIiIiIicjgFmEVERERERERERESkLQowi4iIiIiIiIiIiEhbFGAWERERERERERERkbYowCwiIiIiIiIiIiIibVGAWURERERERERERETa8v8BBTfFft0WeHcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot scatterplots of this\n",
    "fig, ax = plt.subplots(1,3, figsize=(20,8))\n",
    "\n",
    "# get correlations\n",
    "lrs_corr, _ = pearsonr(Param_recov['real_alpha'].values, Param_recov['est_alpha'].values)\n",
    "bs_corr, _ = pearsonr(Param_recov['real_beta'].values, Param_recov['est_beta'].values)\n",
    "ws_corr, _ = pearsonr(Param_recov['real_w'].values, Param_recov['est_w'].values)\n",
    "\n",
    "# round r to 2 decimals\n",
    "lrs_corr_s = round(lrs_corr,2)\n",
    "bs_corr_s = round(bs_corr,2)\n",
    "ws_corr_s = round(ws_corr,2)\n",
    "\n",
    "# plot learning rate\n",
    "Param_recov.plot(x = \"real_alpha\", y = \"est_alpha\" ,kind=\"scatter\", ax=ax[0], \n",
    "                xlabel=\"real alpha\", ylabel =\"estimated alpha\", color= \"r\")\n",
    "\n",
    "# text bubble with correlation \n",
    "ax[0].text(0,0.9, f'$alpha$ r = {lrs_corr_s}', color = 'black', size=16,\n",
    "       bbox=dict(facecolor='none', edgecolor='black', boxstyle = 'round'))\n",
    "\n",
    "# plot inverse temperature\n",
    "Param_recov.plot(x = \"real_beta\", y = \"est_beta\", kind=\"scatter\", ax=ax[1],\n",
    "                xlabel=\"real beta\", ylabel =\"estimated beta\", color= \"b\")\n",
    "\n",
    "# text bubble with correlation \n",
    "ax[1].text(0,9, f'$beta$ r = {bs_corr_s}', color = 'black', size=16,\n",
    "       bbox=dict(facecolor='none', edgecolor='black', boxstyle = 'round'))\n",
    "\n",
    "# plot model-based decision making\n",
    "Param_recov.plot(x = \"real_w\", y = \"est_w\" ,kind=\"scatter\", ax=ax[2],\n",
    "                xlabel=\"real w\", ylabel =\"estimated w\", color = \"g\")\n",
    "\n",
    "# text bubble with correlation \n",
    "ax[2].text(0,0.9, f'$w$ r = {ws_corr_s}', color = 'black', size=16,\n",
    "       bbox=dict(facecolor='none', edgecolor='black', boxstyle = 'round'))\n",
    "\n",
    "\n",
    "fig.suptitle('Parameter recovery results', size = 24)\n",
    "ax[0].set_title(\"Learning rate\", size = 18)\n",
    "ax[1].set_title(\"Inverse temperature\", size = 18)\n",
    "ax[2].set_title(\"Model-based decision making\", size = 18)\n",
    "ax[0].set_xlabel(\"Real learning rate\", size = 16)\n",
    "ax[1].set_xlabel(\"Real inverse temperature\", size = 16)\n",
    "ax[2].set_xlabel(\"Real model-based decision making\", size = 16)\n",
    "ax[0].set_ylabel(\"Est. learning rate\", size = 16)\n",
    "ax[1].set_ylabel(\"Est. inverse temperature\", size = 16)\n",
    "ax[2].set_ylabel(\"Est. model-based decision making\", size = 16)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f3122",
   "metadata": {},
   "source": [
    "If everything worked out well, this shouldn't look too bad at all. Pretty neat right?\n",
    "\n",
    "The correlation strength _r_ is plotted as a text bubble for each parameter. Generally, you would expect strong correlations for these types of analyses (for example, r > 0.6). What values do you get out?\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cd3235",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have made it to the end of this workshop. Congratulations! Thanks for sticking with us, we hope you learned a lot. Please let us know if you have any feedback, so we can improve this workshop!\n",
    "\n",
    "![Jlo_clap](https://media.giphy.com/media/fnK0jeA8vIh2QLq3IZ/giphy.gif?cid=ecf05e4775ki51jbkrhdf8wduwauofz4e83wk3lz0x94mrbs&rid=giphy.gif&ct=g)\n",
    "\n",
    "<br>\n",
    "Before we say goodbye, here are a few things to keep in mind about how to go forward with model-fitting in general, and fitting data for this type in particular. \n",
    "\n",
    "Because we wanted to make this an accessible workshop, we have really oversimplified a lot of the code used to simulate behavior and to fit the data. When we fit these models, we include parameter values that account for a host of other things, such as participants being prone to hit the same key regardless of the outcomes in the task, and we also may include free parameters for future reward discounting and the decay of the eligibility of prior actions for current updates, as we briefly talked about in Step 1 of the workshop.\n",
    "\n",
    "Moreover, a lot of computational modelers have stopped to find the maximum likelihood, but rather the maximum _posterior probabiity_. Derived from Bayes theorem, this probability does not just reflect the likelihood of the data under a model but also the probability of that model per se. We have found that this provides superior fits to data.\n",
    "\n",
    "So as a cautionary warning, it is good to keep in mind that for best practices of computational modeling of behavioral data, some more work is involved. To keep things simple in the workshop today we therefore had to cut some corners. We believe that this was most likely to benefit you, especially if you were unfamiliar with computational modeling. However, this also means that if you think this kind of work is for you, there is still a lot to learn. We hope you are excited about that! \n",
    "\n",
    "A good way to start would be to watch David Silver's introduction to reinforcement learning (https://www.youtube.com/watch?v=2pWv7GOvuf0link link also in Step 1 script), where he goes over the basics of reinforcement learning. \n",
    "\n",
    "Additionally, the paper \"Ten Simple rules for the computational modeling of behavioral data\" by Rober Wilson and Anne Collins is a great overview of good practices for computational modeling: https://elifesciences.org/articles/49547\n",
    "\n",
    "We hope you enjoyed the workshop and it was our goal to show you that, even though this stuff seems daunting from the outset, understanding is often just few headscratches away.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b2d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
