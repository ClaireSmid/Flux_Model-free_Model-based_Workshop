{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b06e0c1",
   "metadata": {},
   "source": [
    "# Flux Computational Workshop 17 September 2021\n",
    "## Computational modeling of goal-directed and habitual reinforcement-learning strategies\n",
    "\n",
    "Claire Smid<sup>1</sup> and Wouter Kool<sup>2</sup><br>\n",
    "<sub><sup>1</sup> University College London<br>\n",
    "<sup>2</sup> Washington University in St. Louis<br></sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b19bc7d",
   "metadata": {},
   "source": [
    "### Workshop Step 2: Goal and Purpose\n",
    "\n",
    "In the last section, you successfully implemented a model-free learner that performs our two-step task. Here, we will insert a model-based decision maker into this function. At the same time, we will add a 'weighting' parameter that lets us simulate fully model-free agents, fully model-based agents, and anything in between. As we saw before, humans perform tasks like this with a mixture of model-free and model-based control, so in this workbook, we will be getting closer to actually simulating _human behavior_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe95a12",
   "metadata": {},
   "source": [
    "# Section 2: model-based reinforcement learning\n",
    "## The two-step task\n",
    "Quick reminder: each trial of our two-step task starts in one of two 'first-stage states', that both offer a choice between a pair of spaceships. For each pair, one spaceship will always lead to the red planet, and the other always to the purple planet. Here is the task structure once again:\n",
    "\n",
    "![Task_structure](https://drive.google.com/uc?export=view&id=1IHQ7adLC9cg7wfEBlZNTd3QYWVIdVmBA)\n",
    "_Stimuli adapted from Decker et al. 2016_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c28565c",
   "metadata": {},
   "source": [
    "### Model-free vs model-based reinforcement learning\n",
    "In the first script, we talked about the distinction between model-free and model-based reinforcement learning. \n",
    "\n",
    "The key characteristic of a model-based decision maker is that they use the structure of the environment to _plan towards goals_. They use their knowledge of how spaceships transition to planets to infer the expected value of each spaceship. A model-free agent on the other, as you know well, only learns throughs simple action-outcome association.\n",
    "\n",
    "In other words, this means that model-based agents can transfer experiences obtained after one pair of spaceships to the other pair, but model-free agents can not. \n",
    "\n",
    "This difference becomes explicitly clear in the graphs below. These depict the probability that an agent revisits the same planet as on the previous trial ('stay' probability) as a function of (1) whether or not a reward was received on that trial and (2) whether the starting state is the same. As you can see, the model-based agent (middle panel) is unaffected by which first-stage state it encounters. It simply goes to a planet if it earned reward there on the previous trial, regardless of the starting state is the same. The model-free learner (left panel), on the other hand, is only more likely to revisit the same second-stage state if it encounters the same pair of rockets.\n",
    "\n",
    "As we mentioned before, humans display a mixture of both strategies (third panel). Let's write a computational model that is able to capture this type of behavior.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1gvISmBfkmkgRqqgcJE6sLbztGqlKqagG) <br>\n",
    "_Edited from Doll et al. 2015_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e501c00",
   "metadata": {},
   "source": [
    "## Simulating a mixture of model-based and model-free behavior on the two-step task\n",
    "We will achieve this in a few steps. First, we will copy the softmax and temporal difference learning rules from the last workbook. Jupyter/Google Colab notebooks do not have access to functions in other notebooks, so we must introduce them here again.\n",
    "\n",
    "More importantly, we will then insert a model-based decision maker in the simulation code. It will use the transition structure to compute an expected value for all actions (spaceships) in the current first-stage state.\n",
    "\n",
    "Finally, we will use mixture parameter $w$ in this model to calculate a weighted combination of model-based and model-free values in the first-stage state, and then let the agent decide use this combination of Q-values.\n",
    "\n",
    "With this function, we are hoping to replicate the model-based panel in the middle, as well as the mixture panel on the right. It looks like they would behave quite differently. Let's see this for ourselves!\n",
    "\n",
    "#### Quick note on using multiple Jupyter/Google Colab notebooks\n",
    "We will need to run all the cells that we use in the script, and import all the packages, to have access to them in the current script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf00fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we'll import some packages that we'll use\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc798a",
   "metadata": {},
   "source": [
    "### Step 1. Drifting reward probabilities\n",
    "Just as like in the previous script, we are going to generate drifting reward probabilities in the same way. You can run this script as is (it is identical to the code in the first script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f88174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create drifting reward rates \n",
    "def generate_rewards(n_trials, bounds, init_vals, drift_rate):\n",
    "\n",
    "    # pre-allocate the rest\n",
    "    reward_probs = np.ones((n_trials, 2)) * 0.5\n",
    "\n",
    "    # To start with, we initialise one planet lower than the other\n",
    "    reward_probs[0,:] = random.sample( init_vals, len(init_vals))\n",
    "\n",
    "    # next, we loop through the trials, and add Gaussian noise to each planet's reward probability for each trial separately\n",
    "    for t in np.arange(n_trials-1)+1:\n",
    "\n",
    "        for state in range(2):\n",
    "\n",
    "            upd = np.random.normal(0, drift_rate) # random Gaussian noise\n",
    "            \n",
    "            reward_probs[t, state] = reward_probs[t-1, state] + upd\n",
    "            \n",
    "            # reflecting bounds \n",
    "            reward_probs[t, state] = min(reward_probs[t, state],max(bounds[1]*2-reward_probs[t, state],bounds[0]))\n",
    "            reward_probs[t, state] = max(reward_probs[t, state],min(bounds[0]*2-reward_probs[t, state],bounds[1]))\n",
    "            \n",
    "    return reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c465f",
   "metadata": {},
   "source": [
    "### Step 2. Choice and update results\n",
    "Same as previously, we are going to write out the softmax function and the temporal difference updating rule. To save time, just copy and paste your final functions from the last script here!\n",
    "\n",
    "#### First, write out the softmax rule as you wrote it previously. \n",
    "\n",
    "You can copy-paste your previous softmax rule here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Q,beta): \n",
    "    \n",
    "    p = # calculate the reward probabities\n",
    "    \n",
    "    return p # here we return the computed reward probabilities\n",
    "\n",
    "# hint: you should us np.exp, and np.sum. You can multiply numpy arrays with a scalar  using *, \n",
    "# and you can divide numbers using /\n",
    "# Google things you do not understand! Or ask us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a402054",
   "metadata": {},
   "source": [
    "#### Second, let's write out the temporal difference updating rule\n",
    "Write out (or copy-paste) your temporal difference updating rule again like you did in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f336878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_rule(alpha, Q_chosen, PE):\n",
    "    \n",
    "    Q_chosen_updated = # compute \n",
    "    \n",
    "    return Q_chosen_updated\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719863d4",
   "metadata": {},
   "source": [
    "### How exactly does model-based learning work?\n",
    "\n",
    "Perhaps, you feel a bit intimidated by the term \"model-based control\". It sounds like it will be pretty complicated, and difficult to code. However, it turns out that model-based control in this task is pretty straightforward. To see this, we are first going to guide you through an example, without any programming, so you can get a good intuition!\n",
    "\n",
    "Imagine that an agent has been playing the task in the diagram already, and that it has learned some model-free values for each spaceship and for each alien. These model-free values are given in the diagram as well. (We are assuming that model-free learning always happens, regardless of whether each decision is more model-based or more model-free.)\n",
    "\n",
    "Given this state of the learner, an agent can make choices in (a) the left or the right first-stage state, and (b) it can do this using either the first-stage model-free values or calculate model-based values online. \n",
    "\n",
    "To see how this would lead to different forms of valuation, we have included a table on the right that you should try to fill out (perhaps on a sheet of paper or in an Excel spreadsheet). For each first-stage state, try to write down what the model-based and the model-free values for the spaceships are.\n",
    "\n",
    "<mark>First, let's imagine that you are a model-free decision maker, which values would you use for each pair of spaceships in the two first-stage states?</mark>\n",
    "\n",
    "<mark>Then, imagine that you are fully model-based and you plan towards the second-stage model-free values. What would the model-based values of the spaceships be in both first-stage sates?</mark>\n",
    "\n",
    "![Task_structure_Toy_Example](https://drive.google.com/uc?export=view&id=1ZfuZh2UoEaBVnm2OkhE8xRytAtMFGpiE)\n",
    "    \n",
    "**Hint 1**: You don't have to do any math! All values you need are already in the figure.\n",
    "\n",
    "**Hint 2**: A model-free agent only uses the values it has approximated through trial-and-error, while the model-based agent plans through the task using the _underlying structure_ of the task!\n",
    "\n",
    "If you get stuck with this, do let Claire or Wouter know!\n",
    "\n",
    "We hope that after this section, you know that model-based valuation of avalaible actions on this task is pretty straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95298bd7",
   "metadata": {},
   "source": [
    "### Step 3. Simulating model-based behavior on the two-step task\n",
    "Now, we finally get to code up a version of model-based control! Hurray! Even though this may sound a bit daunting, you will soon discover that you have done most of the work already. On each trial the agent will (a) compute compute model-based values for the spaceships in the current first-stage state and then (b) 'mix' those with the existing model-free values in order to choose an action based on this mixture.\n",
    "\n",
    "#### Model-based calculations\n",
    "To include a model-based controller, you should add lines of code at the start of each trial that calculate the model-based values (<code>Qmb</code>) of the available space ships _on the fly_. This means that on each trial the model-based system will think about the alien that each space ship visits, and then it will use the value of this alien as the expected, model-based, value of choosing that space ship. (Just like you did in the example above.)\n",
    "\n",
    "#### Mixture model\n",
    "In the next step, you will combine these model-based values with the already available first-stage model-free values. Specifically, you will compute a _weighted_ mixture of these values according to a mixture parameter <code>w</code>. This mixing weight needs to be provided as an input parameter for the <code>mb_mf_agent</code> function below, alongside the learning rate <code>alpha</code> and the inverse temperature <code>beta</code>. \n",
    "\n",
    "In a single line of code, you will mix these two sets of Q-values into a variable called <code>Qnet</code>. You need to do this in such a way that when <code>w=0</code> the agent is fully model-free and <code>Qnet</code> is identical to <code>Qmf1</code>. In contrast, when <code>w=1</code> the agent is fully model-free and <code>Qnet</code> is identical to <code>Qmb</code>. More importantly, w should be able to take on any value between 0 and 1, and therefore it will describe a _spectrum_ of control strategies, ranging from fully model-free to fully model-based and everything in between. This is going to require some thinking on your end!\n",
    "\n",
    "**Moreover, you should rewrite your code so that the agent uses <code>Qnet</code> (and not just Qmf1) to make its decision at the first stage.**\n",
    "\n",
    "#### Let's get to it!\n",
    "In the section below, you will have to finish a function with both a model-free and a model-based agent. However, this will be mostly copying your work from the previous notebook straight into this one. Nothing will change to the updating of model-free values for any of the states, how the transitions occur, or how the rewards are generated.\n",
    "\n",
    "All the new code will occur after the first-stage state is randomly drawn and before the first-stage cohice is realized. We provide some more hints in the comments.\n",
    "\n",
    "In the end, your completed code will be able to simulate the behaviour of humans performing this task -- like a mixture of habitual and goal-directed strategies. We again provide code below for you test your work, and plot its behavior.\n",
    "\n",
    "After you finish this code, you will investigate the differences between model-free and model-based performance. Does model-based control pay off? Next, you will start to investigate whether it is always more accurate, or whether there are some settings in which it underperforms.\n",
    "\n",
    "In the section below, you can use your knowledge about the temporal difference updating rule and the softmax function and the model-free model to complete the model-based agent's code. The completed code will simulate behaviour of a model-based agent on the task, which we can then plot afterwards and compare to our plots above.\n",
    "\n",
    "Some things to keep in mind:\n",
    "- You need to include the 'w' parameter which acts as a mixing weight\n",
    "- Use the toy example above to implement *model-based* control.\n",
    "- Use Qnet to make the decision at the first stage\n",
    "\n",
    "Give this a go, and if you get stuck at any point, do let Claire and Wouter know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a for loop, and generate choices for the agent as they go through the trials and \n",
    "# learn from rewards\n",
    "\n",
    "def mb_mf_agent(n_trials, alpha, beta, w):\n",
    "    \n",
    "    # We'll use your generate_rewards function to generate drifting reward probabilities\n",
    "    bounds = [0, 1]\n",
    "    init_vals = [0.4, 0.6]\n",
    "    drift_rate = 0.2\n",
    "    reward_probs = generate_rewards(n_trials, bounds, init_vals, drift_rate)\n",
    "    \n",
    "    # pre-allocate empty arrays for the data (in chronological order)\n",
    "    \n",
    "    # from previous trial\n",
    "    prev_rews = np.zeros(n_trials) # this will log the rewards won on the previous trial (i-1) \n",
    "    \n",
    "    # first stage (spaceships)\n",
    "    first_stage_state = np.zeros(n_trials) # this will log which spaceship pair the participants saw\n",
    "    state_sim = np.zeros(n_trials) # this will log whether participants saw the same spaceship pair, or the other one \n",
    "    choices = np.zeros(n_trials) # which rocket the participants chose\n",
    "    \n",
    "    # second stage (planet)\n",
    "    second_stage_state = np.zeros(n_trials) # this will log which planet the participants transitioned to\n",
    "    stay = np.zeros(n_trials) # whether participants repeated their transition to the same planet, or not \n",
    "    rewards = np.zeros(n_trials) # this will log how many rewards the participants won on this trial\n",
    "\n",
    "    # Initialise model-free values for the spaceships\n",
    "    Qmf1 = np.zeros((2,2)) + 0.5 # do we need to explain arrays in arrays?\n",
    "    \n",
    "    # Initialise model-free values for the planets\n",
    "    Qmf2 = np.full(2, 0.5)\n",
    "    \n",
    "    # Initialise model-based values\n",
    "    Qmb = np.full(2, 0.5)\n",
    "    \n",
    "    # Now, we loop through the trials, making choices\n",
    "    for i, reward_prob in enumerate(reward_probs):\n",
    "\n",
    "        # Randomly start in rocket pair 1 (0) or pair 2 (1)\n",
    "        s1 = np.random.choice([0, 1])\n",
    "        \n",
    "        # calculate model-based q-values - fill these in\n",
    "        # note that Qmb just reflects the MB values of the currently available actions. \n",
    "        # remove: They are calculated online, so there is no updating necessary\n",
    "        Qmb[0] = Qmf2[0]                                #--- Fill this in\n",
    "        Qmb[1] = Qmf2[1]                                #--- Fill this in\n",
    "        \n",
    "        # mix the two sets of Q-values here with the mixing weight (remember that when w = 0, only the model-free\n",
    "        # q-values matter, while when w = 1, only the model-based values are taken into account)\n",
    "        Qnet = w * Qmb + (1-w) * Qmf1[s1]               #--- Fill this in\n",
    "        \n",
    "        # Agent makes a choice using softmax\n",
    "        p = softmax(Qnet, beta)                         #--- Fill this in\n",
    "\n",
    "        \n",
    "        ### the code below will be identical to the model in Step1\n",
    "        \n",
    "        # Make a weighted choice using the np.random.choice function and the variable p (you can use help(function) \n",
    "        # to find out more) (identical to first model)\n",
    "        choice =                                        #--- Copy from function in Step 1\n",
    "        \n",
    "        # Make the transition to the planet\n",
    "        s2 =                                            #--- Copy from function in Step 1\n",
    "\n",
    "        # Calculate first prediction error (no reward here)\n",
    "        pe_1 =                                          #--- Copy from function in Step 1                            \n",
    "        \n",
    "        # update choice\n",
    "        Qmf1[s1,choice] =                               #--- Copy from function in Step 1\n",
    "        \n",
    "        # Agent receives a reward (1 or 0) (depending on reward probability)\n",
    "        r =                                             #--- Copy from function in Step 1\n",
    "        \n",
    "        # Calculate second prediction error (actual reward)\n",
    "        pe_2 =                                          #--- Copy from function in Step 1                                      \n",
    "        \n",
    "        # update value of alien (planet)\n",
    "        Qmf2[s2] =                                      #--- Copy from function in Step 1\n",
    "        \n",
    "        # update value of choice (spaceship)\n",
    "        Qmf1[s1,choice] =                               #--- Copy from function in Step 1\n",
    "        \n",
    "        \n",
    "        # save data from this trial to our pre-allocated arrays\n",
    "        rewards[i] = r\n",
    "        first_stage_state[i] = s1\n",
    "        second_stage_state[i] = s2\n",
    "        \n",
    "        # these are based on the previous trial (i-1)\n",
    "        if i != 0: # start at second trial (first trial this is set to 0)\n",
    "            # whether they transitioned to the same planet on this trial\n",
    "            if s2 == second_stage_state[i-1]:\n",
    "                stay[i] = 1\n",
    "            # whether they saw the same spaceship pair on this trial\n",
    "            if s1 == first_stage_state[i-1]:\n",
    "                state_sim[i] = 1\n",
    "            # whether they received a reward on the previous trial\n",
    "            prev_rews[i] = rewards[i-1]\n",
    "        \n",
    "        \n",
    "        # you can uncomment the lines below to see what happens each trial in printed form.\n",
    "        # note that if you uncomment these, the figure in the cell below will print all the way \n",
    "        # at the bottom of the printed statements (just comment the lines out again to see figure immediately)\n",
    "        \n",
    "#         ###   comment code between this this out or in (shortcut = ctrl + /)\n",
    "        \n",
    "#        # print spaceship choice and which pair was seen \n",
    "#        # spaceship chosen\n",
    "#         if choice == 0:\n",
    "#             position = 'left'\n",
    "#         else:\n",
    "#             position = 'right'\n",
    "            \n",
    "#         print(f'trial: {i+1}, rocket pair: {s1+1}, spaceship chosen: {position}')\n",
    "              \n",
    "#         # print which planet was transitioned to\n",
    "#         if s2 == 0:\n",
    "#             color = 'red'\n",
    "#         else:\n",
    "#             color = 'purple'\n",
    "\n",
    "#         print(f'transitioned to {color} planet, first prediction error: {pe_1}')\n",
    "#         print(f'stay: {bool(stay[i])}, agent received: {r}')\n",
    "#         print(f'{color} planet prediction error: {pe_2}')\n",
    "#         print(f'Model-free q-values for spaceships:\\n[1:left, 1:right,\\n2:left, 2:right]\\n{Qmf1}')\n",
    "#         print(f'Model-free q-values for aliens/planets:\\n[red, purple]\\n{Qmf2}')\n",
    "#         print(f'Model-based q-values for spaceships:\\n{Qmb}') # this is now 2 values\n",
    "#         print(f'Qnet for planets\\n[red, purple]\\n{Qnet}\\n')\n",
    "        \n",
    "#         ###   comment code between this out or in\n",
    "\n",
    "        \n",
    "    # data returned by function\n",
    "    return stay, state_sim, prev_rews, rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d7617",
   "metadata": {},
   "source": [
    "#### Testing the mixture model\n",
    "\n",
    "In the cell below, you can run the mixture function that you are writing, and you can see if it runs without any errors. If you do run into errors, try to see if you can decipher what might be going wrong from the outputs (and Google). If you get stuck, do ask Claire or Wouter for help with your code. \n",
    "\n",
    "Note that the code running without any errors does not mean that your code is doing what you _want_ it do. Your code does not know your intentions, and will blissfully execute your commands. There are many ways in which you can write wrong code, without this notebook giving you an error message.\n",
    "\n",
    "To debug your code, try printing the different choices that the simulated agent makes, and the Q-values for each choice. Are they updating as expected? At the end of the function above, we have provided useful print statements that we commented out. You can uncomment these, and use them to view the output of each trial consecutively to see how things change. Note that you will need to rerun the cell above after any change you make before the code below reflects these changes.\n",
    "\n",
    "Try to think about the toy example you completed above. Are the model-based (Qmb) values calculated as you expect? What needs to happen here? In addition, are the model-free (Qmf) values still updating as you expect them to?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a3b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise values\n",
    "n_trials = 200\n",
    "alpha = 0.5\n",
    "beta = 4\n",
    "w = 1\n",
    "\n",
    "# run an agent\n",
    "[stay, state_sim, prev_rews, rewards] = mb_mf_agent(n_trials, alpha, beta, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb41c5a",
   "metadata": {},
   "source": [
    "#### Plotting new stay probability plots for _w_ = 1 or _w_ = 0\n",
    "You can run the code below to plot simulated behaviour from your code.\n",
    "\n",
    "A great way to test would be to set _w_ to 0 or 1, and to view whether this results in the right \"stay behavior\" as a function of whether a reward was received on the previous trial, and whether the starting state is the same. You can see the example plots of this behavior for a pure model-free and model-based agent below. Is there a qualitative match?\n",
    "\n",
    "Remember that you can increase the trial size with which you simulate data to see a less \"noisy\" plot, to help you detemine if you are generating the behavior you expect. You will notice that the plots are quite variable with just 200 trials, but very stable with 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42da4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an agent and plot the data\n",
    "n_trials = 200\n",
    "alpha = 0.5\n",
    "beta = 4\n",
    "w =             # Fill this in\n",
    "\n",
    "# to print correct title on plot\n",
    "if w == 0:\n",
    "    behave = 'model-free'\n",
    "elif w == 1:\n",
    "    behave = 'model-based'\n",
    "else:\n",
    "    behave = 'mixture'\n",
    "\n",
    "[stay, state_sim, prev_rews, rewards] = mb_mf_agent(n_trials, alpha, beta, w)\n",
    "\n",
    "# calculate probabilities\n",
    "df = pd.DataFrame({\"Rocket_State_Sim\":state_sim,\"Previous_Rewards\":prev_rews,\"Stay\":stay})\n",
    "plot_data = df.groupby([\"Rocket_State_Sim\",\"Previous_Rewards\"]).mean().unstack()\n",
    "a = plot_data.reset_index()\n",
    "\n",
    "# plot the graph\n",
    "fig, ax = plt.subplots()\n",
    "a.plot(x = \"Rocket_State_Sim\", kind=\"bar\", ylim = [0,1],\n",
    "       stacked=False,ax=ax)\n",
    "\n",
    "# figure legends\n",
    "ax.legend([\"Loss\",\"Win\"])\n",
    "ax.set_ylabel(\"Stay Probability\")\n",
    "plt.xticks(rotation=1)\n",
    "ax.set_xticklabels(['Different','Same'])\n",
    "ax.set_xlabel(\"Starting State Similarity\")\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# title corresponding to w value\n",
    "ax.set_title(f'Simulated {behave} agent', fontweight='bold')\n",
    "\n",
    "# text bubble for w according to value\n",
    "ax.text(-0.6,0.9, f'w = {w}', color = 'black', size=12,\n",
    "       bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', boxstyle = 'round'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c92d788",
   "metadata": {},
   "source": [
    "#### Let's compare these to both the model-free and model-based stay probabilities, and to the behavior from participants on a task like this.\n",
    "\n",
    "![MF_MB_Stay](https://drive.google.com/uc?export=view&id=1gvISmBfkmkgRqqgcJE6sLbztGqlKqagG)\n",
    "<br>\n",
    "_edited from Doll et al. 2015_\n",
    "\n",
    "How does the staying behavior change in this task? What do you see when <code>w=0</code>,  <code>w=1</code>, or anywhere in between?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a7c5aa",
   "metadata": {},
   "source": [
    "### Comparing performance between model-free and model-based agents.\n",
    "So, this task can be completed using two different strategies, but it's not clear how those two strategies translate to the amount of earned reward.\n",
    "\n",
    "To investigate this, the script below will simulate behavior for both a purely model-based agent (w=1) and a purely model-free agent. For both, it will compute the number of trials on which it earned reward. These scores (and their differences) will then be printed. This way, you can test whether the model-based agent does a better job in performing this task than the model-free learner!\n",
    "\n",
    "Try it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a model-based agent and check performance\n",
    "n_trials = 200\n",
    "alpha = 0.5\n",
    "beta = 4\n",
    "\n",
    "# simulate a model-based agent and check performance\n",
    "w = 1\n",
    "\n",
    "[stay, state_sim, prev_rews, mb_rewards] = mb_mf_agent(n_trials, alpha, beta, w)\n",
    "\n",
    "# calculate performance \n",
    "mb_score = sum(mb_rewards)*100 / n_trials\n",
    "print(f'Model-based performance: {round(mb_score,2)}% of the trials\\n')\n",
    "\n",
    "# Simulate a model-free agent and check performance\n",
    "# we will use the same values as above, except that we set w to 0\n",
    "w = 0\n",
    "\n",
    "[stay, state_sim, prev_rews, mf_rewards] = mb_mf_agent(n_trials, alpha, beta, w)\n",
    "\n",
    "# calculate performance \n",
    "mf_score = sum(mf_rewards)*100 / n_trials\n",
    "print(f'Model-free performance: {round(mf_score,2)}% of the trials\\n')\n",
    "\n",
    "\n",
    "# calculate difference in performance between the model-based and model-free agent\n",
    "print(f'Model-based increase in performance: {round(mb_score - mf_score,2)}% of the trials\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110bb411",
   "metadata": {},
   "source": [
    "<mark>Try to answer these questions:</mark>\n",
    "- Does the performance also change with a higher or lower _w_?\n",
    "- What happens for values of _w_ between 0 and 1?\n",
    "\n",
    "**Hint**: to see more robust scores, you can increase the number of simulated trials (hopefully this will show you that it is important to have enough trials and participants in your experiments!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00201c45",
   "metadata": {},
   "source": [
    "### Extra assignment: tracking the Q-values\n",
    "\n",
    "We can also plot the Q-values which were generated by this model, same as we did previously, along with the reward probabilities that we generated previously. \n",
    "\n",
    "If you set _w_ to 1, we can plot only the model-based values, and see if they do a better job than in the previous script! \n",
    "Which model tracks better? the model-free or model-based one?\n",
    "\n",
    "We have written the code to generate the plots below this cell. In the cell directly below, you can use the exact same model we used before, only you have to save (return) a few more things:\n",
    "- Qnet values of the planets for each trial\n",
    "- True reward probabilities\n",
    "- The choices the agent made on each trial\n",
    "\n",
    "**Hint**: This is almost exactly identical to how you did this in the last script, except now we just have a different name for the Q-values (the ones used to make decisions in your mixture agent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ad1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mb_mf_agent_Qs(n_trials, alpha, beta, w):\n",
    "    \n",
    "    # We'll use your generate_rewards function to generate drifting reward probabilities\n",
    "    bounds = [0, 1]\n",
    "    init_vals = [0.4, 0.6]\n",
    "    drift_rate = 0.2\n",
    "    reward_probs = generate_rewards(n_trials, bounds, init_vals, drift_rate)\n",
    "    \n",
    "    # Pre-allocate empty arrays for the data\n",
    "    \n",
    "    choices = np.zeros(n_trials) # which spaceship the participants chose\n",
    "    rewards = np.zeros(n_trials) # how many rewards the participants won on this trial\n",
    "    Qnets =                      # what the Q-values are on each trial     #--- Fill this in\n",
    "\n",
    "    # Initialise model-free values for the spaceships\n",
    "    Qmf1 = np.zeros((2,2)) + 0.5 \n",
    "    \n",
    "    # Initialise model-free values for the planets\n",
    "    Qmf2 = np.full(2, 0.5)\n",
    "    \n",
    "    # Initialise model-based values\n",
    "    Qmb = np.full(2, 0.5)\n",
    "    \n",
    "    # Now, we loop through the trials, making choices\n",
    "    for i, reward_prob in enumerate(reward_probs):\n",
    "\n",
    "        # Randomly start in rocket pair 1 (0) or pair 2 (1)\n",
    "        s1 = np.random.choice([0, 1])\n",
    "        \n",
    "        ### different:\n",
    "        # calculate model-based q-values - fill these in\n",
    "        # note that Qmb just reflects the MB values of the currently available actions. \n",
    "        # remove: They are calculated online, so there is no updating necessary\n",
    "        Qmb[0] =                        #--- Copy from your function above\n",
    "        Qmb[1] =                        #--- Copy from your function above\n",
    "        \n",
    "        ### different:\n",
    "        # mix the two sets of Q-values here with the mixing weight (remember that when w = 0, only the model-free\n",
    "        # q-values matter, while when w = 1, only the model-based values are taken into account)\n",
    "        Qnet =                          #--- Copy from your function above\n",
    "        \n",
    "        ### different:\n",
    "        # Agent makes a choice using softmax\n",
    "        p =                             #--- Copy from your function above\n",
    "\n",
    "        # Make a weighted choice using the np.random.choice function and the variable p (you can use help(function) \n",
    "        # to find out more) (identical to first model)\n",
    "        choice = choices[i] =           #--- Copy from your function above (note, we also save choices[i] here)\n",
    "        \n",
    "        # Make the transition to the planet\n",
    "        s2 =                            #--- Copy from your function above\n",
    "\n",
    "        # Calculate first prediction error (no reward here)\n",
    "        pe_1 =                          #--- Copy from your function above                          \n",
    "        \n",
    "        # update choice\n",
    "        Qmf1[s1,choice] =               #--- Copy from your function above\n",
    "        \n",
    "        # Agent receives a reward (1 or 0) (depending on reward probability)\n",
    "        r = int(np.random.uniform(0,1) < reward_probs[i,s2])\n",
    "        \n",
    "        # Calculate second prediction error (actual reward)\n",
    "        pe_2 =                          #--- Copy from your function above                                       \n",
    "        \n",
    "        # update value of alien (planet)\n",
    "        Qmf2[s2] =                      #--- Copy from your function above\n",
    "        \n",
    "        # update value of choice (spaceship)\n",
    "        Qmf1[s1,choice] =               #--- Copy from your function above\n",
    "              \n",
    "        # save q values\n",
    "        # you need to save 2 q-values for every trial, which are the same q-values as for the second stage (alien/planet) \n",
    "        Qnets[:,i] = Qnet                                #--- Fill this in\n",
    "        \n",
    "             \n",
    "    # data returned by function\n",
    "    return choices, Qnets, reward_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f3499",
   "metadata": {},
   "source": [
    "Now, we can run the cell below to generate a figure to track the q-values again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91da7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot how well this model did at tracking the highest probability of receiving a reward\n",
    "\n",
    "# set values\n",
    "n_trials = 200\n",
    "alpha = 0.5\n",
    "beta = 4\n",
    "w = 1\n",
    "\n",
    "# to print correct title on plot\n",
    "if w == 0:\n",
    "    behave = 'Model-free'\n",
    "elif w == 1:\n",
    "    behave = 'Model-based'\n",
    "else:\n",
    "    behave = 'Mixture'\n",
    "\n",
    "# run the agent\n",
    "choices, Qnets, reward_probs = mb_mf_agent_Qs(n_trials, alpha, beta, w)\n",
    "\n",
    "colors = ['red','purple']\n",
    "\n",
    "# Define plot sizes\n",
    "fig, ax = plt.subplots(figsize=(15, 4), facecolor='w')\n",
    "ax = plt.subplot2grid((3, 1), (0, 0), colspan=1, rowspan=2)\n",
    "\n",
    "# plot the Q-values\n",
    "for i, (i, color) in enumerate(zip(range(2), colors)):\n",
    "    ax.plot(np.arange(n_trials), Qnets[i, :], alpha = 0.8, label = 'Q-value ' + str(i+1), color = color)\n",
    "    \n",
    "# defining some plot stuff\n",
    "ax.set_ylim(0, 1) # ax.set_ylim(0, 1)\n",
    "ax.set_xlim(0, n_trials)\n",
    "ax.set_xticks([])\n",
    "ax.set_ylabel('Q-values')\n",
    "ax.legend()\n",
    "\n",
    "# Overlay reward values\n",
    "for i, (i, color) in enumerate(zip(range(2), colors)):\n",
    "    if i == 0:\n",
    "        label_ = 'Red planet'\n",
    "    else:\n",
    "        label_ = 'Purple planet'\n",
    "                \n",
    "    ax.plot(np.arange(n_trials), reward_probs[:,i], alpha = 0.6, linestyle = '--', label = label_, color = color)\n",
    "    \n",
    "\n",
    "# adding some labels and legends\n",
    "ax.set_ylabel('Probability of reward')\n",
    "\n",
    "# title corresponding to w value\n",
    "ax.set_title(f'{behave} tracking of values for planet and true reward probabilities', fontweight='bold')\n",
    "\n",
    "# text bubble for w according to value\n",
    "ax.text(190,0.2, f'w = {w}', color = 'black', size=12,\n",
    "       bbox=dict(facecolor='white', alpha=0.8, edgecolor='black', boxstyle = 'round,pad=1'))\n",
    "\n",
    "ax.legend(loc=1)\n",
    "\n",
    "# plot the choices the agent made (either red or purple planet)\n",
    "ax = plt.subplot2grid((3, 1), (2, 0), colspan=1, rowspan=1)\n",
    "for i, choice in enumerate(choices):\n",
    "    ax.scatter(i, 0, color=colors[int(choice)], s=15, alpha = 0.8)\n",
    "    \n",
    "# defining some plot stuff\n",
    "ax.set_xlim(0, n_trials)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel('Trials')\n",
    "ax.set_ylabel('Choices')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a87ae",
   "metadata": {},
   "source": [
    "In the cell above, set <code>w</code> to different values, and inspect the graphs.\n",
    "\n",
    "<mark>What do you think? Is there a difference in how the Q-values are tracked when the agent is purely model-based compared to when it is purely model-free?</mark>\n",
    "\n",
    "<mark>How does this exercise help you explain the difference in average reward between the model-based and the model-free decision maker?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a52a2",
   "metadata": {},
   "source": [
    "### Well done! You have finished the second step of this tutorial, amazing work!\n",
    "If you want to (and we still have time) feel free to open the next script for Step3, where will practice fitting the model to simulated data, and seeing how well our model does at recovering the parameter values (alpha, beta and w) that we use to simulate that data. This process is called parameter recovery (which you apparently have already learned about a little bit in the first session of today)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde71a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
